[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Packages (2e)",
    "section": "",
    "text": "Welcome to R packages by Hadley Wickham and Jenny Bryan. Packages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data. In this book you’ll learn how to turn your code into packages that others can easily download and use. Writing a package can seem overwhelming at first. So start with the basics and improve it over time. It doesn’t matter if your first version isn’t perfect as long as the next version is better.\nThis is the work-in-progress 2nd edition of the book."
  },
  {
    "objectID": "Preface.html",
    "href": "Preface.html",
    "title": "Preface",
    "section": "",
    "text": "Welcome to the 2nd edition of R Packages! If you’re familiar with the 1st edition, this preface describes the major changes so that you can focus your reading on the new areas.\nThere are X main goals for this edition:\n\nUpdate to reflect changes in the devtools package, specifically, its “conscious uncoupling” into a set of smaller, more focused packages.\nExpanded coverage of workflow and process, alongside the presentation of all the important moving parts that make up an R package.\nmore to come\n\nSpecifics about individual chapters:\n\nNew Chapter 2, “The Whole Game”, previews key steps in the package development process.\nNew Chapter 3, “Setup”, has been carved out of the previous Introduction and gained more detail.\nThe sections “Organising your functions” and “Code style”, from Chapter 7, “R code”, have been removed, in favor of an online style guide, https://style.tidyverse.org/. The style guide is paired with the new styler package (Müller and Walthert 2018) which can automatically apply many of the rules.\nThe chapter formerly known as “Package structure” has been expanded and split into two chapters, one covering package structure and state (Chapter 4) and another on workflows and tooling (Chapter 5).\nThe chapters on Git/GitHub and C/C++ have been removed. These didn’t have quite enough information to be useful, and since the first edition of the book, other resources have arisen that are better learning resources.\nThe very short inst chapter has been combined into Chapter 9, with all the other directories that are of relatively minor importance.\n\n\n\n\n\nMüller, Kirill, and Lorenz Walthert. 2018. Styler: Non-Invasive Pretty Printing of R Code. http://styler.r-lib.org."
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In R, the fundamental unit of shareable code is the package. A package bundles together code, data, documentation, and tests, and is easy to share with others. As of June 2022, there were over 18,000 packages available on the Comprehensive R Archive Network, or CRAN, the public clearing house for R packages. This huge variety of packages is one of the reasons that R is so successful: the chances are that someone has already solved a problem that you’re working on, and you can benefit from their work by downloading their package.\nIf you’re reading this book, you already know how to work with packages in the following ways:\nThe goal of this book is to teach you how to develop packages so that you can write your own, not just use other people’s. Why write a package? One compelling reason is that you have code that you want to share with others. Bundling your code into a package makes it easy for other people to use it, because like you, they already know how to use packages. If your code is in a package, any R user can easily download it, install it and learn how to use it.\nBut packages are useful even if you never share your code. As Hilary Parker says in her introduction to packages: “Seriously, it doesn’t have to be about sharing your code (although that is an added benefit!). It is about saving yourself time.” Organising code in a package makes your life easier because packages come with conventions. For example, you put R code in R/, you put tests in tests/ and you put data in data/. These conventions are helpful because:\nIt’s even possible to use packages to structure your data analyses (e.g., Marwick, Boettiger, and Mullen (2018a) or Marwick, Boettiger, and Mullen (2018b)), although we won’t delve deeply into that use case here."
  },
  {
    "objectID": "Introduction.html#intro-phil",
    "href": "Introduction.html#intro-phil",
    "title": "1  Introduction",
    "section": "\n1.1 Philosophy",
    "text": "1.1 Philosophy\nThis book espouses our philosophy of package development: anything that can be automated, should be automated. Do as little as possible by hand. Do as much as possible with functions. The goal is to spend your time thinking about what you want your package to do rather than thinking about the minutiae of package structure.\nThis philosophy is realized primarily through the devtools package, which is the public face for a suite of R functions that automate common development tasks. The release of version 2.0.0 in October 2018 marked its internal restructuring into a set of more focused packages, with devtools becoming more of a meta-package. The usethis package is the sub-package you are most likely to interact with directly; we explain the devtools-usethis relationship in Section 3.2.\nAs always, the goal of devtools is to make package development as painless as possible. It encapsulates the best practices developed by Hadley Wickham, initially from his years as a prolific solo developer. More recently, he has assembled a team of developers at RStudio, who collectively look after hundreds of open source R packages, including those known as the tidyverse. The reach of this team allows us to explore the space of all possible mistakes at an extraordinary scale. Fortunately, it also affords us the opportunity to reflect on both the successes and failures, in the company of expert and sympathetic colleagues. We try to develop practices that make life more enjoyable for both the maintainer and users of a package. The devtools meta-package is where these lessons are made concrete.\ndevtools works hand-in-hand with RStudio, which we believe is the best development environment for most R users. The main alternative is Emacs Speaks Statistics (ESS), which is a rewarding environment if you’re willing to put in the time to learn Emacs and customise it to your needs. The history of ESS stretches back over 20 years (predating R!), but it’s still actively developed and many of the workflows described in this book are also available there. For those loyal to vim, we recommend the Nvim-R plugin.\n\n\n\n\n\n\nRStudio\n\n\n\nThroughout the book, we highlight specific ways that RStudio can expedite your package development workflow, in specially formatted sections like this.\n\n\nTogether, devtools and RStudio insulate you from the low-level details of how packages are built. As you start to develop more packages, we highly recommend that you learn more about those details. The best resource for the official details of package development is always the official writing R extensions manual1. However, this manual can be hard to understand if you’re not already familiar with the basics of packages. It’s also exhaustive, covering every possible package component, rather than focusing on the most common and useful components, as this book does. Writing R extensions is a useful resource once you’ve mastered the basics and want to learn what’s going on under the hood."
  },
  {
    "objectID": "Introduction.html#intro-outline",
    "href": "Introduction.html#intro-outline",
    "title": "1  Introduction",
    "section": "\n1.2 In this book",
    "text": "1.2 In this book\n\nThe first part of the book is all about giving you all the tools you need to start your package development journey and we highly recommend that you read it in order. We begin in Chapter 2 with a run through the complete development of a small package. It’s meant to paint the big picture and suggest a workflow, before we descend into the detailed treatment of the key components of an R package. Then in Chapter 3 you’ll learn how to prepare your system for package development, and in Chapter 4 you’ll learn the basic structure of a package and how that varies across different states. Next, in Chapter 5, we’ll cover the core workflows that come up repeatedly for package developers. Then the first section of the book in Chapter 6 with another case study, this time focusing on how you might convert a script to a package and discussing the challenges you’ll face along the way.\nThe remainder of the book is design to be read as needed. Pick and choose between the packages as you need them.\nFirst we cover key package components: Chapter 7 discusses where your code lives and how to organize it, Chapter 8 shows you how to include data in your package, and Chapter 9 covers a few less important files and directories that need to be discussed somewhere.\nNext we’ll dive into to the package metadata, starting with DESCRIPTION and NAMESPACE in Chapter 10. We’ll then go deep into dependencies in Chapter 11, discussing when and how to depend on another package. We’ll finish off this part with a look at licensing in Chapter 12.\nTo ensure your package works as designed (and continues to work as you make changes), it’s essential to test your code, so the next three chapters cover the art and science of testing. Chapter 13 gets you started with the basics of testing with the testthat package. Chapter 14 teaches you how to design and organise tests in the most effective way. Then we finish off our coverage of testing in Chapter 15 which teaches you advanced skills to tackle the most challenging of cases.\nIf you want other people (including future-you!) to understand how to use the functions in your package, you’ll need to document them. Chapter 16 gets you started using roxygen to document the functions in your package. Function documentation is only helpful if you know what function to look up, so next in Chapter 17 we’ll discuss vignettes, which help you document the package as a whole. We’ll finish up documentation with a discussion of other important markdown files like README.md and NEWS.md in Chapter 18, and creating a package website with pkgdown in Chapter 19.\nThe book concludes with a look at the release of your package to CRAN and the maintenance thereafter. Chapter 20 dives into R CMD check, the most important tool for verifying your package is free from major defects. You’ll then learn how you can run R CMD check automatically every time you change your package in Chapter 21. If you’re planning on submitting your package to CRAN, Chapter 22 shows you the process and gives you our tips and tricks to maximize the chances of success. We conclude with a discussion the post-release lifecycle in Chapter 23.\nThis is a lot to learn, but don’t feel overwhelmed. Start with a minimal subset of useful features (e.g. just an R/ directory!) and build up over time. To paraphrase the Zen monk Shunryu Suzuki: “Each package is perfect the way it is — and it can use a little improvement”."
  },
  {
    "objectID": "Introduction.html#you-wont-learn",
    "href": "Introduction.html#you-wont-learn",
    "title": "1  Introduction",
    "section": "\n1.3 What you won’t learn",
    "text": "1.3 What you won’t learn\nThere are a few very important topics that you won’t learn about in this book:\n\n\nGit and GitHub: mastering a version control system is vital to easily collaborate with others, and is useful even for solo work because it allows you to easily undo mistakes. Learn from http://happygitwithr.com/.\nCompiled code: R code is designed for human efficiency, not computer efficiency, so it’s useful to have a tool in your back pocket that allows you to write fast code. Learn more in https://adv-r.hadley.nz/rcpp.html, or https://cpp11.r-lib.org.\nMarkdown and RMarkdown."
  },
  {
    "objectID": "Introduction.html#intro-ack",
    "href": "Introduction.html#intro-ack",
    "title": "1  Introduction",
    "section": "\n1.4 Acknowledgments",
    "text": "1.4 Acknowledgments\nSince the first edition of R Packages was published, the packages supporting the workflows described here have undergone extensive development. The original trio of devtools, roxygen2, and testthat has expanded to include the packages created by the “conscious uncoupling” of devtools, as described in Section 3.2. Most of these packages originate with Hadley Wickham (HW), because of their devtools roots. There are many other significant contributors, many of whom now serve as maintainers:\n\ndevtools: HW, Winston Chang, Jim Hester (maintainer, >= v1.13.5)\nusethis: HW, Jennifer Bryan (maintainer >= v1.5.0)\nroxygen2: HW (maintainer), Peter Danenburg, Manuel Eugster\n\ntestthat: HW (maintainer)\ndesc: Gábor Csárdi (maintainer), Kirill Müller, Jim Hester\n\npkgbuild: HW, Jim Hester (maintainer)\npkgload: HW, Jim Hester (maintainer), Winston Chang\n\nrcmdcheck: Gábor Csárdi (maintainer)\nremotes: HW, Jim Hester (maintainer), Gábor Csárdi, Winston Chang, Martin Morgan, Dan Tenenbaum\n\nrevdepcheck: HW, Gábor Csárdi (maintainer)\nsessioninfo: HW, Gábor Csárdi (maintainer), Winston Chang, Robert Flight, Kirill Müller, Jim Hester\n\n\nThis book and the R package development community benefit tremendously from experts who smooth over specific pain points:\n\n\nKevin Ushey, JJ Allaire, and Dirk Eddelbuettel tirelessly answered all sorts of C, C++, and Rcpp questions.\n\nCraig Citro wrote much of the initial code to facilitate using Travis-CI with R packages.\n\nJeroen Ooms also helps to maintain R community infrastructure, such as the current R support for Travis-CI (along with Jim Hester), and the Windows toolchain.\n\nTODO: revisit rest of this section when 2nd edition nears completion. Currently applies to and worded for 1st edition.\nOften the only way I learn how to do it the right way is by doing it the wrong way first. For suffering through many package development errors, I’d like to thank all the CRAN maintainers, especially Brian Ripley, Uwe Ligges and Kurt Hornik.\nThis book was written and revised in the open and it is truly a community effort: many people read drafts, fix typos, suggest improvements, and contribute content. Without those contributors, the book wouldn’t be nearly as good as it is, and we are deeply grateful for their help.\nA special thanks goes to Peter Li, who read the book from cover-to-cover and provided many fixes. I also deeply appreciate the time the reviewers (Duncan Murdoch, Karthik Ram, Vitalie Spinu and Ramnath Vaidyanathan) spent reading the book and giving me thorough feedback.\nThanks go to all contributors who submitted improvements via github (in alphabetical order): @aaronwolen, @adessy, Adrien Todeschini, Andrea Cantieni, Andy Visser, @apomatix, Ben Bond-Lamberty, Ben Marwick, Brett K, Brett Klamer, @contravariant, Craig Citro, David Robinson, David Smith, @davidkane9, Dean Attali, Eduardo Ariño de la Rubia, Federico Marini, Gerhard Nachtmann, Gerrit-Jan Schutten, Hadley Wickham, Henrik Bengtsson, @heogden, Ian Gow, @jacobbien, Jennifer (Jenny) Bryan, Jim Hester, @jmarshallnz, Jo-Anne Tan, Joanna Zhao, Joe Cainey, John Blischak, @jowalski, Justin Alford, Karl Broman, Karthik Ram, Kevin Ushey, Kun Ren, @kwenzig, @kylelundstedt, @lancelote, Lech Madeyski, @lindbrook, @maiermarco, Manuel Reif, Michael Buckley, @MikeLeonard, Nick Carchedi, Oliver Keyes, Patrick Kimes, Paul Blischak, Peter Meissner, @PeterDee, Po Su, R. Mark Sharp, Richard M. Smith, @rmar073, @rmsharp, Robert Krzyzanowski, @ryanatanner, Sascha Holzhauer, @scharne, Sean Wilkinson, @SimonPBiggs, Stefan Widgren, Stephen Frank, Stephen Rushe, Tony Breyal, Tony Fischetti, @urmils, Vlad Petyuk, Winston Chang, @winterschlaefer, @wrathematics, @zhaoy."
  },
  {
    "objectID": "Introduction.html#intro-conventions",
    "href": "Introduction.html#intro-conventions",
    "title": "1  Introduction",
    "section": "\n1.5 Conventions",
    "text": "1.5 Conventions\nThroughout this book, we write fun() to refer to functions, var to refer to variables and function arguments, and path/ for paths.\nLarger code blocks intermingle input and output. Output is commented so that if you have an electronic version of the book, e.g., https://r-pkgs.org, you can easily copy and paste examples into R. Output comments look like #> to distinguish them from regular comments."
  },
  {
    "objectID": "Introduction.html#intro-colophon",
    "href": "Introduction.html#intro-colophon",
    "title": "1  Introduction",
    "section": "\n1.6 Colophon",
    "text": "1.6 Colophon\nThis book was authored using quarto inside RStudio. The website is hosted with Netlify, and automatically updated after every commit by GitHub actions. The complete source is available from GitHub.\nThis version of the book was built with:\n\nlibrary(devtools)\n#> Loading required package: usethis\nlibrary(roxygen2)\nlibrary(testthat)\n#> \n#> Attaching package: 'testthat'\n#> The following object is masked from 'package:devtools':\n#> \n#>     test_file\ndevtools::session_info()\n#> ─ Session info ───────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.2.2 (2022-10-31)\n#>  os       Ubuntu 20.04.5 LTS\n#>  system   x86_64, linux-gnu\n#>  ui       X11\n#>  language (EN)\n#>  collate  C.UTF-8\n#>  ctype    C.UTF-8\n#>  tz       UTC\n#>  date     2022-11-11\n#>  pandoc   2.5 @ /usr/bin/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────\n#>  package     * version    date (UTC) lib source\n#>  brio          1.1.3      2021-11-30 [1] RSPM\n#>  cachem        1.0.6      2021-08-19 [1] RSPM\n#>  callr         3.7.3      2022-11-02 [1] RSPM\n#>  cli           3.4.1      2022-09-23 [1] RSPM\n#>  crayon        1.5.2      2022-09-29 [1] RSPM\n#>  devtools    * 2.4.5      2022-10-11 [1] RSPM\n#>  digest        0.6.30     2022-10-18 [1] RSPM\n#>  ellipsis      0.3.2      2021-04-29 [1] RSPM\n#>  evaluate      0.18       2022-11-07 [1] RSPM\n#>  fastmap       1.1.0      2021-01-25 [1] RSPM\n#>  fs            1.5.2      2021-12-08 [1] RSPM\n#>  glue          1.6.2      2022-02-24 [1] RSPM\n#>  htmltools     0.5.3      2022-07-18 [1] RSPM\n#>  htmlwidgets   1.5.4      2021-09-08 [1] RSPM\n#>  httpuv        1.6.6      2022-09-08 [1] RSPM\n#>  jsonlite      1.8.3      2022-10-21 [1] RSPM\n#>  knitr         1.40       2022-08-24 [1] RSPM\n#>  later         1.3.0      2021-08-18 [1] RSPM\n#>  lifecycle     1.0.3      2022-10-07 [1] RSPM\n#>  magrittr      2.0.3      2022-03-30 [1] RSPM\n#>  memoise       2.0.1      2021-11-26 [1] RSPM\n#>  mime          0.12       2021-09-28 [1] RSPM\n#>  miniUI        0.1.1.1    2018-05-18 [1] RSPM\n#>  pkgbuild      1.3.1      2021-12-20 [1] RSPM\n#>  pkgload       1.3.1      2022-10-28 [1] RSPM\n#>  prettyunits   1.1.1      2020-01-24 [1] RSPM\n#>  processx      3.8.0      2022-10-26 [1] RSPM\n#>  profvis       0.3.7      2020-11-02 [1] RSPM\n#>  promises      1.2.0.1    2021-02-11 [1] RSPM\n#>  ps            1.7.2      2022-10-26 [1] RSPM\n#>  purrr         0.3.5      2022-10-06 [1] RSPM\n#>  R6            2.5.1      2021-08-19 [1] RSPM\n#>  Rcpp          1.0.9      2022-07-08 [1] RSPM\n#>  remotes       2.4.2      2021-11-30 [1] RSPM\n#>  rlang         1.0.6      2022-09-24 [1] RSPM\n#>  rmarkdown     2.17       2022-10-07 [1] RSPM\n#>  roxygen2    * 7.2.1      2022-07-18 [1] RSPM\n#>  sessioninfo   1.2.2      2021-12-06 [1] RSPM\n#>  shiny         1.7.3      2022-10-25 [1] RSPM\n#>  stringi       1.7.8      2022-07-11 [1] RSPM\n#>  stringr       1.4.1.9000 2022-11-11 [1] Github (tidyverse/stringr@ebf3823)\n#>  testthat    * 3.1.5      2022-10-08 [1] RSPM\n#>  urlchecker    1.0.1      2021-11-30 [1] RSPM\n#>  usethis     * 2.1.6      2022-05-25 [1] RSPM\n#>  vctrs         0.5.0      2022-10-22 [1] RSPM\n#>  xfun          0.34       2022-10-18 [1] RSPM\n#>  xml2          1.3.3      2021-11-30 [1] RSPM\n#>  xtable        1.8-4      2019-04-21 [1] RSPM\n#> \n#>  [1] /home/runner/work/_temp/Library\n#>  [2] /opt/R/4.2.2/lib/R/site-library\n#>  [3] /opt/R/4.2.2/lib/R/library\n#> \n#> ──────────────────────────────────────────────────────────────────\n\n\n\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018a. “Packaging Data Analytical Work Reproducibly Using r (and Friends).” The American Statistician 72 (1): 80–88. https://doi.org/10.1080/00031305.2017.1375986.\n\n\n———. 2018b. “Packaging Data Analytical Work Reproducibly Using r (and Friends).” PeerJ Preprints 6 (March): e3192v2. https://doi.org/10.7287/peerj.preprints.3192v2."
  },
  {
    "objectID": "Whole-game.html",
    "href": "Whole-game.html",
    "title": "2  The whole game",
    "section": "",
    "text": "Spoiler alert!\nThis chapter runs through the development of a small toy package. It’s meant to paint the Big Picture and suggest a workflow, before we descend into the detailed treatment of the key components of an R package.\nTo keep the pace brisk, we exploit the modern conveniences in the devtools package and the RStudio IDE. In later chapters, we are more explicit about what those helpers are doing for us."
  },
  {
    "objectID": "Whole-game.html#load-devtools-and-friends",
    "href": "Whole-game.html#load-devtools-and-friends",
    "title": "2  The whole game",
    "section": "\n2.2 Load devtools and friends",
    "text": "2.2 Load devtools and friends\nYou can initiate your new package from any active R session. You don’t need to worry about whether you’re in an existing or new project or not. The functions we use take care of this.\nLoad the devtools package, which is the public face of a set of packages that support various aspects of package development. The most obvious of these is the usethis package, which you’ll see is also being loaded.\n\nlibrary(devtools)\n#> Loading required package: usethis\n\nDo you have an old version of devtools? Compare your version against ours and upgrade if necessary.\n\npackageVersion(\"devtools\")\n#> [1] '2.4.5'"
  },
  {
    "objectID": "Whole-game.html#toy-package-regexcite",
    "href": "Whole-game.html#toy-package-regexcite",
    "title": "2  The whole game",
    "section": "\n2.3 Toy package: regexcite",
    "text": "2.3 Toy package: regexcite\nWe use various functions from devtools to build a small toy package from scratch, with features commonly seen in released packages:\n\nFunctions to address a specific need, in this case helpers for work with regular expressions.\nVersion control and an open development process.\n\nThis is completely optional in your work, but highly recommended. You’ll see how Git and GitHub help us expose all the intermediate stages of our toy package.\n\n\nAccess to established workflows for installation, getting help, and checking quality.\n\nDocumentation for individual functions via roxygen2.\nUnit testing with testthat.\nDocumentation for the package as a whole via an executable README.Rmd.\n\n\n\nWe call the package regexcite and it will have a couple functions that make common tasks with regular expressions easier. Please note that these functions are super simple and definitely not the point! For real work, there are several proper R packages that address this problem space:\n\n\nstringr (which uses stringi)\nstringi\nrex\nrematch2\n\nThe regexcite package itself is not our goal here. It is a device for demonstrating a typical workflow for package development with devtools."
  },
  {
    "objectID": "Whole-game.html#peek-at-the-finished-product",
    "href": "Whole-game.html#peek-at-the-finished-product",
    "title": "2  The whole game",
    "section": "\n2.4 Peek at the finished product",
    "text": "2.4 Peek at the finished product\nThe regexcite package is tracked during its development with the Git version control system. This is purely optional and you can certainly follow along without implementing this. A nice side benefit is that we eventually connect it to a remote repository on GitHub, which means you can see the glorious result we are working towards by visiting regexcite on GitHub: https://github.com/jennybc/regexcite. By inspecting the commit history and especially the diffs, you can see exactly what changes at each step of the process laid out below."
  },
  {
    "objectID": "Whole-game.html#create_package",
    "href": "Whole-game.html#create_package",
    "title": "2  The whole game",
    "section": "\n2.5 create_package()\n",
    "text": "2.5 create_package()\n\nCall create_package() to initialize a new package in a directory on your computer. create_package() will automatically create that directory if it doesn’t exist yet (and that is usually the case). See Section 5.2.3 for more.\nMake a deliberate choice about where to create this package on your computer. It should probably be somewhere within your home directory, alongside your other R projects. It should not be nested inside another RStudio Project, R package, or Git repo. Nor should it be in an R package library, which holds packages that have already been built and installed. The conversion of the source package we create here into an installed package is part of what devtools facilitates. Don’t try to do devtools’ job for it! See Section 5.2.4 for more.\nSubstitute your chosen path into a create_package() call like this:\n\ncreate_package(\"~/path/to/regexcite\")\n\nWe have to work in a temp directory, because this book is built non-interactively, in the cloud. Behind the scenes, we’re executing our own create_package() command, but don’t be surprised if our output differs a bit from yours.\n\n#> ✔ Creating '/tmp/RtmpHuKhho/regexcite/'\n#> ✔ Setting active project to '/tmp/RtmpHuKhho/regexcite'\n#> ✔ Creating 'R/'\n#> ✔ Writing 'DESCRIPTION'\n#> Package: regexcite\n#> Title: What the Package Does (One Line, Title Case)\n#> Version: 0.0.0.9000\n#> Authors@R (parsed):\n#>     * First Last <first.last@example.com> [aut, cre] (YOUR-ORCID-ID)\n#> Description: What the package does (one paragraph).\n#> License: `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n#>     license\n#> Encoding: UTF-8\n#> Roxygen: list(markdown = TRUE)\n#> RoxygenNote: 7.2.1\n#> ✔ Writing 'NAMESPACE'\n#> ✔ Writing 'regexcite.Rproj'\n#> ✔ Adding '^regexcite\\\\.Rproj$' to '.Rbuildignore'\n#> ✔ Adding '.Rproj.user' to '.gitignore'\n#> ✔ Adding '^\\\\.Rproj\\\\.user$' to '.Rbuildignore'\n#> ✔ Setting active project to '<no active project>'\n\nIf you’re working in RStudio, you should find yourself in a new instance of RStudio, opened into your new regexcite package (and Project). If you somehow need to do this manually, navigate to the directory and double click on regexcite.Rproj. RStudio has special handling for packages and you should now see a Build tab in the same pane as Environment and History.\n\nWhat’s in this new directory that is also an R package and, probably, an RStudio Project? Here’s a listing (locally, you can consult your Files pane):\n\n#> # A tibble: 6 × 2\n#>   path            type     \n#>   <fs::path>      <fct>    \n#> 1 .Rbuildignore   file     \n#> 2 .gitignore      file     \n#> 3 DESCRIPTION     file     \n#> 4 NAMESPACE       file     \n#> 5 R               directory\n#> 6 regexcite.Rproj file\n\n\n\n\n\n\n\nRStudio\n\n\n\nIn the Files pane, go to More (gear symbol) > Show Hidden Files to toggle the visibility of hidden files (a.k.a. “dotfiles”). A select few are visible all the time, but sometimes you want to see them all.\n\n\n\n\n.Rbuildignore lists files that we need to have around but that should not be included when building the R package from source. More in Section 4.4.1.\n\n.Rproj.user, if you have it, is a directory used internally by RStudio.\n\n.gitignore anticipates Git usage and ignores some standard, behind-the-scenes files created by R and RStudio. Even if you do not plan to use Git, this is harmless.\n\nDESCRIPTION provides metadata about your package. We edit this shortly.\n\nNAMESPACE declares the functions your package exports for external use and the external functions your package imports from other packages. At this point, it is empty, except for a comment declaring that this is a file we will not edit by hand.\nThe R/ directory is the “business end” of your package. It will soon contain .R files with function definitions.\n\nregexcite.Rproj is the file that makes this directory an RStudio Project. Even if you don’t use RStudio, this file is harmless. Or you can suppress its creation with create_package(..., rstudio = FALSE). More in Section 5.3.\n\nYou probably need to call library(devtools) again, because create_package() has probably dropped you into a fresh R session, in your new package.\n\nlibrary(devtools)"
  },
  {
    "objectID": "Whole-game.html#use_git",
    "href": "Whole-game.html#use_git",
    "title": "2  The whole game",
    "section": "\n2.6 use_git()\n",
    "text": "2.6 use_git()\n\nThe regexcite directory is an R source package and an RStudio Project. Now we make it also a Git repository, with use_git(). (By the way, use_git() works in any project, regardless of whether it’s an R package.)\n\nuse_git()\n#> ✔ Initialising Git repo\n#> ✔ Adding '.Rhistory', '.Rdata', '.httr-oauth', '.DS_Store' to '.gitignore'\n\nIn an interactive session, you will be asked if you want to commit some files here and you should probably accept the offer. Behind the scenes, we’ll cause the same to happen for us.\nWhat’s new? Only the creation of a .git directory, which is hidden in most contexts, including the RStudio file browser. Its existence is evidence that we have indeed initialized a Git repo here.\n\n#> # A tibble: 1 × 2\n#>   path       type     \n#>   <fs::path> <fct>    \n#> 1 .git       directory\n\nIf you’re using RStudio, it probably requested permission to relaunch itself in this Project, which you should do. You can do so manually by quitting, then relaunching RStudio by double clicking on regexcite.Rproj. Now, in addition to package development support, you have access to a basic Git client in the Git tab of the Environment/History/Build pane.\n\nClick on History (the clock icon in the Git pane) and, if you consented, you will see an initial commit made via use_git():\n\n#> # A tibble: 1 × 3\n#>   commit                                   author            message\n#>   <chr>                                    <chr>             <chr>  \n#> 1 6de5a0a0465723a624c155c908de0b490e84ee69 rosericazondekon… \"Initi…\n\n\n\n\n\n\n\nRStudio\n\n\n\nRStudio can initialize a Git repository, in any Project, even if it’s not an R package, as long you’ve set up RStudio + Git integration. Do Tools > Version Control > Project Setup. Then choose Version control system: Git and initialize a new git repository for this project."
  },
  {
    "objectID": "Whole-game.html#write-the-first-function",
    "href": "Whole-game.html#write-the-first-function",
    "title": "2  The whole game",
    "section": "\n2.7 Write the first function",
    "text": "2.7 Write the first function\nA fairly common task when dealing with strings is the need to split a single string into many parts. The strsplit() function in base R does exactly this.\n\n(x <- \"alfa,bravo,charlie,delta\")\n#> [1] \"alfa,bravo,charlie,delta\"\nstrsplit(x, split = \",\")\n#> [[1]]\n#> [1] \"alfa\"    \"bravo\"   \"charlie\" \"delta\"\n\nTake a close look at the return value.\n\nstr(strsplit(x, split = \",\"))\n#> List of 1\n#>  $ : chr [1:4] \"alfa\" \"bravo\" \"charlie\" \"delta\"\n\nThe shape of this return value often surprises people or, at least, inconveniences them. The input is a character vector of length one and the output is a list of length one. This makes total sense in light of R’s fundamental tendency towards vectorization. But sometimes it’s still a bit of a bummer. Often you know that your input is morally a scalar, i.e. it’s just a single string, and really want the output to be the character vector its parts.\nThis leads R users to employ various methods of “unlist”-ing the result:\n\nunlist(strsplit(x, split = \",\"))\n#> [1] \"alfa\"    \"bravo\"   \"charlie\" \"delta\"\n\nstrsplit(x, split = \",\")[[1]]\n#> [1] \"alfa\"    \"bravo\"   \"charlie\" \"delta\"\n\nThe second, safer solution is the basis for the inaugural function of regexcite: strsplit1().\n\nstrsplit1 <- function(x, split) {\n  strsplit(x, split = split)[[1]]\n}\n\nThis book does not teach you how to write functions in R. To learn more about that take a look at the Functions chapter of R for Data Science and the Functions chapter of Advanced R.\n\n\n\n\n\n\nTip\n\n\n\nThe name of strsplit1() is a nod to the very handy paste0(), which first appeared in R 2.15.0 in 2012. paste0() was created to address the extremely common use case of paste()-ing strings together without a separator. paste0() has been lovingly described as “statistical computing’s most influential contribution of the 21st century”."
  },
  {
    "objectID": "Whole-game.html#use_r",
    "href": "Whole-game.html#use_r",
    "title": "2  The whole game",
    "section": "\n2.8 use_r()\n",
    "text": "2.8 use_r()\n\nWhere shall we define strsplit1()? Save it in a .R file, in the R/ subdirectory of your package. A reasonable starting position is to make a new .R file for each user-facing function in your package and name the file after the function. As you add more functions, you’ll want to relax this and begin to group related functions together. We’ll save the definition of strsplit1() in the file R/strsplit1.R.\nThe helper use_r() creates and/or opens a script below R/. It really shines in a more mature package, when navigating between .R files and the associated test file. But, even here, it’s useful to keep yourself from getting too carried away while working in Untitled4.\n\nuse_r(\"strsplit1\")\n#> • Edit 'R/strsplit1.R'\n#> • Call `use_test()` to create a matching test file\n\nPut the definition of strsplit1() and only the definition of strsplit1() in R/strsplit1.R and save it. The file R/strsplit1.R should NOT contain any of the other top-level code we have recently executed, such as the definition of our practice input x, library(devtools), or use_git(). This foreshadows an adjustment you’ll need to make as you transition from writing R scripts to R packages. Packages and scripts use different mechanisms to declare their dependency on other packages and to store example or test code. We explore this further in Chapter 7."
  },
  {
    "objectID": "Whole-game.html#sec-whole-game-load-all",
    "href": "Whole-game.html#sec-whole-game-load-all",
    "title": "2  The whole game",
    "section": "\n2.9 load_all()\n",
    "text": "2.9 load_all()\n\nHow do we test drive strsplit1()? If this were a regular R script, we might use RStudio to send the function definition to the R Console and define strsplit1() in the global environment. Or maybe we’d call source(\"R/strsplit1.R\"). For package development, however, devtools offers a more robust approach. See Section 5.5 for more.\nCall load_all() to make strsplit1() available for experimentation.\n\nload_all()\n#> ℹ Loading regexcite\n\nNow call strsplit1(x) to see how it works.\n\n(x <- \"alfa,bravo,charlie,delta\")\n#> [1] \"alfa,bravo,charlie,delta\"\nstrsplit1(x, split = \",\")\n#> [1] \"alfa\"    \"bravo\"   \"charlie\" \"delta\"\n\nNote that load_all() has made the strsplit1() function available, although it does not exist in the global environment.\n\nexists(\"strsplit1\", where = globalenv(), inherits = FALSE)\n#> [1] FALSE\n\nIf you see TRUE instead of FALSE, that indicates you’re still using a script-oriented workflow and sourcing your functions. Here’s how to get back on track:\n\nClean out the global environment and restart R.\nRe-attach devtools with library(devtools) and re-load regexcite with load_all().\nRedefine the test input x and call strsplit1(x, split = \",\") again. This should work!\nRun exists(\"strsplit1\", where = globalenv(), inherits = FALSE) again and you should see FALSE.\n\nload_all() simulates the process of building, installing, and attaching the regexcite package. As your package accumulates more functions, some exported, some not, some of which call each other, some of which call functions from packages you depend on, load_all() gives you a much more accurate sense of how the package is developing than test driving functions defined in the global environment. Also load_all() allows much faster iteration than actually building, installing, and attaching the package.\nReview so far:\n\nWe wrote our first function, strsplit1(), to split a string into a character vector (not a list containing a character vector).\nWe used load_all() to quickly make this function available for interactive use, as if we’d built and installed regexcite and attached it via library(regexcite).\n\n\n\n\n\n\n\nRStudio\n\n\n\nRStudio exposes load_all() in the Build menu, in the Build pane via More > Load All, and in keyboard shortcuts Ctrl + Shift + L (Windows & Linux) or Cmd + Shift + L (macOS).\n\n\n\n2.9.1 Commit strsplit1()\n\nIf you’re using Git, use your preferred method to commit the new R/strsplit1.R file. We do so behind the scenes here and here’s the associated diff.\n\ndiff --git a/R/strsplit1.R b/R/strsplit1.R\nnew file mode 100644\nindex 0000000..29efb88\n--- /dev/null\n+++ b/R/strsplit1.R\n@@ -0,0 +1,3 @@\n+strsplit1 <- function(x, split) {\n+  strsplit(x, split = split)[[1]]\n+}\n\nFrom this point on, we commit after each step. Remember these commits are available in the public repository."
  },
  {
    "objectID": "Whole-game.html#check",
    "href": "Whole-game.html#check",
    "title": "2  The whole game",
    "section": "\n2.10 check()\n",
    "text": "2.10 check()\n\nWe have informal, empirical evidence that strsplit1() works. But how can we be sure that all the moving parts of the regexcite package still work? This may seem silly to check, after such a small addition, but it’s good to establish the habit of checking this often.\nR CMD check, executed in the shell, is the gold standard for checking that an R package is in full working order. check() is a convenient way to run this without leaving your R session.\nNote that check() produces rather voluminous output, optimized for interactive consumption. We intercept that here and just reveal a summary. Your local check() output will be different.\n\ncheck()\n\n\n── R CMD check results ─────────────────── regexcite 0.0.0.9000 ────\nDuration: 20.8s\n\n❯ checking DESCRIPTION meta-information ... WARNING\n  Non-standard license specification:\n    `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n    license\n  Standardizable: FALSE\n\n0 errors ✔ | 1 warning ✖ | 0 notes ✔\n\nRead the output of the check! Deal with problems early and often. It’s just like incremental development of .R and .Rmd files. The longer you go between full checks that everything works, the harder it becomes to pinpoint and solve your problems.\nAt this point, we expect 1 warning (and 0 errors, 0 notes):\nNon-standard license specification:\n  `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n  license\nWe’ll address that soon, by doing exactly what it says.\n\n\n\n\n\n\nRStudio\n\n\n\nRStudio exposes check() in the Build menu, in the Build pane via Check, and in keyboard shortcuts Ctrl + Shift + E (Windows & Linux) or Cmd + Shift + E (macOS)."
  },
  {
    "objectID": "Whole-game.html#edit-description",
    "href": "Whole-game.html#edit-description",
    "title": "2  The whole game",
    "section": "\n2.11 Edit DESCRIPTION\n",
    "text": "2.11 Edit DESCRIPTION\n\nThe DESCRIPTION file provides metadata about your package and is covered fully in Section 10.2. This is a good time to have a look at regexcite’s current DESCRIPTION. You’ll see it’s populated with boilerplate content, which needs to be replaced.\nMake these edits:\n\nMake yourself the author. If you don’t have an ORCID, you can omit the comment = ... portion.\nWrite some descriptive text in the Title and Description fields.\n\n\n\n\n\n\n\nRStudio\n\n\n\nUse Ctrl + . in RStudio and start typing “DESCRIPTION” to activate a helper that makes it easy to open a file for editing. In addition to a filename, your hint can be a function name. This is very handy once a package has lots of files.\n\n\nWhen you’re done, DESCRIPTION should look similar to this:\n\n\nPackage: regexcite\nTitle: Make Regular Expressions More Exciting\nVersion: 0.0.0.9000\nAuthors@R: \n    person(\"Jane\", \"Doe\", , \"jane@example.com\", role = c(\"aut\", \"cre\"))\nDescription: Convenience functions to make some common tasks with string\n    manipulation and regular expressions a bit easier.\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n    license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.1.2"
  },
  {
    "objectID": "Whole-game.html#use_mit_license",
    "href": "Whole-game.html#use_mit_license",
    "title": "2  The whole game",
    "section": "\n2.12 use_mit_license()\n",
    "text": "2.12 use_mit_license()\n\n\nPick a License, Any License. – Jeff Atwood\n\nWe currently have a placeholder in the License field of DESCRIPTION that’s deliberately invalid and suggests a resolution.\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n    license\nLet’s call use_mit_license().\n\nuse_mit_license()\n#> ✔ Setting License field in DESCRIPTION to 'MIT + file LICENSE'\n#> ✔ Writing 'LICENSE'\n#> ✔ Writing 'LICENSE.md'\n#> ✔ Adding '^LICENSE\\\\.md$' to '.Rbuildignore'\n\nThis configures the License field correctly for the MIT license, which promises to name the copyright holders and year in a LICENSE file. Open the newly created LICENSE file and confirm it looks something like this:\n\nwriteLines(readLines(\"LICENSE\"))\nYEAR: 2022\nCOPYRIGHT HOLDER: regexcite authors\n\nLike other license helpers, use_mit_license() also puts a copy of the full license in LICENSE.md and adds this file to .Rbuildignore. It’s considered a best practice to include a full license in your package’s source, such as on GitHub, but CRAN disallows the inclusion of this file in a package tarball."
  },
  {
    "objectID": "Whole-game.html#whole-game-document",
    "href": "Whole-game.html#whole-game-document",
    "title": "2  The whole game",
    "section": "\n2.13 document()\n",
    "text": "2.13 document()\n\nWouldn’t it be nice to get help on strsplit1(), just like we do with other R functions? This requires that your package have a special R documentation file, man/strsplit1.Rd, written in an R-specific markup language that is sort of like LaTeX. Luckily we don’t necessarily have to author that directly.\nWe write a specially formatted comment right above strsplit1(), in its source file, and then let a package called roxygen2 handle the creation of man/strsplit1.Rd. The motivation and mechanics of roxygen2 are covered in Chapter 16.\nIf you use RStudio, open R/strsplit1.R in the source editor and put the cursor somewhere in the strsplit1() function definition. Now do Code > Insert roxygen skeleton. A very special comment should appear above your function, in which each line begins with #'. RStudio only inserts a barebones template, so you will need to edit it to look something like that below.\nIf you don’t use RStudio, create the comment yourself. Regardless, you should modify it to look something like this:\n\n#' Split a string\n#'\n#' @param x A character vector with one element.\n#' @param split What to split on.\n#'\n#' @return A character vector.\n#' @export\n#'\n#' @examples\n#' x <- \"alfa,bravo,charlie,delta\"\n#' strsplit1(x, split = \",\")\nstrsplit1 <- function(x, split) {\n  strsplit(x, split = split)[[1]]\n}\n\n\nBut we’re not done yet! We still need to trigger the conversion of this new roxygen comment into man/strsplit1.Rd with document():\n\ndocument()\n#> ℹ Updating regexcite documentation\n#> Setting `RoxygenNote` to \"7.2.1\"\n#> ℹ Loading regexcite\n#> Writing 'NAMESPACE'\n#> Writing 'strsplit1.Rd'\n\n\n\n\n\n\n\nRStudio\n\n\n\nRStudio exposes document() in the Build menu, in the Build pane via More > Document, and in keyboard shortcuts Ctrl + Shift + D (Windows & Linux) or Cmd + Shift + D (macOS).\n\n\nYou should now be able to preview your help file like so:\n\n?strsplit1\n\nYou’ll see a message like “Rendering development documentation for ‘strsplit1’”, which reminds that you are basically previewing draft documentation. That is, this documentation is present in your package’s source, but is not yet present in an installed package. In fact, we haven’t installed regexcite yet, but we will soon.\nNote also that your package’s documentation won’t be properly wired up until it has been formally built and installed. This polishes off niceties like the links between help files and the creation of a package index.\n\n2.13.1 NAMESPACE changes\nIn addition to converting strsplit1()’s special comment into man/strsplit1.Rd, the call to document() updates the NAMESPACE file, based on @export tags found in roxygen comments. Open NAMESPACE for inspection. The contents should be:\n\n\n# Generated by roxygen2: do not edit by hand\n\nexport(strsplit1)\n\nThe export directive in NAMESPACE is what makes strsplit1() available to a user after attaching regexcite via library(regexcite). Just as it is entirely possible to author .Rd files “by hand”, you can manage NAMESPACE explicitly yourself. But we choose to delegate this to devtools (and roxygen2)."
  },
  {
    "objectID": "Whole-game.html#check-again",
    "href": "Whole-game.html#check-again",
    "title": "2  The whole game",
    "section": "\n2.14 check() again",
    "text": "2.14 check() again\nregexcite should pass R CMD check cleanly now and forever more: 0 errors, 0 warnings, 0 notes.\n\ncheck()\n\n\n── R CMD check results ─────────────────── regexcite 0.0.0.9000 ────\nDuration: 22.6s\n\n0 errors ✔ | 0 warnings ✔ | 0 notes ✔"
  },
  {
    "objectID": "Whole-game.html#install",
    "href": "Whole-game.html#install",
    "title": "2  The whole game",
    "section": "\n2.15 install()\n",
    "text": "2.15 install()\n\nSince we have a minimum viable product now, let’s install the regexcite package into your library via install():\n\ninstall()\n\n\n* checking for file ‘/tmp/RtmpHuKhho/regexcite/DESCRIPTION’ ... OK\n* preparing ‘regexcite’:\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\n* building ‘regexcite_0.0.0.9000.tar.gz’\nRunning /opt/R/4.2.2/lib/R/bin/R CMD INSTALL \\\n  /tmp/RtmpHuKhho/regexcite_0.0.0.9000.tar.gz --install-tests \n* installing to library ‘/home/runner/work/_temp/Library’\n* installing *source* package ‘regexcite’ ...\n** using staged installation\n** R\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (regexcite)\n\n\n\n\n\n\n\nRStudio\n\n\n\nRStudio exposes similar functionality in the Build menu and in the Build pane via Install and Restart, and in keyboard shortcuts Ctrl + Shift + B (Windows & Linux) or Cmd + Shift + B (macOS).\n\n\nNow we can attach and use regexcite like any other package. Let’s revisit our small example from the top. This is a good time to restart your R session and ensure you have a clean workspace.\n\nlibrary(regexcite)\n\nx <- \"alfa,bravo,charlie,delta\"\nstrsplit1(x, split = \",\")\n#> [1] \"alfa\"    \"bravo\"   \"charlie\" \"delta\"\n\nSuccess!"
  },
  {
    "objectID": "Whole-game.html#use_testthat",
    "href": "Whole-game.html#use_testthat",
    "title": "2  The whole game",
    "section": "\n2.16 use_testthat()\n",
    "text": "2.16 use_testthat()\n\nWe’ve tested strsplit1() informally, in a single example. We can formalize this as a unit test. This means we express a concrete expectation about the correct strsplit1() result for a specific input.\nFirst, we declare our intent to write unit tests and to use the testthat package for this, via use_testthat():\n\nuse_testthat()\n#> ✔ Adding 'testthat' to Suggests field in DESCRIPTION\n#> ✔ Setting Config/testthat/edition field in DESCRIPTION to '3'\n#> ✔ Creating 'tests/testthat/'\n#> ✔ Writing 'tests/testthat.R'\n#> • Call `use_test()` to initialize a basic test file and open it for editing.\n\nThis initializes the unit testing machinery for your package. It adds Suggests: testthat to DESCRIPTION, creates the directory tests/testthat/, and adds the script tests/testthat.R. You’ll notice that testthat is probably added with a minimum version of 3.0.0 and a second DESCRIPTION field, Config/testthat/edition: 3. We’ll talk more about those details in Chapter 13.\nHowever, it’s still up to YOU to write the actual tests!\nThe helper use_test() opens and/or creates a test file. You can provide the file’s basename or, if you are editing the relevant source file in RStudio, it will be automatically generated. For many of you, if R/strsplit1.R is the active file in RStudio, you can just call use_test(). However, since this book is built non-interactively, we must provide the basename explicitly:\n\nuse_test(\"strsplit1\")\n#> ✔ Writing 'tests/testthat/test-strsplit1.R'\n#> • Edit 'tests/testthat/test-strsplit1.R'\n\nThis creates the file tests/testthat/test-strsplit1.R. If it had already existed, use_test() would have just opened it. Put this content in:\n\ntest_that(\"strsplit1() splits a string\", {\n  expect_equal(strsplit1(\"a,b,c\", split = \",\"), c(\"a\", \"b\", \"c\"))\n})\n\nThis tests that strsplit1() gives the expected result when splitting a string.\nRun this test interactively, as you will when you write your own. Note you’ll have to attach testthat via library(testthat) in your R session first and you’ll probably want to load_all().\nGoing forward, your tests will mostly run en masse and at arm’s length via test():\n\n\ntest()\n#> ℹ Testing regexcite\n#> ✔ | F W S  OK | Context\n#> \n#> ⠏ |         0 | strsplit1                                           \n#> ✔ |         1 | strsplit1\n#> \n#> ══ Results ═════════════════════════════════════════════════════════\n#> [ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]\n\n\n\n\n\n\n\nRStudio\n\n\n\nRStudio exposes test() in the Build menu, in the Build pane via More > Test package, and in keyboard shortcuts Ctrl + Shift + T (Windows & Linux) or Cmd + Shift + T (macOS).\n\n\nYour tests are also run whenever you check() the package. In this way, you basically augment the standard checks with some of your own, that are specific to your package. It is a good idea to use the covr package to track what proportion of your package’s source code is exercised by the tests. More details can be found in Chapter 13."
  },
  {
    "objectID": "Whole-game.html#use_package",
    "href": "Whole-game.html#use_package",
    "title": "2  The whole game",
    "section": "\n2.17 use_package()\n",
    "text": "2.17 use_package()\n\nYou will inevitably want to use a function from another package in your own package. Just as we needed to export strsplit1(), we need to import functions from the namespace of other packages. If you plan to submit a package to CRAN, note that this even applies to functions in packages that you think of as “always available”, such as stats::median() or utils::head().\nOne common dilemma when using R’s regular expression functions is uncertainty about whether to request perl = TRUE or perl = FALSE. And then there are often, but not always, other arguments that alter how patterns are matched, such as fixed, ignore.case, and invert. It can be hard to keep track of which functions use which arguments and how the arguments interact, so many users never get to the point where they retain these details without rereading the docs.\nThe stringr package “provides a cohesive set of functions designed to make working with strings as easy as possible”. In particular, stringr uses one regular expression system everywhere (ICU regular expressions) and uses the same interface in every function for controlling matching behaviors, such as case sensitivity. Some people find this easier to internalize and program around. Let’s imagine you decide you’d rather build regexcite based on stringr (and stringi) than base R’s regular expression functions.\nFirst, declare your general intent to use some functions from the stringr namespace with use_package():\n\nuse_package(\"stringr\")\n#> ✔ Adding 'stringr' to Imports field in DESCRIPTION\n#> • Refer to functions with `stringr::fun()`\n\nThis adds the stringr package to the “Imports” section of DESCRIPTION. And that is all it does.\nLet’s revisit strsplit1() to make it more stringr-like. Here’s a new take on it:\n\nstr_split_one <- function(string, pattern, n = Inf) {\n  stopifnot(is.character(string), length(string) <= 1)\n  if (length(string) == 1) {\n    stringr::str_split(string = string, pattern = pattern, n = n)[[1]]\n  } else {\n    character()\n  }\n}\n\nNotice that we:\n\nRename the function to str_split_one(), to signal that that is a wrapper around stringr::str_split().\nAdopt the argument names from stringr::str_split(). Now we have string and pattern (and n), instead of x and split.\nIntroduce a bit of argument checking and edge case handling. This is unrelated to the switch to stringr and would be equally beneficial in the version built on strsplit().\nUse the package::function() form when calling stringr::str_split(). This specifies that we want to call the str_split() function from the stringr namespace. There is more than one way to call a function from another package and the one we endorse here is explained fully in Section 10.4.\n\nWhere should we write this new function definition? I’d like to keep following the convention where we name the .R file after the function it defines, so now we need to do some fiddly file shuffling. Because this comes up fairly often in real life, we have the rename_files() function, which choreographs the renaming of a file in R/ and its associated companion files below test/.\n\nrename_files(\"strsplit1\", \"str_split_one\")\n#> ✔ Moving 'R/strsplit1.R' to 'R/str_split_one.R'\n#> ✔ Moving 'tests/testthat/test-strsplit1.R' to 'tests/testthat/test-str_split_one.R'\n\nRemember: the file name work is purely aspirational. We still need to update the contents of these files!\nHere are the updated contents of R/str_split_one.R. In addition to changing the function definition, we’ve also updated the roxygen header to reflect the new arguments and to include examples that show off the stringr features.\n\n#' Split a string\n#'\n#' @param string A character vector with, at most, one element.\n#' @inheritParams stringr::str_split\n#'\n#' @return A character vector.\n#' @export\n#'\n#' @examples\n#' x <- \"alfa,bravo,charlie,delta\"\n#' str_split_one(x, pattern = \",\")\n#' str_split_one(x, pattern = \",\", n = 2)\n#'\n#' y <- \"192.168.0.1\"\n#' str_split_one(y, pattern = stringr::fixed(\".\"))\nstr_split_one <- function(string, pattern, n = Inf) {\n  stopifnot(is.character(string), length(string) <= 1)\n  if (length(string) == 1) {\n    stringr::str_split(string = string, pattern = pattern, n = n)[[1]]\n  } else {\n    character()\n  }\n}\n\nDon’t forget to also update the test file!\nHere are the updated contents of tests/testthat/test-str_split_one.R. In addition to the change in the function’s name and arguments, we’ve added a couple more tests.\n\ntest_that(\"str_split_one() splits a string\", {\n  expect_equal(str_split_one(\"a,b,c\", \",\"), c(\"a\", \"b\", \"c\"))\n})\n\ntest_that(\"str_split_one() errors if input length > 1\", {\n  expect_error(str_split_one(c(\"a,b\",\"c,d\"), \",\"))\n})\n\ntest_that(\"str_split_one() exposes features of stringr::str_split()\", {\n  expect_equal(str_split_one(\"a,b,c\", \",\", n = 2), c(\"a\", \"b,c\"))\n  expect_equal(str_split_one(\"a.b\", stringr::fixed(\".\")), c(\"a\", \"b\"))\n})\n\nBefore we take the new str_split_one() out for a test drive, we need to call document(). Why? Remember that document() does two main jobs:\n\nConverts our roxygen comments into proper R documentation.\n(Re)generates NAMESPACE.\n\nThe second point is especially important here, since we will no longer export strsplit1() and we will newly export str_split_one(). Don’t be dismayed by the warning about \"Objects listed as exports, but not present in namespace: strsplit1\". That always happens when you remove something from the namespace.\n\ndocument()\n#> ℹ Updating regexcite documentation\n#> ℹ Loading regexcite\n#> Warning: Objects listed as exports, but not present in namespace:\n#> • strsplit1\n#> Writing 'NAMESPACE'\n#> Writing 'str_split_one.Rd'\n#> Deleting 'strsplit1.Rd'\n\nTry out the new str_split_one() function by simulating package installation via load_all():\n\nload_all()\n#> ℹ Loading regexcite\nstr_split_one(\"a, b, c\", pattern = \", \")\n#> [1] \"a\" \"b\" \"c\""
  },
  {
    "objectID": "Whole-game.html#use_github",
    "href": "Whole-game.html#use_github",
    "title": "2  The whole game",
    "section": "\n2.18 use_github()\n",
    "text": "2.18 use_github()\n\nYou’ve seen us making commits during the development process for regexcite. You can see an indicative history at https://github.com/jennybc/regexcite. Our use of version control and the decision to expose the development process means you can inspect the state of the regexcite source at each developmental stage. By looking at so-called diffs, you can see exactly how each devtools helper function modifies the source files that constitute the regexcite package.\nHow would you connect your local regexcite package and Git repository to a companion repository on GitHub?\n\n\nuse_github() is a helper that we recommend for the long-term. We won’t demonstrate it here because it requires some credential setup on your end. We also don’t want to tear down and rebuild the public regexcite package every time we build this book.\nSet up the GitHub repo first! It sounds counter-intuitive, but the easiest way to get your work onto GitHub is to initiate there, then use RStudio to start working in a synced local copy. This approach is described in Happy Git’s workflows New project, GitHub first and Existing project, GitHub first.\nCommand line Git can always be used to add a remote repository post hoc. This is described in the Happy Git workflow Existing project, GitHub last.\n\nAny of these approaches will connect your local regexcite project to a GitHub repo, public or private, which you can push to or pull from using the Git client built into RStudio."
  },
  {
    "objectID": "Whole-game.html#use_readme_rmd",
    "href": "Whole-game.html#use_readme_rmd",
    "title": "2  The whole game",
    "section": "\n2.19 use_readme_rmd()\n",
    "text": "2.19 use_readme_rmd()\n\nNow that your package is on GitHub, the README.md file matters. It is the package’s home page and welcome mat, at least until you decide to give it a website (see pkgdown), add a vignette (see Chapter 17), or submit it to CRAN (see Chapter 22).\nThe use_readme_rmd() function initializes a basic, executable README.Rmd ready for you to edit:\n\nuse_readme_rmd()\n#> ✔ Writing 'README.Rmd'\n#> ✔ Adding '^README\\\\.Rmd$' to '.Rbuildignore'\n#> • Update 'README.Rmd' to include installation instructions.\n#> ✔ Writing '.git/hooks/pre-commit'\n\nIn addition to creating README.Rmd, this adds some lines to .Rbuildignore, and creates a Git pre-commit hook to help you keep README.Rmd and README.md in sync.\nREADME.Rmd already has sections that prompt you to:\n\nDescribe the purpose of the package.\nProvide installation instructions. If a GitHub remote is detected when use_readme_rmd() is called, this section is pre-filled with instructions on how to install from GitHub.\nShow a bit of usage.\n\nHow to populate this skeleton? Copy stuff liberally from DESCRIPTION and any formal and informal tests or examples you have. Anything is better than nothing. Otherwise … do you expect people to install your package and comb through individual help files to figure out how to use it? They probably won’t.\nWe like to write the README in R Markdown, so it can feature actual usage. The inclusion of live code also makes it less likely that your README grows stale and out-of-sync with your actual package.\nIf RStudio has not already done so, open README.Rmd for editing. Make sure it shows some usage of str_split_one().\nThe README.Rmd we use is here: README.Rmd and here’s what it contains:\n\n---\noutput: github_document\n---\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n```{r, include = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  fig.path = \"man/figures/README-\",\n  out.width = \"100%\"\n)\n```\n\n**NOTE: This is a toy package created for expository purposes, for the second edition of [R Packages](https://r-pkgs.org). It is not meant to actually be useful. If you want a package for factor handling, please see [stringr](https://stringr.tidyverse.org), [stringi](https://stringi.gagolewski.com/),\n[rex](https://cran.r-project.org/package=rex), and\n[rematch2](https://cran.r-project.org/package=rematch2).**\n\n# regexcite\n\n<!-- badges: start -->\n<!-- badges: end -->\n\nThe goal of regexcite is to make regular expressions more exciting!\nIt provides convenience functions to make some common tasks with string manipulation and regular expressions a bit easier.\n\n## Installation\n\nYou can install the development version of regexcite from [GitHub](https://github.com/) with:\n      \n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"jennybc/regexcite\")\n```\n\n## Usage\n\nA fairly common task when dealing with strings is the need to split a single string into many parts.\nThis is what `base::strplit()` and `stringr::str_split()` do.\n\n```{r}\n(x <- \"alfa,bravo,charlie,delta\")\nstrsplit(x, split = \",\")\nstringr::str_split(x, pattern = \",\")\n```\n\nNotice how the return value is a **list** of length one, where the first element holds the character vector of parts.\nOften the shape of this output is inconvenient, i.e. we want the un-listed version.\n\nThat's exactly what `regexcite::str_split_one()` does.\n\n```{r}\nlibrary(regexcite)\n\nstr_split_one(x, pattern = \",\")\n```\n\nUse `str_split_one()` when the input is known to be a single string.\nFor safety, it will error if its input has length greater than one.\n\n`str_split_one()` is built on `stringr::str_split()`, so you can use its `n` argument and stringr's general interface for describing the `pattern` to be matched.\n\n```{r}\nstr_split_one(x, pattern = \",\", n = 2)\n\ny <- \"192.168.0.1\"\nstr_split_one(y, pattern = stringr::fixed(\".\"))\n```\n\nDon’t forget to render it to make README.md! The pre-commit hook should remind you if you try to commit README.Rmd, but not README.md, and also when README.md appears to be out-of-date.\nThe very best way to render README.Rmd is with build_readme(), because it takes care to render with the most current version of your package, i.e. it installs a temporary copy from the current source.\n\nbuild_readme()\n#> ℹ Installing regexcite in temporary library\n#> ℹ Building '/tmp/RtmpHuKhho/regexcite/README.Rmd'\n\nYou can see the rendered README.md simply by visiting regexcite on GitHub.\nFinally, don’t forget to do one last commit. And push, if you’re using GitHub."
  },
  {
    "objectID": "Whole-game.html#the-end-check-and-install",
    "href": "Whole-game.html#the-end-check-and-install",
    "title": "2  The whole game",
    "section": "\n2.20 The end: check() and install()\n",
    "text": "2.20 The end: check() and install()\n\nLet’s run check() again to make sure all is still well.\n\ncheck()\n\n\n── R CMD check results ─────────────────── regexcite 0.0.0.9000 ────\nDuration: 24.3s\n\n0 errors ✔ | 0 warnings ✔ | 0 notes ✔\n\nregexcite should have no errors, warnings or notes. This would be a good time to re-build and install it properly. And celebrate!\n\ninstall()\n\n\n* checking for file ‘/tmp/RtmpHuKhho/regexcite/DESCRIPTION’ ... OK\n* preparing ‘regexcite’:\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\nRemoved empty directory ‘regexcite/tests/testthat/_snaps’\n* building ‘regexcite_0.0.0.9000.tar.gz’\nRunning /opt/R/4.2.2/lib/R/bin/R CMD INSTALL \\\n  /tmp/RtmpHuKhho/regexcite_0.0.0.9000.tar.gz --install-tests \n* installing to library ‘/home/runner/work/_temp/Library’\n* installing *source* package ‘regexcite’ ...\n** using staged installation\n** R\n** tests\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (regexcite)\n\nFeel free to visit the regexcite package on GitHub, which is exactly as developed here. The commit history reflects each individual step, so use the diffs to see the addition and modification of files, as the package evolved. The rest of this book goes in greater detail for each step you’ve seen here and much more."
  },
  {
    "objectID": "Setup.html#setup-prep",
    "href": "Setup.html#setup-prep",
    "title": "3  System setup",
    "section": "\n3.1 Prepare your system",
    "text": "3.1 Prepare your system\nTo get started, make sure you have the latest version of R (at least 4.2.2, which is the version being used to render this book), then run the following code to get the packages you’ll need:\n\ninstall.packages(c(\"devtools\", \"roxygen2\", \"testthat\", \"knitr\"))\n\nMake sure you have a recent version of the RStudio integrated development environment (IDE). In fact, consider using the preview version and updating regularly. Compared to the official released version, the preview gives you access to the latest and greatest features and only slightly increases your chances of finding a bug. It is distinct from the more volatile daily build.\n\nPreview version: https://www.rstudio.com/products/rstudio/download/preview/\n\nReleased version: https://www.rstudio.com/products/rstudio/download/\n\nMost readers can use the free, open source version of RStudio Desktop."
  },
  {
    "objectID": "Setup.html#sec-setup-usage",
    "href": "Setup.html#sec-setup-usage",
    "title": "3  System setup",
    "section": "\n3.2 devtools, usethis, and you",
    "text": "3.2 devtools, usethis, and you\n\n“I am large, I contain multitudes.”\n— Walt Whitman, Song of Myself\n\nAfter 7 years of development, devtools had grown into a rather unwieldy package, making maintenance difficult. Version 2.0.0, released in late 2018, marked the conscious uncoupling of devtools, with most functionality moving into seven smaller packages. Through various means, devtools continues to expose all its usual functionality, although it is mostly maintained elsewhere. For example, devtools might provide a wrapper function in order to set user-friendly defaults, introduce helpful interactive behaviour, or to combine functionality from multiple sub-packages.\nWhat’s our recommended approach to devtools and its constituent packages? It varies, depending on whether you’re working in useR or developeR mode:\n\nFor interactive use, useRs should attach devtools and think of it as the provider of your favorite functions for package development.\nFor programmatic use, such as inside another package, developeRs should NOT depend on devtools, but should instead access functions via the package that is their primary home.\n\ndevtools should rarely appear in the role of pkg in a qualified call of the form pkg::fcn(). Instead, pkg should be the package where fcn() is defined.\nAn exception to this is that we continue to feature devtools::install_github() as the way to install the development version of a package in its README, even though install_github() actually lives in the remotes package. That’s because this piece of advice pertains to interactive use, where we prefer to emphasize devtools.\n\n\nTry to report bugs on the package that is a function’s primary home. The help for devtools::fcn() usually states when devtools is re-exporting a function from another package.\n\nExample of how to simulate installing and loading a package, during interactive development:\n\nlibrary(devtools)\nload_all()\n\nIf that same functionality is used inside an R package, this is the preferred call:\n\npkgload::load_all()\n\nThe usethis package is the one constituent package that more people may be aware of and that they may use directly. It now holds the functions that act on the files and folders in an R project, most especially for any project that is also an R package. All functions in usethis are made available by devtools. So, once you attach devtools, you can use any function in usethis without qualification, i.e. just call use_testthat(). If you choose to specify the namespace, such as when working in a more programmatic style, then access usethis functions directly: do usethis::use_testthat() instead of devtools::use_testthat().\n\n3.2.1 Personal startup configuration\nYou can attach devtools like so:\n\nlibrary(devtools)\n\nBut it soon grows aggravating to repeatedly attach devtools in every R session. Therefore, we strongly recommend attaching devtools in your .Rprofile startup file, like so:\n\nif (interactive()) {\n  suppressMessages(require(devtools))\n}\n\nFor convenience, the function use_devtools() creates .Rprofile, if needed, opens it for editing, and puts the necessary lines of code on the clipboard and the screen. Another package you may want to handle this way is testthat.\n\n\n\n\n\n\nWarning\n\n\n\nIn general, it’s a bad idea to attach packages in .Rprofile, as it invites you to create R scripts that don’t reflect all of their dependencies via explicit calls to library(foo). But devtools is a workflow package that smooths the process of package development and is, therefore, unlikely to get baked into any analysis scripts. Note how we still take care to only attach in interactive sessions.\n\n\nusethis consults certain options when, for example, creating R packages de novo. This allows you to specify personal defaults for yourself as a package maintainer or for your preferred license. Here’s an example of a code snippet that could go in .Rprofile:\n\noptions(\n  usethis.full_name = \"Jane Doe\",\n  usethis.description = list(\n    `Authors@R` = 'person(\"Jane\", \"Doe\", email = \"jane@example.com\", role = c(\"aut\", \"cre\"), \n    comment = c(ORCID = \"YOUR-ORCID-ID\"))',\n    License = \"MIT + file LICENSE\",\n    Version = \"0.0.0.9000\"\n  ),\n  usethis.protocol  = \"ssh\"  \n)\n\nThe following code installs the development versions of devtools and usethis, which may be important during the revision of the book.\n\ndevtools::install_github(\"r-lib/devtools\")\ndevtools::install_github(\"r-lib/usethis\")"
  },
  {
    "objectID": "Setup.html#setup-tools",
    "href": "Setup.html#setup-tools",
    "title": "3  System setup",
    "section": "\n3.3 R build toolchain",
    "text": "3.3 R build toolchain\nTo be fully capable of building R packages from source, you’ll also need a compiler and a few other command line tools. This may not be strictly necessary until you want to build packages containing C or C++ code. Especially if you are using RStudio, you can set this aside for now. The IDE will alert you and provide support once you try to do something that requires you to setup your development environment. Read on for advice on doing this yourself.\n\n3.3.1 Windows\nOn Windows the collection of tools needed for building packages from source is called Rtools.\nRtools is NOT an R package. It is NOT installed with install.packages(). Instead, download it from https://cran.r-project.org/bin/windows/Rtools/ and run the installer.\nDuring the Rtools installation you may see a window asking you to “Select Additional Tasks”.\n\nDo not select the box for “Edit the system PATH”. devtools and RStudio should put Rtools on the PATH automatically when it is needed.\nDo select the box for “Save version information to registry”. It should be selected by default.\n\n3.3.2 macOS\nYou need to install the Xcode command line tools, which requires that you register as an Apple developer (don’t worry, it’s free).\nThen, in the shell, do:\nxcode-select --install\nAlternatively, you can install the current release of full Xcode from the Mac App Store. This includes a very great deal that you do not need, but it offers the advantage of App Store convenience.\n\n3.3.3 Linux\nMake sure you’ve installed not only R, but also the R development tools. For example, on Ubuntu (and Debian) you need to install the r-base-dev package.\n\n3.3.4 Verify system prep\nYou can request a “(package) development situation report” with devtools::dev_sitrep():\n\ndevtools::dev_sitrep()\n#> ── R ───────────────────────────────────────────────────────────────────────\n#> • version: 4.1.2\n#> • path: '/Library/Frameworks/R.framework/Versions/4.1/Resources/'\n#> ── RStudio ─────────────────────────────────────────────────────────────────\n#> • version: 2022.2.0.443\n#> ── devtools ────────────────────────────────────────────────────────────────\n#> • version: 2.4.3.9000\n#> • devtools or its dependencies out of date:\n#>   'gitcreds', 'gh'\n#>   Update them with `devtools::update_packages(\"devtools\")`\n#> ── dev package ─────────────────────────────────────────────────────────────\n#> • package: 'rpkgs'\n#> • path: '/Users/jenny/rrr/r-pkgs/'\n#> • rpkgs dependencies out of date:\n#>   'gitcreds', 'generics', 'tidyselect', 'dplyr', 'tidyr', 'broom', 'gh'\n#>  Update them with `devtools::install_dev_deps()`\n\nIf this reveals that certain tools or packages are missing or out-of-date, you are encouraged to update them."
  },
  {
    "objectID": "Structure.html#introduction",
    "href": "Structure.html#introduction",
    "title": "4  Package structure and state",
    "section": "\n4.1 Introduction",
    "text": "4.1 Introduction\nThis chapter will start you on the road to package development by converting the implicit knowledge you’ve gained from using R packages into the explicit knowledge needed to create and modify them. You’ll learn about the various states a package can be in and the difference between a package and library (and why you should care)."
  },
  {
    "objectID": "Structure.html#sec-package-states",
    "href": "Structure.html#sec-package-states",
    "title": "4  Package structure and state",
    "section": "\n4.2 Package states",
    "text": "4.2 Package states\nWhen you create or modify a package, you work on its “source code” or “source files”. You interact with the in-development package in its source form. This is NOT the package form you are most familiar with from day-to-day usage. Package development workflows make much more sense if you understand the five states an R package can be in:\n\nsource\nbundled\nbinary\ninstalled\nin-memory\n\nYou already know some of the functions that put packages into these states. For example, install.packages() can move a package from the source, bundled, or binary states into the installed state. devtools::install_github() takes a source package on GitHub and moves it into the installed state. The library() function loads an installed package into memory, making it available for immediate and direct use."
  },
  {
    "objectID": "Structure.html#sec-source-package",
    "href": "Structure.html#sec-source-package",
    "title": "4  Package structure and state",
    "section": "\n4.3 Source package",
    "text": "4.3 Source package\nA source package is just a directory of files with a specific structure. It includes particular components, such as a DESCRIPTION file, an R/ directory containing .R files, and so on. Most of the remaining chapters in this book are dedicated to detailing these components.\nIf you are new to package development, you may have never seen a package in source form! You might not even have any source packages on your computer. The easiest way to see a package in source form right away is to browse around its code on the web.\nMany R packages are developed in the open on GitHub (or GitLab or similar). The best case scenario is that you visit the package’s CRAN landing page, e.g.:\n\nforcats: https://cran.r-project.org/package=forcats\n\nreadxl: https://cran.r-project.org/package=readxl\n\n\nand one of its URLs links to a repository on a public hosting service, e.g.:\n\nforcats: https://github.com/tidyverse/forcats\n\nreadxl: https://github.com/tidyverse/readxl\n\n\nSome maintainers forget to list this URL, even though the package is developed in a public repository, but you still might be able to discover it via search.\nEven if a package is not developed on a public platform, you can visit its source in the unofficial, read-only mirror maintained by R-hub. Examples:\n\nMASS: https://github.com/cran/MASS\n\ncar: https://github.com/cran/car\n\n\nNote that exploring a package’s source and history within the cran GitHub organisation is not the same as exploring the package’s true development venue, because this source and its evolution is just reverse-engineered from the package’s CRAN releases. This presents a redacted view of the package and its history, but, by definition, it includes everything that is essential."
  },
  {
    "objectID": "Structure.html#bundled-package",
    "href": "Structure.html#bundled-package",
    "title": "4  Package structure and state",
    "section": "\n4.4 Bundled package",
    "text": "4.4 Bundled package\nA bundled package is a package that’s been compressed into a single file. By convention (from Linux), package bundles in R use the extension .tar.gz and are sometimes referred to as “source tarballs”. This means that multiple files have been reduced to a single file (.tar) and then compressed using gzip (.gz). While a bundle is not that useful on its own, it’s a platform-agnostic, transportation-friendly intermediary between a source package and an installed package.\nIn the rare case that you need to make a bundle from a package you’re developing locally, use devtools::build(). Under the hood, this calls pkgbuild::build() and, ultimately, R CMD build, which is described further in the Building package tarballs section of Writing R Extensions.\nThis should tip you off that a package bundle or “source tarball” is not simply the result of making a tar archive of the source files, then compressing with gzip. By convention, in the R world, a few more operations are carried out when making the .tar.gz file and this is why we’ve elected to refer to this as a package bundle.\nEvery CRAN package is available in bundled form, via the “Package source” field of individual landing pages. Continuing our examples from above, you could download the bundles forcats_0.4.0.tar.gz and readxl_1.3.1.tar.gz (or whatever the current versions may be). You could unpack such a bundle in the shell (not the R console) like so:\ntar xvf forcats_0.4.0.tar.gz\nIf you decompress a bundle, you’ll see it looks almost the same as a source package. This diagram summarises the files present in the top-level directory for source, bundled, and binary versions of devtools.\nTODO: Remake this figure https://github.com/hadley/r-pkgs/issues/587.\n\n\n\n\nFigure 4.1: Side-by-side comparison of source, bundled, and binary package.\n\n\n\n\nThe main differences between a source package and an uncompressed bundle are:\n\nVignettes have been built, so rendered outputs, such as HTML, appear below inst/doc/ and a vignette index appears in the build/ directory, usually alongside a PDF package manual.\nA local source package might contain temporary files used to save time during development, like compilation artefacts in src/. These are never found in a bundle.\nAny files listed in .Rbuildignore are not included in the bundle. These are typically files that facilitate your development process, but that should be excluded from the distributed product.\n\n\n4.4.1 .Rbuildignore\n\nYou won’t need to contemplate the exact structure of package .tar.gz files very often, but you do need to understand the .Rbuildignore file. It controls which files from the source package make it into the downstream forms.\nEach line of .Rbuildignore is a Perl-compatible regular expression that is matched, without regard to case, against the path to each file in the source package 1. If the regular expression matches, that file or directory is excluded. Note there are some default exclusions implemented by R itself, mostly relating to classic version control systems and editors, such as SVN, Git, and Emacs.\nWe usually modify .Rbuildignore with the usethis::use_build_ignore() function, which takes care of easy-to-forget details, such as regular expression anchoring and escaping. To exclude a specific file or directory (the most common use case), you MUST anchor the regular expression. For example, to exclude a directory called “notes”, the .Rbuildignore entry must be ^notes$, whereas the unanchored regular expression notes will match any file name containing “notes”, e.g. R/notes.R, man/important-notes.R, data/endnotes.Rdata, etc. We find that use_build_ignore() helps us get more of our .Rbuildignore entries right the first time.\n.Rbuildignore is a way to resolve some of the tension between the practices that support your development process and CRAN’s requirements for submission and distribution. Even if you aren’t planning to release on CRAN, following these conventions will allow you to make the best use of R’s built-in tooling for package checking and installation. The affected files fall into two broad, semi-overlapping classes:\n\nFiles that help you generate package contents programmatically. Examples:\n\nUsing README.Rmd to generate an informative and current README.md.\nStoring .R scripts to create and update internal or exported data.\n\n\nFiles that drive package development, checking, and documentation, outside of CRAN’s purview. Examples:\n\nFiles relating to the RStudio IDE.\nUsing the pkgdown package to generate a website.\nConfiguration files related to continuous integration/deployment and monitoring test coverage.\n\n\n\nHere is a non-exhaustive list of typical entries in the .Rbuildignore file for a package in the tidyverse:\n^.*\\.Rproj$         # Designates the directory as an RStudio Project\n^\\.Rproj\\.user$     # Used by RStudio for temporary files\n^README\\.Rmd$       # An Rmd file used to generate README.md\n^LICENSE\\.md$       # Full text of the license\n^cran-comments\\.md$ # Comments for CRAN submission\n^data-raw$          # Code used to create data included in the package\n^pkgdown$           # Resources used for the package website\n^_pkgdown\\.yml$     # Configuration info for the package website\n^\\.github$          # Contributing guidelines, CoC, issue templates, etc.\nNote that the comments above must not appear in an actual .Rbuildignore file; they are included here only for exposition.\nWe’ll mention when you need to add files to .Rbuildignore whenever it’s important. Remember that usethis::use_build_ignore() is an attractive way to manage this file. Furthermore, many usethis functions that add a file that should be listed in .Rbuildignore take care of this automatically. For example, use_read_rmd() adds “^README\\.Rmd$” to .Rbuildignore."
  },
  {
    "objectID": "Structure.html#sec-structure-binary",
    "href": "Structure.html#sec-structure-binary",
    "title": "4  Package structure and state",
    "section": "\n4.5 Binary package",
    "text": "4.5 Binary package\nIf you want to distribute your package to an R user who doesn’t have package development tools, you’ll need to provide a binary package. The primary maker and distributor of binary packages is CRAN, not individual maintainers. But even if you delegate the responsibility of distributing your package to CRAN, it’s still important for a maintainer to understand the nature of a binary package.\nLike a package bundle, a binary package is a single file. Unlike a bundled package, a binary package is platform specific and there are two basic flavors: Windows and macOS. (Linux users are generally required to have the tools necessary to install from .tar.gz files, although the emergence of resources like RStudio Public Package Manager is giving Linux users the same access to binary packages as their colleagues on Windows and macOS.)\nBinary packages for macOS are stored as .tgz, whereas Windows binary packages end in .zip. If you need to make a binary package, use devtools::build(binary = TRUE) on the relevant operating system. Under the hood, this calls pkgbuild::build(binary = TRUE) and, ultimately, R CMD INSTALL --build, which is described further in the Building binary packages section of Writing R Extensions.\nYou submit a package bundle and CRAN makes and distributes the package binaries.\nCRAN packages are usually available in binary form, for both macOS and Windows, for the current, previous, and (possibly) development versions of R. Continuing our examples from above, you could download binary packages such as:\n\nforcats for macOS: forcats_0.4.0.tgz\n\nreadxl for Windows: readxl_1.3.1.zip\n\n\nand this is, indeed, part of what’s usually going on behind the scenes when you call install.packages().\nIf you uncompress a binary package, you’ll see that the internal structure is rather different from a source or bundled package. Figure 4.1 includes this comparison. Here are some of the most notable differences:\n\nThere are no .R files in the R/ directory - instead there are three files that store the parsed functions in an efficient file format. This is basically the result of loading all the R code and then saving the functions with save(). (In the process, this adds a little extra metadata to make things as fast as possible).\nA Meta/ directory contains a number of .rds files. These files contain cached metadata about the package, like what topics the help files cover and a parsed version of the DESCRIPTION file. (You can use readRDS() to see exactly what’s in those files). These files make package loading faster by caching costly computations.\nThe actual help content appears in help/ and html/ (no longer in man/).\nIf you had any code in the src/ directory, there will now be a libs/ directory that contains the results of compiling the code. On Windows, there are subdirectories for 32 bit (i386/) and 64 bit (x64/) environments.\nIf you had any objects in data/, they have now been converted into a more efficient form.\nThe contents of inst/ are moved to the top-level directory. For example, vignette files are now in doc/.\nSome files and folders have been dropped, such as README, build/, tests/, and vignettes/."
  },
  {
    "objectID": "Structure.html#sec-installed-package",
    "href": "Structure.html#sec-installed-package",
    "title": "4  Package structure and state",
    "section": "\n4.6 Installed package",
    "text": "4.6 Installed package\nAn installed package is a binary package that’s been decompressed into a package library (described in Section 4.8). The following diagram illustrates the many ways a package can be installed. This diagram is complicated! In an ideal world, installing a package would involve stringing together a set of simple steps: source -> bundle, bundle -> binary, binary -> installed. In the real world, it’s not this simple because there are often (faster) shortcuts available.\n\n\n\n\n\nFigure 4.2: Many methods for converting between package states.\n\n\n\n\nThe built-in command line tool R CMD INSTALL powers all package installation. It can install a package from source files, a bundle (a.k.a. a source tarball), or a binary package. Details are available in the Installing packages section of R Installation and Administration. Just like with devtools::build(), devtools provides a wrapper function, devtools::install(), that makes this tool available from within an R session.\nMost useRs understandably like to install packages from the comfort of an R session and directly from CRAN. The built-in function install.packages() meets this need. It can download the package, in various forms, install it, and optionally attend to the installation of dependencies.\nThere is a price, however, for the convenience of installing R packages from within an R session. As you might expect, it can be a bit tricky to re-install a package that is already in use in the current session. This actually works most of the time, but sometimes it does not, especially when installing an R package with compiled code on Windows. Due to how file handles are locked on Windows, an attempt to install a new version of a package that’s in use can result in a corrupt installation where the package’s R code has been updated, but its compiled code has not. When troubleshooting, Windows users should strive to install packages in a clean R session, with as few packages loaded as possible.\ndevtools exposes a family of install_*() functions to address some needs beyond the reach of install.packages() or to make existing capabilities easier to access. These functions are actually maintained in the remotes package and are re-exported by devtools.\n\nlibrary(remotes)\n\nfuns <- as.character(lsf.str(\"package:remotes\"))\ngrep(\"^install_.+\", funs, value = TRUE)\n#>  [1] \"install_bioc\"      \"install_bitbucket\" \"install_cran\"     \n#>  [4] \"install_deps\"      \"install_dev\"       \"install_git\"      \n#>  [7] \"install_github\"    \"install_gitlab\"    \"install_local\"    \n#> [10] \"install_remote\"    \"install_svn\"       \"install_url\"      \n#> [13] \"install_version\"\n\ninstall_github() is the flagship example of a sub-family of functions that can download a package from a remote location that is not CRAN and do whatever is necessary to install it. The rest of the devtools/remotes install_*() functions are aimed at making things that are technically possible with base tooling a bit easier or more explicit, such as install_version() which installs a specific version of a CRAN package.\n2022-09-01 note: The pak package (https://pak.r-lib.org) is gradually becoming our recommended tool for package installation and, most especially, for installing packages directly from GitHub. I.e. it is replacing remotes::install_github() It is likely that this recommendation will become official before the second edition of R Packages is published.\n\nAnalogous to .Rbuildignore, described in section Section 4.4.1, .Rinstignore lets you keep files present in a package bundle out of the installed package. However, in contrast to .Rbuildignore, this is rather obscure and rarely needed."
  },
  {
    "objectID": "Structure.html#in-memory-package",
    "href": "Structure.html#in-memory-package",
    "title": "4  Package structure and state",
    "section": "\n4.7 In-memory package",
    "text": "4.7 In-memory package\nWe finally arrive at a command familiar to everyone who uses R:\n\nlibrary(usethis)\n\nAssuming usethis is installed, this call makes its functions available for use, i.e. now we can do:\n\ncreate_package(\"/path/to/my/coolpackage\")\n\nThe usethis package has been loaded into memory and, in fact, has also been attached to the search path. The distinction between loading and attaching packages is not important when you’re writing scripts, but it’s very important when you’re writing packages. You’ll learn more about the difference and why it’s important in Section 11.5.2.\nlibrary() is not a great way to iteratively tweak and test drive a package you’re developing, because it only works for an installed package. In Section 5.5, you’ll learn how devtools::load_all() accelerates development by allowing you to load a source package directly into memory."
  },
  {
    "objectID": "Structure.html#sec-library",
    "href": "Structure.html#sec-library",
    "title": "4  Package structure and state",
    "section": "\n4.8 Package libraries",
    "text": "4.8 Package libraries\nWe just discussed the library() function, whose name is inspired by what it does. When you call library(pkg), R looks through the current libraries for an installed package named “pkg” and, if successful, it makes foo available for use.\nIn R, a library is a directory containing installed packages, sort of like a library for books. Unfortunately, in the R world, you will frequently encounter confused usage of the words “library” and “package”. It’s common for someone to refer to dplyr, for example, as a library when it is actually a package. There are a few reasons for the confusion. First, R’s terminology arguably runs counter to broader programming conventions, where the usual meaning of “library” is closer to what we mean by “package”. The name of the library() function itself probably reinforces the wrong associations. Finally, this vocabulary error is often harmless, so it’s easy for R users to fall into the wrong habit and for people who point out this mistake to look like insufferable pedants. But here’s the bottom line:\n\nWe use the library() function to load 2 a package.\n\nThe distinction between the two is important and useful as you get involved in package development.\nYou can have multiple libraries on your computer. In fact, many of you already do, especially if you’re on Windows. You can use .libPaths() to see which libraries are currently active. Here’s how this might look on Windows:\n\n# on Windows\n.libPaths()\n#> [1] \"C:/Users/jenny/Documents/R/win-library/3.6\"\n#> [2] \"C:/Program Files/R/R-3.6.0/library\"\n\nlapply(.libPaths(), list.dirs, recursive = FALSE, full.names = FALSE)\n#> [[1]]\n#>   [1] \"abc\"           \"anytime\"       \"askpass\"       \"assertthat\"   \n#>  ...\n#> [145] \"zeallot\"      \n#> \n#> [[2]]\n#>  [1] \"base\"         \"boot\"         \"class\"        \"cluster\"     \n#>  [5] \"codetools\"    \"compiler\"     \"datasets\"     \"foreign\"     \n#>  [9] \"graphics\"     \"grDevices\"    \"grid\"         \"KernSmooth\"  \n#> [13] \"lattice\"      \"MASS\"         \"Matrix\"       \"methods\"     \n#> [17] \"mgcv\"         \"nlme\"         \"nnet\"         \"parallel\"    \n#> [21] \"rpart\"        \"spatial\"      \"splines\"      \"stats\"       \n#> [25] \"stats4\"       \"survival\"     \"tcltk\"        \"tools\"       \n#> [29] \"translations\" \"utils\"\n\nHere’s a similar look on macOS (but your results may vary):\n\n# on macOS\n.libPaths()\n#> [1] \"/Users/jenny/Library/R/3.6/library\"\n#> [2] \"/Library/Frameworks/R.framework/Versions/3.6/Resources/library\"\n\nlapply(.libPaths(), list.dirs, recursive = FALSE, full.names = FALSE)\n#> [[1]]\n#>    [1] \"abc\"                  \"abc.data\"             \"abind\"                \n#>  ...\n#> [1033] \"Zelig\"                \"zip\"                  \"zoo\"                 \n#> \n#> [[2]]\n#>  [1] \"base\"         \"boot\"         \"class\"        \"cluster\"     \n#>  [5] \"codetools\"    \"compiler\"     \"datasets\"     \"foreign\"     \n#>  [9] \"graphics\"     \"grDevices\"    \"grid\"         \"KernSmooth\"  \n#> [13] \"lattice\"      \"MASS\"         \"Matrix\"       \"methods\"     \n#> [17] \"mgcv\"         \"nlme\"         \"nnet\"         \"parallel\"    \n#> [21] \"rpart\"        \"spatial\"      \"splines\"      \"stats\"       \n#> [25] \"stats4\"       \"survival\"     \"tcltk\"        \"tools\"       \n#> [29] \"translations\" \"utils\"\n\nIn both cases we see two active libraries, consulted in this order:\n\nA user library\nA system-level or global library\n\nThis setup is typical on Windows, but is something you usually need to opt into on macOS 3. With this setup, add-on packages installed from CRAN (or elsewhere) or under local development are kept in the user library. Above, the macOS system is used as a primary development machine and has many packages here (~1000), whereas the Windows system is only used occasionally and is much more spartan. The core set of base and recommended packages that ship with R live in the system-level library and are the same on macOS and Windows. This separation appeals to many developers and makes it easy to, for example, clean out your add-on packages without disturbing your base R installation.\nIf you’re on macOS and only see one library, there is no urgent need to change anything. But next time you upgrade R, consider creating a user-level library. By default, R looks for a user library found at the path stored in the environment variable R_LIBS_USER, which itself defaults to ~/Library/R/x.y/library. When you install, R x.y.z and prior to installing any add-on packages, use dir.create(\"~/Library/R/x.y/library\") to set up a user library. Now you will have the library setup seen above. Alternatively, you could setup a user library elsewhere and tell R about that by setting the R_LIBS_USER environment variable in .Renviron.\nThe filepaths for these libraries also make it clear they are associated with a specific version of R (3.6.x at the time of writing), which is also typical. This reflects and enforces the fact that you need to reinstall your add-on packages when you update R from, say, 3.5 to 3.6, which is a change in the minor version. You generally do not need to re-install add-on packages for a patch release, e.g., going from R 3.6.0 to 3.6.1.\nAs your R usage grows more sophisticated, it’s common to start managing package libraries with more intention. For example, tools like renv (and its predecessor packrat) automate the process of managing project-specific libraries. This can be important for making data products reproducible, portable, and isolated from one another. A package developer might prepend the library search path with a temporary library, containing a set of packages at specific versions, in order to explore issues with backwards and forwards compatibility, without affecting other day-to-day work. Reverse dependency checks are another example where we explicitly manage the library search path.\nHere are the main levers that control which libraries are active, in order of scope and persistence:\n\nEnvironment variables, like R_LIBS and R_LIBS_USER, which are consulted at startup.\nCalling .libPaths() with one or more filepaths.\nExecuting small snippets of code with a temporarily altered library search path via withr::with_libpaths().\nArguments to individual functions, like install.packages(lib =) and library(lib.loc =).\n\nFinally, it’s important to note that library() should NEVER be used inside a package. Packages and scripts rely on different mechanisms for declaring their dependencies and this is one of the biggest adjustments you need to make in your mental model and habits. We explore this topic fully in Section 10.4."
  },
  {
    "objectID": "Workflow101.html#introduction",
    "href": "Workflow101.html#introduction",
    "title": "5  Fundamental development workflows",
    "section": "\n5.1 Introduction",
    "text": "5.1 Introduction\nHaving peeked under the hood of R packages and libraries in Chapter 4, here we provide the basic workflows for creating a package and moving it through the different states that come up during development."
  },
  {
    "objectID": "Workflow101.html#create-a-package",
    "href": "Workflow101.html#create-a-package",
    "title": "5  Fundamental development workflows",
    "section": "\n5.2 Create a package",
    "text": "5.2 Create a package\n\n5.2.1 Survey the existing landscape\n\nMany packages are born out of one person’s frustration at some common task that should be easier. How should you decide whether something is package-worthy? There’s no definitive answer, but it’s helpful to appreciate at least two types of payoff:\n\nProduct: your life will be better when this functionality is implemented formally, in a package.\nProcess: greater mastery of R will make you more effective in your work.\n\nIf all you care about is the existence of a product, then your main goal is to navigate the space of existing packages. Silge, Nash, and Graves organized a survey and sessions around this at useR! 2017 and their write up for the R Journal (Silge, Nash, and Graves 2018) provides a comprehensive roundup of resources.\nIf you are looking for ways to increase your R mastery, you should still educate yourself about the landscape. But there are plenty of good reasons to make your own package, even if there is relevant prior work. The way experts got that way is by actually building things, often very basic things, and you deserve the same chance to learn by tinkering. If you’re only allowed to work on things that have never been touched, you’re likely looking at problems that are either very obscure or very difficult.\nFinally, it’s also valid to evaluate the suitability of existing tools on the basis of user interface, defaults, and edge case behaviour. If a package can technically do what you need, but it’s very unergonomic for your use case, it’s fair to say it doesn’t meet your needs. In this case, it can still make sense for you to develop your own implementation or to write wrapper functions that hide the sharp edges.\nIf your work falls into a well-defined domain, educate yourself about the existing R packages, even if you’ve resolved to create your own package. Do they follow specific design patterns? Are there specific data structures that are common as the primary input and output? For example, there is a very active R community around spatial data analysis (r-spatial.org) that has successfully self-organised to promote greater consistency across packages with different maintainers. In modeling, the hardhat package provides scaffolding for creating a modeling package that plays well with the tidymodels ecosystem. Your package will get more usage and will need less documentation if it fits nicely into the surrounding landscape.\n\n5.2.2 Name your package\n\n“There are only two hard things in Computer Science: cache invalidation and naming things.”\n— Phil Karlton\n\nBefore you can create your package, you need to come up with a name for it. This can be the hardest part of creating a package! (Not least because no one can automate it for you.)\n\n5.2.2.1 Formal requirements\nThere are three formal requirements:\n\nThe name can only consist of letters, numbers, and periods, i.e., ..\nIt must start with a letter.\nIt cannot end with a period.\n\nUnfortunately, this means you can’t use either hyphens or underscores, i.e., - or _, in your package name. We recommend against using periods in package names, due to confusing associations with file extensions and S3 methods.\n\n5.2.2.2 Pragmatic advice\nIf you plan to share your package with others, it’s worth spending a few minutes to come up with a good name. Here are some things to consider:\n\nPick a unique name that’s easy to Google. This makes it easy for potential users to find your package (and associated resources) and for you to see who’s using it.\n\nDon’t pick a name that’s already in use on CRAN or Bioconductor. You may also want to consider some other types of name collision:\n\nIs there an in-development package maturing on, say, GitHub that already has some history and seems to be heading towards release?\nIs this name already used for another piece of software or for a library or framework in, e.g., the Python or JavaScript ecosystem?\n\n\nAvoid using both upper and lower case letters: doing so makes the package name hard to type and even harder to remember. For example, it’s hard to remember if it’s Rgtk2 or RGTK2 or RGtk2.\nGive preference to names that are pronounceable, so people are comfortable talking about your package and have a way to hear it inside their head.\n\nFind a word that evokes the problem and modify it so that it’s unique:\n\nlubridate makes dates and times easier.\nrvest “harvests” the content from web pages.\nr2d3 provides utilities for working with D3 visualisations.\nforcats is an anagram of factors, which we use for categorical data.\n\n\n\nUse abbreviations:\n\nRcpp = R + C++ (plus plus)\nbrms = Bayesian Regression Models using Stan\n\n\n\nAdd an extra R:\n\nstringr provides string tools.\nbeepr plays notification sounds.\ncallr calls R, from R.\n\n\n\nDon’t get sued.\n\nIf you’re creating a package that talks to a commercial service, check the branding guidelines. For example, rDrop isn’t called rDropbox because Dropbox prohibits any applications from using the full trademarked name.\n\n\n\nNick Tierney presents a fun typology of package names in his Naming Things blog post; see that for more inspiring examples. He also has some experience with renaming packages, so the post So, you’ve decided to change your r package name is a good resource if you don’t get this right the first time.\n\n5.2.2.3 Use the available package\nIt is impossible to abide by all of the above suggestions simultaneously, so obviously you will need to make some trade-offs. The available package has a function called available() that helps you evaluate a potential package name from many angles:\n\nlibrary(available)\n\navailable(\"doofus\")\n#> Urban Dictionary can contain potentially offensive results,\n#>   should they be included? [Y]es / [N]o:\n#> 1: 1\n#> ── doofus ──────────────────────────────────────────────────────────────────\n#> Name valid: ✔\n#> Available on CRAN: ✔ \n#> Available on Bioconductor: ✔\n#> Available on GitHub:  ✔ \n#> Abbreviations: http://www.abbreviations.com/doofus\n#> Wikipedia: https://en.wikipedia.org/wiki/doofus\n#> Wiktionary: https://en.wiktionary.org/wiki/doofus\n#> Sentiment:???\n\navailable::available() does the following:\n\nChecks for validity.\nChecks availability on CRAN, Bioconductor, and beyond.\nSearches various websites to help you discover any unintended meanings. In an interactive session, the URLs you see above are opened in browser tabs.\nAttempts to report whether name has positive or negative sentiment.\n\n5.2.3 Package creation\nOnce you’ve come up with a name, there are two ways to create the package.\n\nCall usethis::create_package().\nIn RStudio, do File > New Project > New Directory > R Package. This ultimately calls usethis::create_package(), so really there’s just one way.\n\nThis produces the smallest possible working package, with three components:\n\nAn R/ directory, which you’ll learn about in Chapter 7.\nA basic DESCRIPTION file, which you’ll learn about in Section 10.2.\nA basic NAMESPACE file, which you’ll learn about in Section 10.4.\n\nIt may also include an RStudio project file, pkgname.Rproj, that makes your package easy to use with RStudio, as described below. Basic .Rbuildignore and .gitignore files are also left behind.\n\n\n\n\n\n\nWarning\n\n\n\nDon’t use package.skeleton() to create a package. Because this function comes with R, you might be tempted to use it, but it creates a package that immediately throws errors with R CMD build. It anticipates a different development process than we use here, so repairing this broken initial state just makes unnecessary work for people who use devtools (and, especially, roxygen2). Use create_package().\n\n\n\n5.2.4 Where should you create_package()?\nThe main and only required argument to create_package() is the path where your new package will live:\n\ncreate_package(\"path/to/package/pkgname\")\n\nRemember that this is where your package lives in its source form (Section 4.3), not in its installed form (Section 4.6). Installed packages live in a library and we discussed conventional setups for libraries in Section 4.8.\nWhere should you keep source packages? The main principle is that this location should be distinct from where installed packages live. In the absence of external considerations, a typical user should designate a directory inside their home directory for R (source) packages. We discussed this with colleagues and the source of some of your favorite R packages lives inside directories like ~/rrr/, ~/documents/tidyverse/, ~/r/packages/, or ~/pkg/. Some of us use one directory for this, others divide source packages among a few directories based on their development role (contributor vs. not), GitHub organization (tidyverse vs r-lib), development stage (active vs. not), and so on.\nThe above probably reflects that we are primarily tool-builders. An academic researcher might organize their files around individual publications, whereas a data scientist might organize around data products and reports. There is no particular technical or traditional reason for one specific approach. As long as you keep a clear distinction between source and installed packages, just pick a strategy that works within your overall system for file organization, and use it consistently."
  },
  {
    "objectID": "Workflow101.html#sec-projects",
    "href": "Workflow101.html#sec-projects",
    "title": "5  Fundamental development workflows",
    "section": "\n5.3 RStudio Projects",
    "text": "5.3 RStudio Projects\ndevtools works hand-in-hand with RStudio, which we believe is the best development environment for most R users. To be clear, you can use devtools without using RStudio and you can develop packages in RStudio without using devtools. But there is a special, two-way relationship that makes it very rewarding to use devtools and RStudio together.\n\n\n\n\n\n\nRStudio\n\n\n\nAn RStudio Project, with a capital “P”, is a regular directory on your computer that includes some (mostly hidden) RStudio infrastructure to facilitate your work on one or more projects, with a lowercase “p”. A project might be an R package, a data analysis, a Shiny app, a book, a blog, etc.\n\n\n\n5.3.1 Benefits of RStudio Projects\nFrom Section 4.3, you already know that a source package lives in a directory on your computer. We strongly recommend that each source package is also an RStudio Project. Here are some of the payoffs:\n\nProjects are very “launch-able”. It’s easy to fire up a fresh instance of RStudio in a Project, with the file browser and working directory set exactly the way you need, ready for work.\n\nEach Project is isolated; code run in one Project does not affect any other Project.\n\nYou can have several RStudio Projects open at once and code executed in Project A does not have any effect on the R session and workspace of Project B.\n\n\nYou get handy code navigation tools like F2 to jump to a function definition and Ctrl + . to look up functions or files by name.\n\nYou get useful keyboard shortcuts and a clickable interface for common package development tasks, like generating documentation, running tests, or checking the entire package.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio\n\n\n\nTo see the most useful keyboard shortcuts, press Alt + Shift + K or use Help > Keyboard Shortcuts Help.\n\n\n\n\n\n\n\n\nRStudio\n\n\n\nFollow @rstudiotips on Twitter for a regular dose of RStudio tips and tricks.\n\n\n\n5.3.2 How to get an RStudio Project\nIf you follow our recommendation to create new packages with create_package(), this takes care of itself. Each new package will also be an RStudio Project, if you’re working from RStudio.\nThere are various ways to designate the directory of a pre-existing source package as an RStudio Project:\n\nIn RStudio, do File > New Project > Existing Directory.\nCall create_package() with the path to the pre-existing R source package.\nCall usethis::use_rstudio(), with the active usethis project set to an existing R package. In practice, this probably means you just need to make sure working directory is inside the pre-existing package.\n\n5.3.3 What makes an RStudio Project?\nA directory that is an RStudio Project will contain an .Rproj file. Typically, if the directory is named “foo”, the Project file is foo.Rproj. And if that directory is also an R package, then the package name is usually also “foo”. The path of least resistance is to make all of these names coincide and to NOT nest your package inside a subdirectory inside the Project. If you settle on a different workflow, just know it may feel like you are fighting with the tools.\nAn .Rproj file is just a text file. Here is a representative project file you might see in a Project initiated via usethis:\nVersion: 1.0\n\nRestoreWorkspace: No\nSaveWorkspace: No\nAlwaysSaveHistory: Default\n\nEnableCodeIndexing: Yes\nEncoding: UTF-8\n\nAutoAppendNewline: Yes\nStripTrailingWhitespace: Yes\nLineEndingConversion: Posix\n\nBuildType: Package\nPackageUseDevtools: Yes\nPackageInstallArgs: --no-multiarch --with-keep.source\nPackageRoxygenize: rd,collate,namespace\nYou don’t need to modify this file by hand. Instead, use the interface available via Tools > Project Options or Project Options in the Projects menu in the top-right corner.\n\n\n\n\n\n\n\n\n\n\n\n\n5.3.4 How to launch an RStudio Project\nDouble-click the foo.Rproj file in macOS’s Finder or Windows Explorer to launch the foo Project in RStudio.\nYou can also launch Projects from within RStudio via File > Open Project (in New Session) or the Projects menu in the top-right corner.\nIf you use a productivity or launcher app, you can probably configure it to do something delightful for .Rproj files. We both use Alfred for this 1, which is macOS only, but similar tools exist for Windows. In fact, this is a very good reason to use a productivity app in the first place.\nIt is very normal – and productive! – to have multiple Projects open at once.\n\n5.3.5 RStudio Project vs. active usethis project\nYou will notice that most usethis functions don’t take a path: they operate on the files in the “active usethis project”. The usethis package assumes that 95% of the time all of these coincide:\n\nThe current RStudio Project, if using RStudio.\nThe active usethis project.\nCurrent working directory for the R process.\n\nIf things seem funky, call proj_sitrep() to get a “situation report”. This will identify peculiar situations and propose ways to get back to a happier state.\n\n# these should usually be the same (or unset)\nproj_sitrep()\n#> *   working_directory: '/Users/jenny/rrr/readxl'\n#> * active_usethis_proj: '/Users/jenny/rrr/readxl'\n#> * active_rstudio_proj: '/Users/jenny/rrr/readxl'"
  },
  {
    "objectID": "Workflow101.html#working-directory-and-filepath-discipline",
    "href": "Workflow101.html#working-directory-and-filepath-discipline",
    "title": "5  Fundamental development workflows",
    "section": "\n5.4 Working directory and filepath discipline",
    "text": "5.4 Working directory and filepath discipline\nAs you develop your package, you will be executing R code. This will be a mix of workflow calls (e.g., document() or test()) and ad hoc calls that help you write your functions, examples, and tests. We strongly recommend that keep the top-level of your source package as the working directory of your R process. This will generally happen by default, so this is really a recommendation to avoid development workflows that require you to fiddle with working directory.\nIf you’re totally new to package development, you don’t have much basis for supporting or resisting this proposal. But those with some experience may find it somewhat upsetting. How are we supposed to express paths when working in subdirectories, like tests/? As it becomes relevant, we’ll show you how to exploit path-building helpers, such as testthat::test_path(), that determine paths at execution time.\nThe basic idea is that by leaving working directory alone, you are encouraged to write paths that convey intent explicitly (“read foo.csv from the test directory”) instead of implicitly (“read foo.csv from current working directory, which I think is going to be the test directory”). A sure sign of reliance on implicit paths is incessant fiddling with your working directory, because you’re using setwd() to manually fulfill the assumptions that are implicit in your paths.\nThis mentality can design away a whole class of path headaches and makes day-to-day development more pleasant as well. There are two reasons why implicit paths are hard to get right:\n\nRecall the different forms that a package can take during the development cycle (Chapter 4). These states differ from each other in terms of which files and folders exist and their relative positions within the hierarchy. It’s tricky to write relative paths that work across all package states.\nEventually, your package will be processed with built-in tools like R CMD build, R CMD check, and R CMD INSTALL, by you and potentially CRAN. It’s hard to keep track of what the working directory will be at every stage of these processes.\n\nPath helpers like testthat::test_path(), fs::path_package(), and the rprojroot package are extremely useful for building resilient paths that hold up across the whole range of situations that come up during development and usage. Another way to eliminate brittle paths is to be rigorous in your use of proper methods for storing data inside your package (Chapter 8) and to target the session temp directory when appropriate, such as for ephemeral testing artefacts (Chapter 13)."
  },
  {
    "objectID": "Workflow101.html#sec-load-all",
    "href": "Workflow101.html#sec-load-all",
    "title": "5  Fundamental development workflows",
    "section": "\n5.5 Test drive with load_all()\n",
    "text": "5.5 Test drive with load_all()\n\nThe load_all() function is arguably the most important part of the devtools workflow.\n\n# with devtools attached and\n# working directory set to top-level of your source package ...\n\nload_all()\n\n# ... now experiment with the functions in your package\n\nload_all() is the key step in this “lather, rinse, repeat” cycle of package development:\n\nTweak a function definition.\nload_all()\nTry out the change by running a small example or some tests.\n\nWhen you’re new to package development or to devtools, it’s easy to overlook the importance of load_all() and fall into some awkward habits from a data analysis workflow.\n\n5.5.1 Benefits of load_all()\n\nWhen you first start to use a development environment, like RStudio or Emacs + ESS, the biggest win is the ability to send lines of code from an .R script for execution in R console. The fluidity of this is what makes it tolerable to follow the best practice of regarding your source code as real 2 (as opposed to objects in the workspace) and saving .R files (as opposed to saving and reloading .Rdata).\nload_all() has the same significance for package development and, ironically, requires that you NOT test drive package code in the same way as script code. load_all() simulates the fullblown process for seeing the effect of a source code change, which is clunky enough 3 that you won’t want to do it very often. The main benefits of load_all():\n\nYou can iterate quickly, which encourages exploration and incremental progress.\n\nThis iterative speedup is especially noticeable for packages with compiled code.\n\n\nYou get to develop interactively under a namespace regime that accurately mimics how things are when someone uses your installed package:\n\nYou can call your own internal functions directly, without using ::: and without being tempted to temporarily define your functions in the global workspace.\nYou can also call functions from other packages that you’ve imported into your NAMESPACE, without being tempted to attach these dependencies via library().\n\n\n\nload_all() removes friction from the development workflow and eliminates the temptation to use workarounds that often lead to mistakes around namespace and dependency management.\n\n5.5.2 Other ways to call load_all()\n\nWhen working in a Project that is a package, RStudio offers several ways to call load_all():\n\nKeyboard shortcut: Cmd+Shift+L (macOS), Ctrl+Shift+L (Windows, Linux)\nBuild pane’s More … menu\nBuild > Load All\n\ndevtools::load_all() is a thin wrapper around pkgload::load_all() that adds a bit of user-friendliness. It is unlikely you will use load_all() programatically or inside another package, but if you do, you should probably use pkgload::load_all() directly.\nTODO: Decide how to update this diagram and then reposition and re-integrate it with the prose. For example, figure out how to frame w.r.t. RStudio Install and Restart vs. Clean and Rebuild.\n\n\n\n\n\n\n\n\n\nSilge, Julia, John C. Nash, and Spencer Graves. 2018. “Navigating the R Package Universe.” The R Journal 10 (2): 558–63. https://doi.org/10.32614/RJ-2018-058."
  },
  {
    "objectID": "Package-within.html#introduction",
    "href": "Package-within.html#introduction",
    "title": "6  The package within",
    "section": "\n6.1 Introduction",
    "text": "6.1 Introduction\nThis part of the book ends the same way it started, with the development of a small toy package. Chapter 2 established the basic mechanics, workflow, and tooling of package development, but said practically nothing about the R code inside the package. Here we have a totally different emphasis. In this chapter, we focus primarily on the package’s R code and how it differs from R code in a script.\nWe start with a data analysis script and show how to find the package that lurks within. We isolate and then extract reusable data and logic from the script, put this into an R package, and then use that package in a much simplified script. We make a few rookie mistakes along the way, in order to highlight special considerations for the R code inside a package.\nThe section headers incorporate the NATO phonetic alphabet and have no specific meaning. They are just a convenient way to mark our progress towards a working package."
  },
  {
    "objectID": "Package-within.html#alfa-a-script-that-works",
    "href": "Package-within.html#alfa-a-script-that-works",
    "title": "6  The package within",
    "section": "\n6.2 Alfa: a script that works",
    "text": "6.2 Alfa: a script that works\nHere is a fictional data analysis script data-cleaning.R for a group that collects reports from people who went for a swim:\n\nWhere did you swim and how hot was it outside?\n\nTheir data usually comes as a CSV file, which they read into a data frame.\n\n\n\n\ninfile <- \"swim.csv\"\n(dat <- read.csv(infile))\n#>   name    where temp\n#> 1 Adam    beach   95\n#> 2 Bess    coast   91\n#> 3 Cora seashore   28\n#> 4 Dale    beach   85\n#> 5 Evan  seaside   31\n\nThey then classify each observation as using American (“US”) or British (“UK”) English, based on the word chosen to describe the sandy place where the ocean and land meet. The where column is used to build the new english column.\n\ndat$english[dat$where == \"beach\"] <- \"US\"\ndat$english[dat$where == \"coast\"] <- \"US\"\ndat$english[dat$where == \"seashore\"] <- \"UK\"\ndat$english[dat$where == \"seaside\"] <- \"UK\"\n\nSadly, the temperatures are often reported in a mix of Fahrenheit and Celsius. In the absence of better information, they guess that Americans report temperatures in Fahrenheit and therefore those observations are converted to Celsius.\n\ndat$temp[dat$english == \"US\"] <- (dat$temp[dat$english == \"US\"] - 32) * 5/9\ndat\n#>   name    where temp english\n#> 1 Adam    beach 35.0      US\n#> 2 Bess    coast 32.8      US\n#> 3 Cora seashore 28.0      UK\n#> 4 Dale    beach 29.4      US\n#> 5 Evan  seaside 31.0      UK\n\nFinally, this cleaned (cleaner?) data is written back out to a CSV file. They like to capture a timestamp in the filename when they do this1.\n\nnow <- Sys.time()\ntimestamp <- format(now, \"%Y-%B-%d_%H-%M-%S\")\n(outfile <- paste0(timestamp, \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile)))\n#> [1] \"2022-November-11_07-29-08_swim_clean.csv\"\nwrite.csv(dat, file = outfile, quote = FALSE, row.names = FALSE)\n\nEven if your typical analytical tasks are quite different, hopefully you see a few familiar patterns here. It’s easy to imagine that this group does very similar pre-processing of many similar data files over time. Their analyses can be more efficient and consistent if they make these standard data maneuvers available to themselves as functions in a package, instead of inlining the same data and logic into dozens or hundreds of data ingest scripts."
  },
  {
    "objectID": "Package-within.html#bravo-a-better-script-that-works",
    "href": "Package-within.html#bravo-a-better-script-that-works",
    "title": "6  The package within",
    "section": "\n6.3 Bravo: a better script that works",
    "text": "6.3 Bravo: a better script that works\nThe package that lurks within the original script is actually pretty hard to see! It’s obscured by a few suboptimal coding practices, such as the use of repetitive copy/paste-style code and the mixing of code and data. Therefore a good first step is to refactor this code, isolating as much data and logic as possible in proper objects and functions, respectively.\nAt the same time, we introduce the use of some add-on packages, for several reasons. First, we would actually use the tidyverse for this sort of data wrangling. Second, many people use add-on packages in their scripts, so it is good to see how add-on packages are handled as we create this package.\nHere’s the next version of the script.\n\n\n\n\nlibrary(tidyverse)\n\ninfile <- \"swim.csv\"\ndat <- read_csv(infile, col_types = cols(name = \"c\", where = \"c\", temp = \"d\"))\n\nlookup_table <- tribble(\n      ~where, ~english,\n     \"beach\",     \"US\",\n     \"coast\",     \"US\",\n  \"seashore\",     \"UK\",\n   \"seaside\",     \"UK\"\n)\n\ndat <- dat %>% \n  left_join(lookup_table)\n#> Joining, by = \"where\"\n\nf_to_c <- function(x) (x - 32) * 5/9\n\ndat <- dat %>% \n  mutate(temp = if_else(english == \"US\", f_to_c(temp), temp))\ndat\n#> # A tibble: 5 × 4\n#>   name  where     temp english\n#>   <chr> <chr>    <dbl> <chr>  \n#> 1 Adam  beach     35   US     \n#> 2 Bess  coast     32.8 US     \n#> 3 Cora  seashore  28   UK     \n#> 4 Dale  beach     29.4 US     \n#> 5 Evan  seaside   31   UK\n\nnow <- Sys.time()\ntimestamp <- function(time) format(time, \"%Y-%B-%d_%H-%M-%S\")\noutfile_path <- function(infile) {\n  paste0(timestamp(now), \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile))\n}\nwrite_csv(dat, outfile_path(infile))\n\nThe key features to note are:\n\nWe are using functions from tidyverse packages (specifically from readr and dplyr).\nThe map between different “beach” words and whether they are considered to be US or UK English is now isolated in a lookup table, which lets us create the english column in one go with a left_join(). This also makes it easier to add new words in the future\nThe f_to_c(), timestamp(), and outfile_path() functions now hold the logic for converting temperatures and forming the timestamped output file name.\n\nIt’s getting easier to recognize the reusable bits of this script, i.e. the bits that have nothing to do with a specific input file, like swim.csv. This sort of refactoring often happens naturally on the way to creating your own package, but if it does not, it’s a good idea to do this intentionally."
  },
  {
    "objectID": "Package-within.html#charlie-external-helpers",
    "href": "Package-within.html#charlie-external-helpers",
    "title": "6  The package within",
    "section": "\n6.4 Charlie: external helpers",
    "text": "6.4 Charlie: external helpers\nA typical next step is to move reusable data and logic out of the analysis script and into one or more separate files. This is a conventional opening move, if you want to use these same helper files in multiple analyses.\nHere is the content of beach-lookup-table.csv:\n\n\n\n\nwhere,english\nbeach,US\ncoast,US\nseashore,UK\nseaside,UK\n\nHere is the content of cleaning-helpers.R:\n\nlibrary(tidyverse)\n\nlocalize_beach <- function(dat) {\n  lookup_table <- read_csv(\n    \"beach-lookup-table.csv\",\n    col_types = cols(where = \"c\", english = \"c\")\n  )\n  left_join(dat, lookup_table)\n}\n\nf_to_c <- function(x) (x - 32) * 5/9\n\ncelsify_temp <- function(dat) {\n  mutate(dat, temp = if_else(english == \"US\", f_to_c(temp), temp))\n}\n\nnow <- Sys.time()\ntimestamp <- function(time) format(time, \"%Y-%B-%d_%H-%M-%S\")\noutfile_path <- function(infile) {\n  paste0(timestamp(now), \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile))\n}\n\nWe’ve added some high-level helper functions, localize_beach() and celsify_temp(), to the pre-existing helpers (f_to_c(), timestamp(), and outfile_path()).\nHere is the next version of the data cleaning script, now that we’ve pulled out the helper functions (and lookup table).\n\n\n\n\nlibrary(tidyverse)\nsource(\"cleaning-helpers.R\")\n\ninfile <- \"swim.csv\"\ndat <- read_csv(infile, col_types = cols(name = \"c\", where = \"c\", temp = \"d\"))\n\n(dat <- dat %>% \n    localize_beach() %>% \n    celsify_temp())\n#> Joining, by = \"where\"\n#> # A tibble: 5 × 4\n#>   name  where     temp english\n#>   <chr> <chr>    <dbl> <chr>  \n#> 1 Adam  beach     35   US     \n#> 2 Bess  coast     32.8 US     \n#> 3 Cora  seashore  28   UK     \n#> 4 Dale  beach     29.4 US     \n#> 5 Evan  seaside   31   UK\n\nwrite_csv(dat, outfile_path(infile))\n\nYou’ll notice that the script is getting shorter and, hopefully, easier to read and modify, because repetitive and fussy clutter has been moved out of sight. Whether the code is actually easier to work with is subjective and depends on how natural the “interface” feels for the people who actually preprocess swimming data. These sorts of design decisions are the subject of a separate project: design.tidyverse.org.\nLet’s assume the group agrees that our design decisions are promising, i.e. we seem to be making things better, not worse. Sure, the existing code is not perfect, but this is a typical developmental stage when you’re trying to figure out what the helper functions should be and how they should work."
  },
  {
    "objectID": "Package-within.html#delta-an-attempt-at-a-package",
    "href": "Package-within.html#delta-an-attempt-at-a-package",
    "title": "6  The package within",
    "section": "\n6.5 Delta: an attempt at a package",
    "text": "6.5 Delta: an attempt at a package\nLet’s make a package! Here’s the simplest thing you might hope will “just work”: make cleaning-helpers.R into an R package. Somehow.\nConcretely, we do this:\n\nUse usethis::create_package() to scaffold a new R package.\n\nThis is a good first step!\n\n\nCopy cleaning-helpers.R into the new package, specifically, to R/cleaning-helpers.R.\n\nThis is morally correct, but mechanically wrong in several ways, as we will soon see.\n\n\nCopy beach-lookup-table.csv into the new package. But where? Let’s try the top-level of the source package.\n\nThis is not going to end well. Shipping data in a package is a special topic, which is covered in Chapter 8.\n\n\nInstall this package.\n\nDespite all of the problems identified above, this actually works! Which is interesting, because we can (try to) use it and see what happens.\n\n\n\nHere’s the version of the script that you hope will run after successfully installing this package.\n\nlibrary(tidyverse)\nlibrary(delta)\n\ninfile <- \"swim.csv\"\ndat <- read_csv(infile, col_types = cols(name = \"c\", where = \"c\", temp = \"d\"))\n\ndat <- dat %>% \n  localize_beach() %>% \n  celsify_temp()\n\nwrite_csv(dat, outfile_path(infile))\n\nThe only change from our previous script is that\n\nsource(\"cleaning-helpers.R\")\n\nhas been replaced by\n\nlibrary(delta)\n\nHere’s what actually happens when we try to run this:\n\nlibrary(tidyverse)\nlibrary(delta)\n\ninfile <- \"swim.csv\"\ndat <- read_csv(infile, col_types = cols(name = \"c\", where = \"c\", temp = \"d\"))\n\ndat <- dat %>% \n  localize_beach() %>% \n  celsify_temp()\n#> Error in localize_beach(.) : could not find function \"localize_beach\"\n\nwrite_csv(dat, outfile_path(infile))\n#> Error in outfile_path(infile) : could not find function \"outfile_path\"\n\nNone of our helper functions are actually available for use, even though we call library(delta)! In contrast to source()ing a file of helper functions, attaching a package does not dump its functions into the global workspace. By default, functions in a package are only for internal use. We need to export localize_beach(), celsify_temp(), and outfile_path() so our users can call them. In this book, we achieve this by putting @export in the special roxygen comment above each function (namespace management is covered in Section 10.4).\n\n#' @export\ncelsify_temp <- function(dat) {\n  mutate(dat, temp = if_else(english == \"US\", f_to_c(temp), temp))\n}\n\nLet’s say we do that, run devtools::document() to (re)generate a NAMESPACE file, and re-install the package. Now when we execute our script, it works!\nCorrection: it works sometimes. Specifically, it works if and only if the working directory is set to the top-level of the source package. From any other working directory, we still get an error:\n\nlibrary(tidyverse)\nlibrary(delta)\n\ninfile <- \"swim.csv\"\ndat <- read_csv(infile, col_types = cols(name = \"c\", where = \"c\", temp = \"d\"))\n\ndat <- dat %>% \n  localize_beach() %>% \n  celsify_temp()\n#> Error: 'beach-lookup-table.csv' does not exist in current working directory ('/Users/jenny/tmp').\n\nwrite_csv(dat, outfile_path(infile))\n\nThe lookup table consulted inside localize_beach() cannot be found. One does not simply dump CSV files into the source of an R package and expect things to “just work”. We will fix this in our next iteration of the package (Chapter 8 has full coverage of how to include data in a package).\nBefore we abandon this initial experiment, let’s also marvel at the fact that we were able to install, attach, and, to a certain extent, use a fundamentally broken package. load_all() works fine, too! This is a sobering reminder that you should be running R CMD check, probably via check(), very often during development. This will quickly alert you to many problems that simple installation and usage does not reveal.\nIndeed, R CMD check fails for this package and we see this:\n * installing *source* package ‘delta’ ...\n ** using staged installation\n ** R\n ** byte-compile and prepare package for lazy loading\n Error in library(tidyverse) : there is no package called ‘tidyverse’\n Error: unable to load R code in package ‘delta’\n Execution halted\n ERROR: lazy loading failed for package ‘delta’\n * removing ‘/Users/jenny/rrr/delta.Rcheck/delta’\nWhat do you mean “there is no package called ‘tidyverse’”?!? We’re using it, with no problems, in our main script! Also, we’ve already installed and used this package, why can’t R CMD check install it?\nThis error is what happens when the strictness of R CMD check meets the very first line of R/cleaning-helpers.R:\n\nlibrary(tidyverse)\n\nThis is not how you declare that your package depends on another package (the tidyverse, in this case). This is not how you make functions in another package available for use in yours. Dependencies must be declared in DESCRIPTION (and that’s not all). Since we declared no dependencies, R CMD check takes us at our word and tries to install our package with only the base packages available, which means this library(tidyverse) call fails. A “regular” installation succeeds, simply because the tidyverse is available in your regular library, which hides this particular mistake.\nTo review, copying cleaning-helpers.R to R/cleaning-helpers.R, without further modification, was problematic in (at least) these ways:\n\nDoes not account for exported vs. non-exported functions.\nThe CSV file holding our lookup table cannot be found in the installed package.\nDoes not properly declare our dependency on other add-on packages."
  },
  {
    "objectID": "Package-within.html#echo-a-working-package",
    "href": "Package-within.html#echo-a-working-package",
    "title": "6  The package within",
    "section": "\n6.6 Echo: a working package",
    "text": "6.6 Echo: a working package\nWe’re ready to make the most minimal version of this package that actually works.\nHere is the new version of R/cleaning-helpers.R2:\n\nlookup_table <- dplyr::tribble(\n      ~where, ~english,\n     \"beach\",     \"US\",\n     \"coast\",     \"US\",\n  \"seashore\",     \"UK\",\n   \"seaside\",     \"UK\"\n)\n\n#' @export\nlocalize_beach <- function(dat) {\n  dplyr::left_join(dat, lookup_table)\n}\n\nf_to_c <- function(x) (x - 32) * 5/9\n\n#' @export\ncelsify_temp <- function(dat) {\n  dplyr::mutate(dat, temp = dplyr::if_else(english == \"US\", f_to_c(temp), temp))\n}\n\nnow <- Sys.time()\ntimestamp <- function(time) format(time, \"%Y-%B-%d_%H-%M-%S\")\n\n#' @export\noutfile_path <- function(infile) {\n  paste0(timestamp(now), \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile))\n}\n\nWe’ve gone back to defining the lookup_table with R code, since our initial attempt to read it from CSV created some sort of filepath snafu. This is OK for small, internal, static data, but remember to see Chapter 8 for more general techniques for storing data in a package.\nAll of our calls to tidyverse functions have now been qualified with the name of the specific package that actually provides the function, e.g. dplyr::mutate(). There are other ways to access functions in another package, explained in Section 10.4, but this is our recommended default. It is also our strong recommendation that no one depend on the tidyverse meta-package in a package3. Instead, it is better to identify the specific package(s) you actually use. In this case, our package only uses dplyr.\nThe library(tidyverse) call is gone and instead we declare our use of dplyr in the Imports field of DESCRIPTION:\nPackage: echo\n(... other lines omitted ...)\nImports: \n    dplyr\nThis, together with our use of namespace-qualified calls, like dplyr::left_join(), constitutes a valid way to use another package within ours. The metadata conveyed via DESCRIPTION is covered in Section 10.2.\nAll of the user-facing functions have an @export tag in their roxygen comment, which means that devtools::document() adds them correctly to the NAMESPACE file. Note that f_to_c() is currently only used internally, inside celsify_temp(), so we have not exported it (likewise for timestamp()).\nThis version of the package can be installed, used, AND it technically passes R CMD check, though with 1 note and 1 warning.\n* checking R code for possible problems ... NOTE\ncelsify_temp: no visible binding for global variable ‘english’\ncelsify_temp: no visible binding for global variable ‘temp’\nUndefined global functions or variables:\n  english temp\n\n* checking for missing documentation entries ... WARNING\nUndocumented code objects:\n  ‘celsify_temp’ ‘localize_beach’ ‘outfile_path’\nAll user-level objects in a package should have documentation entries.\nSee chapter ‘Writing R documentation files’ in the ‘Writing R\nExtensions’ manual.\nThe “no visible binding” note is a peculiarity of using dplyr and unquoted variable names inside a package, where the use of bare variable names (english and temp) looks suspicious. We could add either of these lines to any file below R/ to eliminate this note4:\n\n# option 1 (then you should also put utils in Imports)\nutils::globalVariables(c(\"english\", \"temp\"))\n\n# option 2\nenglish <- temp <- NULL\n\nThe warning about missing documentation is because we haven’t properly documented our exported functions. This is a valid concern and something you should absolutely address in a real package. You’ve already seen how to create help files with roxygen comments in the whole game chapter and we cover this topic thoroughly in Chapter 16. Therefore, we won’t discuss this further here."
  },
  {
    "objectID": "Package-within.html#sec-package-within-build-time-run-time",
    "href": "Package-within.html#sec-package-within-build-time-run-time",
    "title": "6  The package within",
    "section": "\n6.7 Foxtrot: build time vs. run time",
    "text": "6.7 Foxtrot: build time vs. run time\nThe package works, which is great, but group members notice something odd about the timestamps:\n\nSys.time()\n#> [1] \"2022-02-24 20:49:59 PST\"\n\noutfile_path(\"INFILE.csv\")\n#> [1] \"2020-September-03_11-06-33_INFILE_clean.csv\"\n\nThe datetime in the timestamped filename doesn’t reflect the time reported by the system. In fact, the users claim that the timestamp never seems to change at all! Why is this?\nRecall how we form the filepath for output files:\n\nnow <- Sys.time()\ntimestamp <- function(time) format(time, \"%Y-%B-%d_%H-%M-%S\")\noutfile_path <- function(infile) {\n  paste0(timestamp(now), \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile))\n}\n\nThe fact that we capture now <- Sys.time() outside of the definition of outfile_path() has probably been vexing some readers for a while. now reflects the instant in time when we execute now <- Sys.time(). In the initial approach, that happened when we source()d cleaning-helpers.R. That’s not ideal, but it was probably a pretty harmless mistake, because the helper file would be source()d shortly before we wrote the outfile.\nBut this approach is quite devastating in the context of a package. now <- Sys.time() is executed when the package is built. And never again. It is very easy to subconsciously assume your package code is re-evaluated when the package is installed, attached, or used. But it is not. Yes, the code inside your functions is absolutely run whenever they are called. But your functions – and any other objects created in top-level code below R/ – are defined exactly once, at build time.\nBy defining now with top-level code below R/, we’ve doomed our package to timestamp all of its output files with the same (wrong) time. The fix is to make sure the Sys.time() call happens at runtime.\nLet’s look again at parts of R/cleaning-helpers.R:\n\nlookup_table <- dplyr::tribble(\n      ~where, ~english,\n     \"beach\",     \"US\",\n     \"coast\",     \"US\",\n  \"seashore\",     \"UK\",\n   \"seaside\",     \"UK\"\n)\n\nnow <- Sys.time()\ntimestamp <- function(time) format(time, \"%Y-%B-%d_%H-%M-%S\")\noutfile_path <- function(infile) {\n  paste0(timestamp(now), \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile))\n}\n\nThere are four top-level <- assignments in this excerpt. The top-level definitions of the data frame lookup_table and the functions timestamp() and outfile_path() are correct. It is appropriate that these be defined exactly once, at build time. The top-level definition of now, which is then used inside outfile_path(), is regrettable.\nHere are better versions of outfile_path():\n\n# always timestamp as \"now\"\noutfile_path <- function(infile) {\n  ts <- timestamp(Sys.time())\n  paste0(ts, \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile))\n}\n\n# allow user to provide a time, but default to \"now\"\noutfile_path <- function(infile, time = Sys.time()) {\n  ts <- timestamp(time)\n  paste0(ts, \"_\", sub(\"(.*)([.]csv$)\", \"\\\\1_clean\\\\2\", infile))\n}\n\nThis illustrates that you need to have a different mindset when defining objects inside a package. The vast majority of those objects should be functions and these functions should generally only use data they create or that is passed via an argument. There are some types of sloppiness that are fairly harmless when a function is defined immediately before its use, but that can be more costly for functions distributed as a package."
  },
  {
    "objectID": "Package-within.html#sec-package-within-side-effects",
    "href": "Package-within.html#sec-package-within-side-effects",
    "title": "6  The package within",
    "section": "\n6.8 Golf: side effects",
    "text": "6.8 Golf: side effects\nThe timestamps now reflect the current time, but the group raises a new concern. As it stands, the timestamps reflect who has done the data cleaning and which part of the world they’re in. The heart of the timestamp strategy is this format string5:\n\nformat(Sys.time(), \"%Y-%B-%d_%H-%M-%S\")\n#> [1] \"2022-November-11_07-29-10\"\n\nThis formats Sys.time() in such a way that it includes the month name (not number) and the local time6.\nHere’s such a timestamp produced by a few hypothetical colleagues cleaning some data at exactly the same instant in time.\n\n\n\n\n\n\n\n\n\nlocation\ntimestamp\nLC_TIME\ntz\n\n\n\nRome, Italy\n2020-settembre-05_00-30-00\nit_IT.UTF-8\nEurope/Rome\n\n\nWarsaw, Poland\n2020-września-05_00-30-00\npl_PL.UTF-8\nEurope/Warsaw\n\n\nSao Paulo, Brazil\n2020-setembro-04_19-30-00\npt_BR.UTF-8\nAmerica/Sao_Paulo\n\n\nGreenwich, England\n2020-September-04_23-30-00\nen_GB.UTF-8\nEurope/London\n\n\n“Computer World!”\n2020-September-04_22-30-00\nC\nUTC\n\n\n\n\nWe see that the month names vary, as does the time, and even the date! The safest choice is to form timestamps with respect to a fixed locale and time zone (presumably the non-geographic choices represented by “Computer World!” above).\nYou do some research and learn that you can force a certain locale via Sys.setlocale() and force a certain time zone by setting the TZ environment variable. Specifically, we set the LC_TIME component of the locale to “C” and the time zone to “UTC” (Coordinated Universal Time). Here’s your first attempt to improve timestamp():\n\ntimestamp <- function(time = Sys.time()) {\n  Sys.setlocale(\"LC_TIME\", \"C\")\n  Sys.setenv(TZ = \"UTC\")\n  format(time, \"%Y-%B-%d_%H-%M-%S\")\n}\n\nBut your Brazilian colleague notices that datetimes print differently, before and after she uses outfile_path() from your package:\nBefore:\n\nformat(Sys.time(), \"%Y-%B-%d_%H-%M-%S\")\n\n\n#> [1] \"2022-novembro-11_04-29-10\"\n\nAfter:\n\noutfile_path(\"INFILE.csv\")\n#> [1] \"2022-November-11_07-29-10_INFILE_clean.csv\"\n\nformat(Sys.time(), \"%Y-%B-%d_%H-%M-%S\")\n#> [1] \"2022-November-11_07-29-10\"\n\nNotice that her month name switched from Portuguese to English and the time is clearly being reported in a different time zone. Our calls to Sys.setlocale() and Sys.setenv() inside timestamp() have made persistent (and very surprising) changes to her R session. This sort of side effect is very undesirable and is extremely difficult to track down and debug, especially in more complicated settings.\nHere are better versions of timestamp():\n\n# use withr::local_*() functions to keep the changes local to timestamp()\ntimestamp <- function(time = Sys.time()) {\n  withr::local_locale(c(\"LC_TIME\" = \"C\"))\n  withr::local_timezone(\"UTC\")\n  format(time, \"%Y-%B-%d_%H-%M-%S\")\n}\n\n# use the tz argument to format.POSIXct()\ntimestamp <- function(time = Sys.time()) {\n  withr::local_locale(c(\"LC_TIME\" = \"C\"))\n  format(time, \"%Y-%B-%d_%H-%M-%S\", tz = \"UTC\")\n}\n\n# put the format() call inside withr::with_*()\ntimestamp <- function(time = Sys.time()) {\n  withr::with_locale(\n    c(\"LC_TIME\" = \"C\"),\n    format(time, \"%Y-%B-%d_%H-%M-%S\", tz = \"UTC\")\n  )\n}\n\nWe show various methods to limit the scope of our changes to LC_TIME and the timezone. A good rule of thumb is to make the scope of such changes as narrow as is possible and practical. The tz argument of format() is the most surgical way to deal with the timezone, but nothing similar exists for LC_TIME. We make the temporary locale modification using the withr package, which provides a very flexible toolkit for temporary state changes. This (and base::on.exit()) are discussed further in Section 7.6.\nThis underscores a point from the previous section: you need to adopt a different mindset when defining functions inside a package. Try to avoid making any changes to the user’s overall state. If such changes are unavoidable, make sure to reverse them (if possible) or to document them explicitly (if related to the function’s primary purpose)."
  },
  {
    "objectID": "Code.html#introduction",
    "href": "Code.html#introduction",
    "title": "7  R code",
    "section": "\n7.1 Introduction",
    "text": "7.1 Introduction\nThe first principle of making a package is that all R code goes in the R/ directory. In this chapter, you’ll learn about organising your functions into files, maintaining a consistent style, and recognizing the stricter requirements for functions in a package (versus in a script). We’ll also remind you of the fundamental workflows for test-driving and formally checking an in-development package: load_all(), test(), and check()."
  },
  {
    "objectID": "Code.html#sec-code-organising",
    "href": "Code.html#sec-code-organising",
    "title": "7  R code",
    "section": "\n7.2 Organise functions into files",
    "text": "7.2 Organise functions into files\nThe only hard rule is that your package should store its function definitions in R scripts, i.e. files with extension .R, that live in the R/ directory1. However, a few more conventions can make the source code of your package easier to navigate and relieve you of re-answering “How should I name this?” each time you create a new file. The Tidyverse Style Guide offers some general advice about file names and also advice that specifically applies to files in a package. We expand on this here.\nThe file name should be meaningful and convey which functions are defined within. While you’re free to arrange functions into files as you wish, the two extremes are bad: don’t put all functions into one file and don’t put each function into its own separate file. This advice should inform your general policy, but there are exceptions to every rule. If a specific function is very large or has lots of documentation, it can make sense to give it its own file, named after the function. More often, a single .R file will contain multiple function definitions: such as a main function and its supporting helpers, a family of related functions, or some combination of the two.\nHere are some examples from the actual source of the tidyr package at version 1.1.2. There are some departures from the hard-and-fast rules given above, which illustrates that there’s a lot of room for judgment here.\n\n\n\n\nOrganising principle\nSource file\nComments\n\n\n\nOne function\ntidyr/R/uncount.R\nDefines exactly one function, uncount(), that’s not particulary large, but doesn’t fit naturally into any other .R file\n\n\nMain function plus helpers\ntidyr/R/separate.R\nDefines the user-facing separate() (an S3 generic), a data.frame method, and private helpers\n\n\nFamily of functions\ntidyr/R/rectangle.R\nDefines a family of functions for “rectangling” nested lists (hoist() and the unnest() functions), all documented together in a big help topic, plus private helpers\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAnother file you often see in the wild is R/utils.R. This is a common place to define small utilities that are used inside multiple package functions. Since they serve as helpers to multiple functions, placing them in R/utils.R makes them easier to re-discover when you return to your package after a long break.\nBob Rudis assembled a collection of such files and did some analysis in the post Dissecting R Package “Utility Belts”.\n\n\nIf it’s very hard to predict which file a function lives in, that suggests it’s time to separate your functions into more files or reconsider how you are naming your functions and/or files.\n\n\n\n\n\n\nRStudio\n\n\n\nThe organisation of functions within files is less important in RStudio, which offers two ways to jump to the definition of a function:\n\n\nPress Ctrl + . (the period) then start typing the name. Keep typing to narrow the list and eventually pick a function (or file) to visit. This works for both functions and files in your project.\n\n\n\n\n\n\nWith your cursor in a function name or with a function name selected, press F2. This works for functions defined in your package or in another package.\n\nAfter navigating to a function with one of these methods, return to where you started by clicking the back arrow at the top-left of the editor () or by pressing Ctrl + F9 (Windows & Linux) or Cmd + F9 (macOS)."
  },
  {
    "objectID": "Code.html#sec-code-load-all",
    "href": "Code.html#sec-code-load-all",
    "title": "7  R code",
    "section": "\n7.3 Fast feedback via load_all()\n",
    "text": "7.3 Fast feedback via load_all()\n\nAs you add or modify functions defined in files below R/, you will naturally want to try them out. We want to reiterate our strong recommendation to use devtools::load_all() to make them available for interactive exploration instead of, for example, source()ing files below R/. The main coverage of load_all() is in Section 5.5 and load_all() also shows up as one of the natural development tasks in Section 2.9. Compared to the alternatives, load_all() helps you to iterate more quickly and provides an excellent approximation to the namespace regime of an installed package."
  },
  {
    "objectID": "Code.html#code-style",
    "href": "Code.html#code-style",
    "title": "7  R code",
    "section": "\n7.4 Code style",
    "text": "7.4 Code style\nWe recommend following the tidyverse style guide (https://style.tidyverse.org), which goes into much more detail than we can here. Its format also allows it to be a more dynamic document than this book.\nAlthough the style guide explains the “what” and the “why”, another important decision is how to enforce a specific code style. For this we recommend the styler package (https://styler.r-lib.org); its default behaviour enforces the tidyverse style guide. There are many ways to apply styler to your code, depending on the context:\n\n\nstyler::style_pkg() restyles an entire R package.\n\nstyler::style_dir() restyles all files in a directory.\n\nusethis::use_tidy_style() is wrapper that applies one of the above functions depending on whether the current project is an R package or not.\n\nstyler::style_file() restyles a single file.\n\nstyler::style_text() restyles a character vector.\n\n\n\n\n\n\n\nRStudio\n\n\n\nWhen styler is installed, the RStudio Addins menu will offer several additional ways to style code:\n\nthe active selection\nthe active file\nthe active package\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you don’t use Git or another version control system, applying a function like styler::style_pkg() is nerve-wracking and somewhat dangerous, because you lack a way to see exactly what changed and to accept/reject such changes in a granular way.\n\n\nThe styler package can also be integrated with various platforms for hosting source code and doing continuous integration. For example, the tidyverse packages use a GitHub Action that restyles a package when triggered by a special comment (/style) on a pull request. This allows maintainers to focus on reviewing the substance of the pull request, without having to nitpick small issues of whitespace or indentation23 ."
  },
  {
    "objectID": "Code.html#understand-when-code-is-executed",
    "href": "Code.html#understand-when-code-is-executed",
    "title": "7  R code",
    "section": "\n7.5 Understand when code is executed",
    "text": "7.5 Understand when code is executed\nUp until now, you’ve probably been writing scripts, R code saved in a file that you execute interactively, perhaps using an IDE and/or source(), or noninteractively via Rscript. There are two main differences between code in scripts and packages:\n\nIn a script, code is run … when you run it! The awkwardness of this statement reflects that it’s hard to even think about this issue with a script. However, we must, in order to appreciate that the code in a package is run when the package is built. This has big implications for how you write the code below R/: package code should only create objects, the vast majority of which will be functions.\nFunctions in your package will be used in situations that you didn’t imagine. This means your functions need to be thoughtful in the way that they interact with the outside world.\n\nWe expand on the first point here and the second in the next section. These topics are also illustrated concretely in Section 6.7.\nWhen you source() a script, every line of code is executed and the results are immediately made available. Things are different with package code, because it is loaded in two steps. When the binary package is built (often, by CRAN) all the code in R/ is executed and the results are saved. When you attach a package with library(), these cached results are re-loaded and certain objects (mostly functions) are made available for your use. The full details on what it means for a package to be in binary form are given in Section 4.5. We refer to the creation of the binary package as (binary) “build time” and, specifically, we mean when R CMD INSTALL --build is run. (You might think that this is what R CMD build does, but that actually makes a bundled package, a.k.a. a “source tarball”.) For macOS and Windows users of CRAN packages, build time is whenever CRAN built the binary package for their OS. For those who install packages from source, build time is essentially when they (built and) installed the package.\nConsider the assignment x <- Sys.time(). If you put this in a script, x tells you when the script was source()d. But if you put that same code in a package, x tells you when the package binary was built. In Section 6.7, we show a complete example of this in the context of forming timestamps inside a package.\nThe main takeaway is this:\n\nAny R code outside of a function is suspicious and should be carefully reviewed.\n\nWe explore a few real-world examples below that show how easy it is to get burned by this “build time vs. load time” issue. Luckily, once you diagnose this problem, it is generally not difficult to fix.\n\n7.5.1 Example: A path returned by system.file()\n\nThe shinybootstrap2 package once had this code below R/:\n\ndataTableDependency <- list(\n  htmlDependency(\n    \"datatables\", \"1.10.2\",\n    c(file = system.file(\"www/datatables\", package = \"shinybootstrap2\")),\n    script = \"js/jquery.dataTables.min.js\"\n  ),\n  htmlDependency(\n    \"datatables-bootstrap\", \"1.10.2\",\n    c(file = system.file(\"www/datatables\", package = \"shinybootstrap2\")),\n    stylesheet = c(\"css/dataTables.bootstrap.css\", \"css/dataTables.extra.css\"),\n    script = \"js/dataTables.bootstrap.js\"\n  )\n)\n\nSo dataTableDependency was a list object defined in top-level package code and its value was constructed from paths obtained via system.file(). As described in a GitHub issue,\n\nThis works fine when the package is built and tested on the same machine. However, if the package is built on one machine and then used on another (as is the case with CRAN binary packages), then this will fail – the dependency will point to the wrong directory on the host.\n\nThe heart of the solution is to make sure that system.file() is called from a function, at run time. Indeed, this fix was made here (in commit 138db47) and in a few other packages that had similar code and a related check was added in htmlDependency() itself. This particular problem would now be caught by R CMD check, due to changes that came with staged installation as of R 3.6.0.\n\n7.5.2 Example: Available colours\nThe crayon package has a function, crayon::show_ansi_colors(), that displays an ANSI colour table on your screen, basically to show what sort of styling is possible. In an early version, the function looked something like this:\n\nshow_ansi_colors <- function(colors = num_colors()) {\n  if (colors < 8) {\n    cat(\"Colors are not supported\")\n  } else if (colors < 256) {\n    cat(ansi_colors_8, sep = \"\")\n    invisible(ansi_colors_8)\n  } else {\n    cat(ansi_colors_256, sep = \"\")\n    invisible(ansi_colors_256)\n  }\n}\n\nansi_colors_8 <- # code to generate a vector covering basic terminal colors\n  \nansi_colors_256 <- # code to generate a vector covering 256 colors\n\nwhere ansi_colors_8 and ansi_colors_256 were character vectors exploring a certain set of colours, presumably styled via ANSI escapes.\nThe problem was those objects were formed and cached when the binary package was built. Since that often happens on a headless server, this likely happens under conditions where terminal colours might not be enabled or even available. Users of the installed package could still call show_ansi_colors() and num_colors() would detect the number of colours supported by their system (256 on most modern computers). But then an un-coloured object would print to screen (the original GitHub issue is r-lib/crayon#37).\nThe solution was to compute the display objects with a function at run time (in commit e2b368a:\n\nshow_ansi_colors <- function(colors = num_colors()) {\n  if (colors < 8) {\n    cat(\"Colors are not supported\")\n  } else if (colors < 256) {\n    cat(ansi_colors_8(), sep = \"\")\n    invisible(ansi_colors_8())\n  } else {\n    cat(ansi_colors_256(), sep = \"\")\n    invisible(ansi_colors_256())\n  }\n}\n\nansi_colors_8 <- function() {\n  # code to generate a vector covering basic terminal colors\n}\n  \nansi_colors_256 <- function() {\n  # code to generate a vector covering 256 colors\n}\n\nLiterally, the same code is used, it is simply pushed down into the body of a function taking no arguments (similar to the shinybootstrap2 example). Each reference to, e.g., the ansi_colors_8 object is replaced by a call to the ansi_colors_8() function.\nThe main takeaway is that functions that assess or expose the capabilities of your package on a user’s system must fully execute on your user’s system. It’s fairly easy to accidentally rely on results that were cached at build time, quite possibly on a different machine.\n\n7.5.3 Example: Aliasing a function\nOne last example shows that, even if you are careful to only define functions below R/, there are still some subtleties to consider. Imagine that you want the function foo() in your package to basically be an alias for the function blah() from some other package, e.g. pkgB. You might be tempted to do this:\n\nfoo <- pkgB::blah\n\nHowever, this will cause foo() in your package to reflect the definition of pkgB::blah() at the version present on the machine where the binary package is built (often CRAN), at that moment in time. If a bug is discovered in pkgB::blah() and subsequently fixed, your package will still use the older, buggy version, until your package is rebuilt (often by CRAN) and your users upgrade, which is completely out of your control. This alternative approach protects you from this:\n\nfoo <- function(...) pkgB::blah(...)\n\nNow, when your user calls foo(), they are effectively calling pkgB::blah(), at the version installed on their machine at that very moment.\nA real example of this affected an older version of knitr, related to how the default “evaluate” hook was being set to evaluate::evaluate() (original issue is yihui/knitr#1441, resolved in commit d6b53e0)."
  },
  {
    "objectID": "Code.html#sec-code-r-landscape",
    "href": "Code.html#sec-code-r-landscape",
    "title": "7  R code",
    "section": "\n7.6 Respect the R landscape",
    "text": "7.6 Respect the R landscape\nAnother big difference between a script and a package is that other people are going to use your package, and they’re going to use it in situations that you never imagined. This means you need to pay attention to the R landscape, which includes not just the available functions and objects, but all the global settings.\nYou have changed the R landscape if you’ve loaded a package with library(), or changed a global option with options(), or modified the working directory with setwd(). If the behaviour of other functions differs before and after running your function, you’ve modified the landscape. Section 6.8 has a concrete example of this involving time zones and the locale-specific printing of datetimes. Changing the landscape is bad because it makes code much harder to understand.\nThere are some functions that modify global settings that you should never use because there are better alternatives:\n\nDon’t use library() or require(). These modify the search path, affecting what functions are available from the global environment. Instead, you should use the DESCRIPTION to specify your package’s requirements, as described in Section 10.2. This also makes sure those packages are installed when your package is installed.\nNever use source() to load code from a file. source() modifies the current environment, inserting the results of executing the code. There is no reason to use source() inside your package, i.e. in a file below R/. Sometimes people source() files below R/ during package development, but as we’ve explained in Section 5.5 and Section 7.3, load_all() is a much better way to load your current code for exploration. If you’re using source() to create a dataset, it is better to use the methods in Chapter 8 for including data in a package.\n\nHere is a non-exhaustive list of other functions that should be used with caution:\n\noptions()\npar()\nsetwd()\nSys.setenv()\nSys.setlocale()\n\nset.seed() (or anything that changes the state of the random number generator)\n\nIf you must use them, make sure to clean up after yourself. Below we show how to do this using functions from the withr package and in base R.\nThe flip side of this coin is that you should avoid relying on the user’s landscape, which might be different to yours. For example, functions that rely on sorting strings are dangerous, because sort order depends on the system locale. Below we see that locales one might actually encounter in practice (C, English, French, etc.) differ in how they sort non-ASCII strings or uppercase versus lowercase letters.\n\nx <- c(\"bernard\", \"bérénice\", \"béatrice\", \"boris\")\n\nwithr::with_locale(c(LC_COLLATE = \"fr_FR\"), sort(x))\n#> [1] \"béatrice\" \"bérénice\" \"bernard\"  \"boris\"\nwithr::with_locale(c(LC_COLLATE = \"C\"), sort(x))\n#> [1] \"bernard\"  \"boris\"    \"béatrice\" \"bérénice\"\n\nx <- c(\"a\", \"A\", \"B\", \"b\", \"A\", \"b\")\n\nwithr::with_locale(c(LC_COLLATE = \"en_CA\"), sort(x))\n#> [1] \"a\" \"A\" \"A\" \"b\" \"b\" \"B\"\nwithr::with_locale(c(LC_COLLATE = \"C\"), sort(x))\n#> [1] \"A\" \"A\" \"B\" \"a\" \"b\" \"b\"\n\nIf you write your functions as if all users have the same system locale as you, your code might fail.\n\n7.6.1 Manage state with withr\nIf you need to modify the R landscape inside a function, then it is important to ensure your change is reversed on exit of that function. This is exactly what base::on.exit() is designed to do. You use on.exit() inside a function to register code to run later, that restores the landscape to its original state. It is important to note that proper tools, such as on.exit(), work even if we exit the function abnormally, i.e. due to an error. This is why it’s worth using the official methods described here over any do-it-yourself solution.\nWe usually manage state using the withr package, which provides a flexible, on.exit()-like toolkit (on.exit() itself is covered in the next section). withr::defer() can be used as a drop-in replacement for on.exit(). Why do we like withr so much? First, it offers many pre-built convenience functions for state changes that come up often. We also appreciate withr’s default stack-like behaviour (LIFO = last in, first out), its usability in interactive sessions, and its envir argument (in more advanced usage).\nThe general pattern is to capture the original state, schedule its eventual restoration “on exit”, then make the state change. Some setters, such as options() or par(), return the old value when you provide a new value, leading to usage that looks like this:\n\nf <- function(x, y, z) {\n  ...\n  old <- options(width = 20)\n  defer(options(old))\n  ...\n}\n\nCertain state changes, such as modifying session options, come up so often that withr offers pre-made helpers. Here are a few of the state change helpers in withr that you are most likely to find useful:\n\n\nDo / undo this\nwithr functions\n\n\n\nSet an R option\n\nwith_options(), local_options()\n\n\n\nSet an environment variable\n\nwith_envvar(), local_envvar()\n\n\n\nChange working directory\n\nwith_dir(), local_dir()\n\n\n\nSet a graphics parameter\n\nwith_par(), local_par()\n\n\n\n\nYou’ll notice each helper comes in two forms that are useful in different situations:\n\n\nwith_*() functions are best for executing small snippets of code with a temporarily modified state. (These functions are inspired by how base::with() works.)\n\nf <- function(x, sig_digits) {\n  # imagine lots of code here\n  withr::with_options(\n    list(digits = sig_digits),\n    print(x)\n  )\n  # ... and a lot more code here\n}\n\n\n\nlocal_*() functions are best for modifying state “from now until the function exits”.\n\ng <- function(x, sig_digits) {\n  withr::local_options(list(digits = sig_digits))\n  print(x)\n  # imagine lots of code here\n}\n\n\n\nDeveloping code interactively with withr is pleasant, because deferred actions can be scheduled even on the global environment. Those cleanup actions can then be executed with withr::deferred_run() or cleared without execution with withr::deferred_clear(). Without this feature, it can be tricky to experiment with code that needs cleanup “on exit”, because it behaves so differently when executed in the console versus at arm’s length inside a function.\nMore in-depth coverage is given in the withr vignette Changing and restoring state and withr will also prove useful when we talk about testing in Chapter 13.\n\n7.6.2 Restore state with base::on.exit()\n\nHere is how the general “save, schedule restoration, change” pattern looks when using base::on.exit().\n\nf <- function(x, y, z) {\n  ...\n  old <- options(mfrow = c(2, 2), pty = \"s\")\n  on.exit(options(old), add = TRUE)\n  ...\n}\n\nOther state changes aren’t available with that sort of setter and you must implement it yourself.\n\ng <- function(a, b, c) {\n  ...\n  scratch_file <- tempfile()\n  on.exit(unlink(scratch_file), add = TRUE)\n  file.create(scratch_file)\n  ...\n}\n\nNote that we specify on.exit(..., add = TRUE), because you almost always want this behaviour, i.e. to add to the list of deferred cleanup tasks rather than to replace them entirely. This (and the default value of after) are related to our preference for withr::defer(), when we’re willing to take a dependency on withr. These issues are explored in a withr vignette.\n\n7.6.3 Isolate side effects\nCreating plots and printing output to the console are two other ways of affecting the global R environment. Often you can’t avoid these (because they’re important!) but it’s good practice to isolate them in functions that only produce output. This also makes it easier for other people to repurpose your work for new uses. For example, if you separate data preparation and plotting into two functions, others can use your data prep work (which is often the hardest part!) to create new visualisations.\n\n7.6.4 When you do need side-effects\nOccasionally, packages do need side-effects. This is most common if your package talks to an external system — you might need to do some initial setup when the package loads. To do that, you can use two special functions: .onLoad() and .onAttach(). These are called when the package is loaded and attached. You’ll learn about the distinction between the two in Chapter 11. For now, you should always use .onLoad() unless explicitly directed otherwise.\nSome common uses of .onLoad() and .onAttach() are:\n\n\nTo set custom options for your package with options(). To avoid conflicts with other packages, ensure that you prefix option names with the name of your package. Also be careful not to override options that the user has already set. Here’s a (highly redacted) version of dplyr’s .onLoad() function which sets an option that controls progress reporting:\n\n.onLoad <- function(libname, pkgname) {\n  op <- options()\n  op.dplyr <- list(\n    dplyr.show_progress = TRUE\n  )\n  toset <- !(names(op.dplyr) %in% names(op))\n  if (any(toset)) options(op.dplyr[toset])\n\n  invisible()\n}\n\nThis allows functions in dplyr to use getOption(\"dplyr.show_progress\") to determine whether to show progress bars, relying on the fact that a sensible default value has already been set.\n\n\n\n\nTo display an informative message when the package loads. This might make usage conditions clear or display package capabilities based on current system conditions. Startup messages are one place where you should use .onAttach() instead of .onLoad(). To display startup messages, always use packageStartupMessage(), and not message(). (This allows suppressPackageStartupMessages() to selectively suppress package startup messages).\n\n.onAttach <- function(libname, pkgname) {\n  packageStartupMessage(\"Welcome to my package\")\n}\n\n\n\nAs you can see in the examples, .onLoad() and .onAttach() are called with two arguments: libname and pkgname. They’re rarely used (they’re a holdover from the days when you needed to use library.dynam() to load compiled code). They give the path where the package is installed (the “library”), and the name of the package.\nIf you use .onLoad(), consider using .onUnload() to clean up any side effects. By convention, .onLoad() and friends are usually saved in a file called R/zzz.R. (Note that .First.lib() and .Last.lib() are old versions of .onLoad() and .onUnload() and should no longer be used.)"
  },
  {
    "objectID": "Code.html#constant-health-checks",
    "href": "Code.html#constant-health-checks",
    "title": "7  R code",
    "section": "\n7.7 Constant health checks",
    "text": "7.7 Constant health checks\nHere is a typical sequence of calls when using devtools for package development:\n\nEdit one or more files below R/.\n\ndocument() (if you’ve made any changes that impact help files or NAMESPACE)\nload_all()\nRun some examples interactively.\n\ntest() (or test_active_file())\ncheck()\n\nAn interesting question is how frequently and rapidly you move through this development cycle. We often find ourselves running through the above sequence several times in an hour or in a day while adding or modifying a single function.\nThose newer to package development might be most comfortable slinging R code and much less comfortable writing and compiling documentation, simulating package build & installation, testing, and running R CMD check. And it is human nature to embrace the familiar and postpone the unfamiliar. This often leads to a dysfunctional workflow where the full sequence above unfolds infrequently, maybe once per month or every couple of months, very slowly and often with great pain:\n\nEdit one or more files below R/.\nBuild, install, and use the package. Iterate occasionally with previous step.\nWrite documentation (once the code is “done”).\nWrite tests (once the code is “done”).\nRun R CMD check right before submitting to CRAN or releasing in some other way.\n\nWe’ve already talked about the value of fast feedback, in the context of load_all(). But this also applies to running document(), test(), and check(). There are defects you just can’t detect from using load_all() and running a few interactive examples that are immediately revealed by more formal checks. Finding and fixing 5 bugs, one at a time, right after you created each one is much easier than troubleshooting all 5 at once (possibly interacting with each other), weeks or months after you last touched the code."
  },
  {
    "objectID": "Code.html#sec-code-cran",
    "href": "Code.html#sec-code-cran",
    "title": "7  R code",
    "section": "\n7.8 CRAN notes",
    "text": "7.8 CRAN notes\n(Each chapter will finish with some hints for submitting your package to CRAN. If you don’t plan on submitting your package to CRAN, feel free to ignore them!)\nIf you’re planning on submitting your package to CRAN, you must use only ASCII characters in your .R files. In practice, this means you are limited to the digits 0 to 9, lowercase letters ‘a’ to ‘z’, uppercase letters ‘A’ to ‘Z’, and common punctuation.\nBut sometimes you need to inline a small bit of character data that includes, e.g., a Greek letter (µ), an accented character (ü), or a symbol (30°). You can use any Unicode character as long as you specify it in the special Unicode escape \"\\u1234\" format. The easiest way to find the correct code point is to use stringi::stri_escape_unicode():\n\nx <- \"This is a bullet •\"\ny <- \"This is a bullet \\u2022\"\nidentical(x, y)\n#> [1] TRUE\ncat(stringi::stri_escape_unicode(x))\n#> This is a bullet \\u2022\n\nSometimes you have the opposite problem. You don’t intentionally have any non-ASCII characters in your R code, but automated checks reveal that you do.\nW  checking R files for non-ASCII characters ...\n   Found the following file with non-ASCII characters:\n     foo.R\n   Portable packages must use only ASCII characters in their R code,\n   except perhaps in comments.\n   Use \\uxxxx escapes for other characters.\nThe most common offenders are “curly” or “smart” single and double quotes that sneak in through copy/paste. The functions tools::showNonASCII() and tools::showNonASCIIfile(file) help you find the offending file(s) and line(s).\n\ntools::showNonASCIIfile(\"R/foo.R\")\n#> 666: #' If you<e2><80><99>ve copy/pasted quotes, watch out!"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "8  Data",
    "section": "",
    "text": "Second edition\n\n\n\nYou are reading the work-in-progress second edition of R Packages. This chapter is undergoing heavy restructuring and may be confusing or incomplete."
  },
  {
    "objectID": "data.html#introduction",
    "href": "data.html#introduction",
    "title": "8  Data",
    "section": "\n8.1 Introduction",
    "text": "8.1 Introduction\nIt’s often useful to include data in a package. If you’re releasing the package to a broad audience, it’s a way to provide compelling use cases for the package’s functions. If you’re releasing the package to a more specific audience, interested either in the data (e.g., NZ census data) or the subject (e.g., demography), it’s a way to distribute that data along with its documentation (as long as your audience is R users).\nThere are three main ways to include data in your package, depending on what you want to do with it and who should be able to use it:\n\nIf you want to store binary data and make it available to the user, put it in data/. This is the best place to put example datasets.\nIf you want to store parsed data, but not make it available to the user, put it in R/sysdata.rda. This is the best place to put data that your functions need.\nIf you want to store raw data, put it in inst/extdata.\n\nA simple alternative to these three options is to include it in the source of your package, either creating by hand, or using dput() to serialise an existing data set into R code.\nEach possible location is described in more detail below."
  },
  {
    "objectID": "data.html#sec-data-data",
    "href": "data.html#sec-data-data",
    "title": "8  Data",
    "section": "\n8.2 Exported data",
    "text": "8.2 Exported data\nThe most common location for package data is (surprise!) data/. Each file in this directory should be an .rda (or .RData) file created by save() containing a single object (with the same name as the file). The easiest way to adhere to these rules is to use usethis::use_data():\n\nx <- sample(1000)\nusethis::use_data(x, mtcars)\n\nIt’s possible to use other types of files, but I don’t recommend it because .rda files are already fast, small and explicit. Other options are described in data(). For larger datasets, you may want to experiment with the compression setting. The default is bzip2, but sometimes gzip or xz can create smaller files.\nIf the DESCRIPTION contains LazyData: true, then datasets will be lazily loaded. This means that they won’t occupy any memory until you use them. The following example shows memory usage before and after loading the nycflights13 package. You can see that memory usage doesn’t change until you inspect the flights dataset stored inside the package.\n\npryr::mem_used()\n#> 56.8 MB\nlibrary(nycflights13)\npryr::mem_used()\n#> 58.5 MB\n\ninvisible(flights)\npryr::mem_used()\n#> 99.1 MB\n\nI recommend that you always include LazyData: true in your DESCRIPTION. usethis::create_package() does this for you. This text is slated for revision for the 2nd edition. TL;DR do NOT include LazyData: true in DESCRIPTION unless you actually ship data in your package. usethis v2.1.0 changes the behaviour of create_package() and use_data() accordingly.\nOften, the data you include in data/ is a cleaned up version of raw data you’ve gathered from elsewhere. I highly recommend taking the time to include the code used to do this in the source version of your package. This will make it easy for you to update or reproduce your version of the data. I suggest that you put this code in data-raw/. You don’t need it in the bundled version of your package, so also add it to .Rbuildignore. Do all this in one step with:\n\nusethis::use_data_raw()\n\nYou can see this approach in practice in some of my recent data packages. I’ve been creating these as packages because the data will rarely change, and because multiple packages can then use them for examples:\n\nbabynames\nfueleconomy\nnasaweather\nnycflights13\nusdanutrients\n\n\n8.2.1 Documenting datasets\nObjects in data/ are always effectively exported (they use a slightly different mechanism than NAMESPACE but the details are not important). This means that they must be documented. Documenting data is like documenting a function with a few minor differences. Instead of documenting the data directly, you document the name of the dataset and save it in R/. For example, the roxygen2 block used to document the diamonds data in ggplot2 is saved as R/data.R and looks something like this:\n\n#' Prices of 50,000 round cut diamonds.\n#'\n#' A dataset containing the prices and other attributes of almost 54,000\n#' diamonds.\n#'\n#' @format A data frame with 53940 rows and 10 variables:\n#' \\describe{\n#'   \\item{price}{price, in US dollars}\n#'   \\item{carat}{weight of the diamond, in carats}\n#'   ...\n#' }\n#' @source \\url{http://www.diamondse.info/}\n\"diamonds\"\n\nThere are two additional tags that are important for documenting datasets:\n\n@format gives an overview of the dataset. For data frames, you should include a definition list that describes each variable. It’s usually a good idea to describe variables’ units here.\n@source provides details of where you got the data, often a \\url{}.\n\nNever @export a data set."
  },
  {
    "objectID": "data.html#data-sysdata",
    "href": "data.html#data-sysdata",
    "title": "8  Data",
    "section": "\n8.3 Internal data",
    "text": "8.3 Internal data\nSometimes functions need pre-computed data tables. If you put these in data/ they’ll also be available to package users, which is not appropriate. Instead, you can save them in R/sysdata.rda. For example, two colour-related packages, munsell and dichromat, use R/sysdata.rda to store large tables of colour data.\nYou can use usethis::use_data() to create this file with the argument internal = TRUE:\n\nx <- sample(1000)\nusethis::use_data(x, mtcars, internal = TRUE)\n\nAgain, to make this data reproducible it’s a good idea to include the code used to generate it. Put it in data-raw/.\nObjects in R/sysdata.rda are not exported (they shouldn’t be), so they don’t need to be documented. They’re only available inside your package."
  },
  {
    "objectID": "data.html#sec-data-extdata",
    "href": "data.html#sec-data-extdata",
    "title": "8  Data",
    "section": "\n8.4 Raw data",
    "text": "8.4 Raw data\nIf you want to show examples of loading/parsing raw data, put the original files in inst/extdata. When the package is installed, all files (and folders) in inst/ are moved up one level to the top-level directory (so they can’t have names like R/ or DESCRIPTION). To refer to files in inst/extdata (whether installed or not), use system.file(). For example, the readr package uses inst/extdata to store delimited files for use in examples:\n\nsystem.file(\"extdata\", \"mtcars.csv\", package = \"readr\")\n#> [1] \"/home/runner/work/_temp/Library/readr/extdata/mtcars.csv\"\n\nBeware: by default, if the file does not exist, system.file() does not return an error - it just returns the empty string:\n\nsystem.file(\"extdata\", \"iris.csv\", package = \"readr\")\n#> [1] \"\"\n\nIf you want to have an error message when the file does not exist, add the argument mustWork = TRUE:\n\nsystem.file(\"extdata\", \"iris.csv\", package = \"readr\", mustWork = TRUE)\n#> Error in system.file(\"extdata\", \"iris.csv\", package = \"readr\", mustWork = TRUE): no file found"
  },
  {
    "objectID": "data.html#other-data",
    "href": "data.html#other-data",
    "title": "8  Data",
    "section": "\n8.5 Other data",
    "text": "8.5 Other data\n\nData for tests: it’s ok to put small files directly in your test directory. But remember unit tests are for testing correctness, not performance, so keep the size small.\nData for vignettes. If you want to show how to work with an already loaded dataset, put that data in data/. If you want to show how to load raw data, put that data in inst/extdata."
  },
  {
    "objectID": "data.html#sec-data-cran",
    "href": "data.html#sec-data-cran",
    "title": "8  Data",
    "section": "\n8.6 CRAN notes",
    "text": "8.6 CRAN notes\nGenerally, package data should be smaller than a megabyte - if it’s larger you’ll need to argue for an exemption. This is usually easier to do if the data is in its own package and won’t be updated frequently. You should also make sure that the data has been optimally compressed:\n\nRun tools::checkRdaFiles() to determine the best compression for each file.\nRe-run usethis::use_data() with compress set to that optimal value. If you’ve lost the code for recreating the files, you can use tools::resaveRdaFiles() to re-save in place."
  },
  {
    "objectID": "misc.html",
    "href": "misc.html",
    "title": "9  Other components",
    "section": "",
    "text": "Second edition\n\n\n\nYou are reading the work-in-progress second edition of R Packages. This chapter is undergoing heavy restructuring and may be confusing or incomplete."
  },
  {
    "objectID": "misc.html#introduction",
    "href": "misc.html#introduction",
    "title": "9  Other components",
    "section": "\n9.1 Introduction",
    "text": "9.1 Introduction\nThere are five other directories that are valid top-level directories. They are rarely used:\n\ninst/: for arbitrary additional files that you want include in your package. This includes a few special files, like the CITATION, described below.\ndemo/: for package demos. These were useful prior to the introduction of vignettes, but are no longer recommended. See below.\nexec/: for executable scripts. Unlike files placed in other directories, files in exec/ are automatically flagged as executable.\npo/: translations for messages. This is useful but beyond the scope of this book. See the Internationalization chapter of “R extensions” for more details.\ntools/: auxiliary files needed during configuration, or for sources that need to generate scripts."
  },
  {
    "objectID": "misc.html#inst",
    "href": "misc.html#inst",
    "title": "9  Other components",
    "section": "\n9.2 Installed files",
    "text": "9.2 Installed files\nWhen a package is installed, everything in inst/ is copied into the top-level package directory. In some sense inst/ is the opposite of .Rbuildignore - where .Rbuildignore lets you remove arbitrary files and directories from the top level, inst/ lets you add them. You are free to put anything you like in inst/ with one caution: because inst/ is copied into the top-level directory, you should never use a subdirectory with the same name as an existing directory. This means that you should avoid inst/build, inst/data, inst/demo, inst/exec, inst/help, inst/html, inst/inst, inst/libs, inst/Meta, inst/man, inst/po, inst/R, inst/src, inst/tests, inst/tools and inst/vignettes.\nThis chapter discusses the most common files found in inst/:\n\ninst/AUTHOR and inst/COPYRIGHT. If the copyright and authorship of a package is particularly complex, you can use plain text files, inst/COPYRIGHTS and inst/AUTHORS, to provide more information.\ninst/CITATION: how to cite the package, see below for details.\ninst/docs: This is an older convention for vignettes, and should be avoided in modern packages.\ninst/extdata: additional external data for examples and vignettes. See Section 8.4 for more detail.\ninst/java, inst/python etc. See below.\n\nTo find a file in inst/ from code use system.file(). For example, to find inst/extdata/mydata.csv, you’d call system.file(\"extdata\", \"mydata.csv\", package = \"mypackage\"). Note that you omit the inst/ directory from the path. This will work if the package is installed, or if it’s been loaded with devtools::load_all().\n\n9.2.1 Package citation\nThe CITATION file lives in the inst directory and is intimately connected to the citation() function which tells you how to cite R and R packages. Calling citation() without any arguments tells you how to cite base R:\n\ncitation()\n#> \n#> To cite R in publications use:\n#> \n#>   R Core Team (2022). R: A language and environment for\n#>   statistical computing. R Foundation for Statistical\n#>   Computing, Vienna, Austria. URL\n#>   https://www.R-project.org/.\n#> \n#> A BibTeX entry for LaTeX users is\n#> \n#>   @Manual{,\n#>     title = {R: A Language and Environment for Statistical Computing},\n#>     author = {{R Core Team}},\n#>     organization = {R Foundation for Statistical Computing},\n#>     address = {Vienna, Austria},\n#>     year = {2022},\n#>     url = {https://www.R-project.org/},\n#>   }\n#> \n#> We have invested a lot of time and effort in creating R,\n#> please cite it when using it for data analysis. See also\n#> 'citation(\"pkgname\")' for citing R packages.\n\nCalling it with a package name tells you how to cite that package:\n\ncitation(\"lubridate\")\n#> \n#> To cite lubridate in publications use:\n#> \n#>   Garrett Grolemund, Hadley Wickham (2011). Dates and Times\n#>   Made Easy with lubridate. Journal of Statistical Software,\n#>   40(3), 1-25. URL https://www.jstatsoft.org/v40/i03/.\n#> \n#> A BibTeX entry for LaTeX users is\n#> \n#>   @Article{,\n#>     title = {Dates and Times Made Easy with {lubridate}},\n#>     author = {Garrett Grolemund and Hadley Wickham},\n#>     journal = {Journal of Statistical Software},\n#>     year = {2011},\n#>     volume = {40},\n#>     number = {3},\n#>     pages = {1--25},\n#>     url = {https://www.jstatsoft.org/v40/i03/},\n#>   }\n\nTo customise the citation for your package, add a inst/CITATION that looks like this:\n\ncitHeader(\"To cite lubridate in publications use:\")\n\ncitEntry(entry = \"Article\",\n  title        = \"Dates and Times Made Easy with {lubridate}\",\n  author       = personList(as.person(\"Garrett Grolemund\"),\n                   as.person(\"Hadley Wickham\")),\n  journal      = \"Journal of Statistical Software\",\n  year         = \"2011\",\n  volume       = \"40\",\n  number       = \"3\",\n  pages        = \"1--25\",\n  url          = \"https://www.jstatsoft.org/v40/i03/\",\n\n  textVersion  =\n  paste(\"Garrett Grolemund, Hadley Wickham (2011).\",\n        \"Dates and Times Made Easy with lubridate.\",\n        \"Journal of Statistical Software, 40(3), 1-25.\",\n        \"URL https://www.jstatsoft.org/v40/i03/.\")\n)\n\nYou need to create inst/CITATION. As you can see, it’s pretty simple: you only need to learn one new function, citEntry(). The most important arguments are:\n\nentry: the type of citation, “Article”, “Book”, “PhDThesis” etc.\nThe standard bibliographic information like title, author (which should be a personList()), year, journal, volume, issue, pages, …\n\nA complete list of arguments can be found in ?bibentry.\nUse citHeader() and citFooter() to add additional exhortations.\n\n9.2.2 Other languages\nSometimes a package contains useful supplementary scripts in other programming languages. Generally, you should avoid these, because it adds an additional extra dependency, but it may be useful when wrapping substantial amounts of code from another language. For example, gdata wraps the Perl module Spreadsheet::ParseExcel to read excel files into R.\nThe convention is to put scripts of this nature into a subdirectory of inst/, inst/python, inst/perl, inst/ruby etc. If these scripts are essential to your package, make sure you also add the appropriate programming language to the SystemRequirements field in the DESCRIPTION. (This field is for human reading so don’t worry about exactly how you specify it.)\nJava is a special case and the best place to learn more is the documentation of the rJava package (http://www.rforge.net/rJava/)."
  },
  {
    "objectID": "misc.html#demo",
    "href": "misc.html#demo",
    "title": "9  Other components",
    "section": "\n9.3 Demos",
    "text": "9.3 Demos\nA demo is an .R file that lives in demo/. Demos are like examples but tend to be longer. Instead of focussing on a single function, they show how to weave together multiple functions to solve a problem.\nYou list and access demos with demo():\n\nShow all available demos: demo().\nShow all demos in a package: demo(package = \"httr\").\nRun a specific demo: demo(\"oauth1-twitter\", package = \"httr\").\nFind a demo: system.file(\"demo\", \"oauth1-twitter.R\", package = \"httr\").\n\nEach demo must be listed in demo/00Index in the following form: demo-name   Demo description. The demo name is the name of the file without the extension, e.g. demo/my-demo.R becomes my-demo.\nBy default the demo asks for human input for each plot: “Hit  to see next plot:”. This behaviour can be overridden by adding devAskNewPage(ask = FALSE) to the demo file. You can add pauses by adding: readline(\"press any key to continue\").\nGenerally, I do not recommend using demos. Instead, consider writing a vignette:\n\nDemos are not automatically tested by R CMD check. This means that they can easily break without your knowledge.\nVignettes have both input and output, so readers can see the results without having to run the code themselves.\nLonger demos need to mingle code with explanation, and RMarkdown is better suited to that task than R comments.\nVignettes are listed on the CRAN package page. This makes it easier for new users to discover them."
  },
  {
    "objectID": "Metadata.html",
    "href": "Metadata.html",
    "title": "10  DESCRIPTION and NAMESPACE",
    "section": "",
    "text": "Second edition\n\n\n\nYou are reading the work-in-progress second edition of R Packages. This chapter is undergoing heavy restructuring and may be confusing or incomplete."
  },
  {
    "objectID": "Metadata.html#introduction",
    "href": "Metadata.html#introduction",
    "title": "10  DESCRIPTION and NAMESPACE",
    "section": "\n10.1 Introduction",
    "text": "10.1 Introduction\nThere are two important files that provide metadata about your package DESCRIPTION and NAMESPACE. The DESCRIPTION provides overall metadata about the package, and the NAMESPACE describes which functions you use from other packages and you expose to the world. In this chapter, you’ll learn the basic structure of these files and some of their simple applications: like the name and title of your package and who wrote it.\nWe’ll continue in the next chapters to explain:\n\nLicensing is a big enough topic that it has a dedicated chapter (Chapter 12). If you have no plans to share your package, you may be able to ignore licensing. But if you plan to share, even if only by putting the code where others can see it, you really should specify a license.\nThe License field which defines who can use your package.\nThe dependencies of your package, which"
  },
  {
    "objectID": "Metadata.html#sec-description",
    "href": "Metadata.html#sec-description",
    "title": "10  DESCRIPTION and NAMESPACE",
    "section": "\n10.2 DESCRIPTION\n",
    "text": "10.2 DESCRIPTION\n\nThe job of the DESCRIPTION file is to store important metadata about your package. When you first start writing packages, you’ll mostly use these metadata to record what packages are needed to run your package. However, as time goes by, other aspects of the metadata file will become useful to you, such as revealing what your package does (via the Title and Description) and whom to contact (you!) if there are any problems.\nEvery package must have a DESCRIPTION. In fact, it’s the defining feature of a package (RStudio and devtools consider any directory containing DESCRIPTION to be a package)1. To get you started, usethis::create_package(\"mypackage\") automatically adds a bare-bones DESCRIPTION file. This will allow you to start writing the package without having to worry about the metadata until you need to. This minimal DESCRIPTION will vary a bit depending on your settings, but should look something like this:\n\nPackage: mypackage\nTitle: What the Package Does (One Line, Title Case)\nVersion: 0.0.0.9000\nAuthors@R: \n    person(\"First\", \"Last\", , \"first.last@example.com\", role = c(\"aut\", \"cre\"),\n           comment = c(ORCID = \"YOUR-ORCID-ID\"))\nDescription: What the package does (one paragraph).\nLicense: `use_mit_license()`, `use_gpl3_license()` or friends to pick a\n    license\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.1\n\nIf you create a lot of packages, you can customize the default content of new DESCRIPTION files by setting the global option usethis.description to a named list. You can pre-configure your preferred name, email, license, etc. See the article on usethis setup for more details.\nDESCRIPTION uses a simple file format called DCF, the Debian control format. You can see most of the structure in the examples in this chapter. Each line consists of a field name and a value, separated by a colon. When values span multiple lines, they need to be indented:\nDescription: The description of a package is usually long,\n    spanning multiple lines. The second and subsequent lines\n    should be indented, usually with four spaces.\nIf you ever need to work with a DESCRIPTION file programmatically, take a look at the desc package, which usethis uses heavily under-the-hood.\nThis chapter will show you how to use the straightforward DESCRIPTION fields."
  },
  {
    "objectID": "Metadata.html#description-title-description",
    "href": "Metadata.html#description-title-description",
    "title": "10  DESCRIPTION and NAMESPACE",
    "section": "\n10.3 Title and description: What does your package do?",
    "text": "10.3 Title and description: What does your package do?\nThe title and description fields describe what the package does. They differ only in length:\n\n\nTitle is a one line description of the package, and is often shown in a package listing. It should be plain text (no markup), capitalised like a title, and NOT end in a period. Keep it short: listings will often truncate the title to 65 characters.\n\nDescription is more detailed than the title. You can use multiple sentences, but you are limited to one paragraph. If your description spans multiple lines (and it should!), each line must be no more than 80 characters wide. Indent subsequent lines with 4 spaces.\n\nThe Title and Description for ggplot2 are:\nTitle: Create Elegant Data Visualisations Using the Grammar of Graphics\nDescription: A system for 'declaratively' creating graphics,\n    based on \"The Grammar of Graphics\". You provide the data, tell 'ggplot2'\n    how to map variables to aesthetics, what graphical primitives to use,\n    and it takes care of the details.\nA good title and description are important, especially if you plan to release your package to CRAN, because they appear on the CRAN download page as follows:\n\n\n\n\n\nFigure 10.1: The CRAN page for ggplot2, highlighting Title and Description.\n\n\n\n\nIf you plan to submit your package to CRAN, both the Title and Description are a frequent source of rejections for reasons not covered by the automated R CMD check. In addition to the basics above, here are a few more tips:\n\nPut the names of R packages, software, and APIs inside single quotes. This goes for both the Title and the Description. See the ggplot2 example above.\nIf you need to use an acronym, try to do so in Description, not in Title. In either case, explain the acronym in Description, i.e. fully expand it.\nDon’t include the package name, especially in Title, which is often prefixed with the package name.\nDo not start with “A package for …” or “This package does …”. This rule makes sense once you look at the list of CRAN packages by name. The information density of such a listing is much higher without a universal prefix like “A package for …”.\n\nIf these constraints give you writer’s block, it often helps to spend a few minutes reading Title and Description of packages already on CRAN. Once you read a couple dozen, you can usually find a way to say what you want to say about your package that is also likely to pass CRAN’s human-enforced checks.\nYou’ll notice that Description only gives you a small amount of space to describe what your package does. This is why it’s so important to also include a README.md file that goes into much more depth and shows a few examples. You’ll learn about that in Section 18.2.\n\n10.3.1 Author: who are you?\nTo identify the package’s author, and whom to contact if something goes wrong, use the Authors@R field. This field is unusual because it contains executable R code rather than plain text. Here’s an example:\nAuthors@R: person(\"Hadley\", \"Wickham\", email = \"hadley@rstudio.com\",\n  role = c(\"aut\", \"cre\"))\n\nperson(\"Hadley\", \"Wickham\", email = \"hadley@rstudio.com\", \n  role = c(\"aut\", \"cre\"))\n#> [1] \"Hadley Wickham <hadley@rstudio.com> [aut, cre]\"\n\nThis command says that Hadley Wickham is both the maintainer (cre) and an author (aut) and that his email address is hadley@rstudio.com. The person() function has four main inputs:\n\nThe name, specified by the first two arguments, given and family (these are normally supplied by position, not name). In English cultures, given (first name) comes before family (last name). In many cultures, this convention does not hold. For a non-person entity, such as “R Core Team” or “RStudio”, use the given argument (and omit family).\nThe email address. It’s important to note that this is the address CRAN uses to let you know if your package needs to be fixed in order to stay on CRAN. Make sure to use an email address that’s likely to be around for a while. CRAN policy requires that this be for a person, as opposed to, e.g., a mailing list.\n\nOne or more three letter codes specifying the role. These are the most important roles to know about:\n\ncre: the creator or maintainer, the person you should bother if you have problems. Despite being short for “creator”, this is the correct role to use for the current maintainer, even if they are not the initial creator of the package.\naut: authors, those who have made significant contributions to the package.\nctb: contributors, those who have made smaller contributions, like patches.\ncph: copyright holder. This is used if the copyright is held by someone other than the author, typically a company (i.e. the author’s employer).\nfnd: funder, the people or organizations that have provided financial support for the development of the package.\n\n(The full list of roles is extremely comprehensive. Should your package have a woodcutter (wdc), lyricist (lyr) or costume designer (cst), rest comfortably that you can correctly describe their role in creating your package. However, note that packages destined for CRAN must limit themselves to the subset of MARC roles listed in the documentation for person().)\n\n\nThe optional comment argument has become more relevant, since person() and CRAN landing pages have gained some nice features around ORCID identifiers. Here’s an example of such usage (note the auto-generated URI):\n\nperson(\n  \"Jennifer\", \"Bryan\",\n  email = \"jenny@rstudio.com\",\n  role = c(\"aut\", \"cre\"),\n  comment = c(ORCID = \"0000-0002-6983-2759\")\n)\n#> [1] \"Jennifer Bryan <jenny@rstudio.com> [aut, cre] (<https://orcid.org/0000-0002-6983-2759>)\"\n\n\n\nYou can list multiple authors with c():\nAuthors@R: c(\n    person(\"Hadley\", \"Wickham\", email = \"hadley@rstudio.com\", role = \"cre\"),\n    person(\"Winston\", \"Chang\", email = \"winston@rstudio.com\", role = \"aut\"),\n    person(\"RStudio\", role = c(\"cph\", \"fnd\")))\nEvery package must have at least one author (aut) and one maintainer (cre) (they might be the same person). The maintainer (cre) must have an email address. These fields are used to generate the basic citation for the package (e.g. citation(\"pkgname\")). Only people listed as authors will be included in the auto-generated citation. There are a few extra details if you’re including code that other people have written, which you can learn about in Section 12.5.\nAn older, still valid approach is to have separate Maintainer and Author fields in DESCRIPTION. However, we strongly recommend the more modern approach of Authors@R and the person() function, because it offers richer metadata for various downstream uses.\n\n10.3.2 Url and BugReports\n\nAs well as the maintainer’s email address, it’s a good idea to list other places people can learn more about your package. The URL field is commonly used to advertise the package’s website and to link to a public source repository, where development happens. Multiple URLs are separated with a comma. BugReports is the URL where bug reports should be submitted, e.g., as GitHub issues. For example, devtools has:\nURL: https://devtools.r-lib.org/, https://github.com/r-lib/devtools\nBugReports: https://github.com/r-lib/devtools/issues\nIf you use usethis::use_github() to connect your local package to a remote GitHub repository, it will automatically populate URL and BugReports for you. If a package is already connected to a remote GitHub repository, usethis::use_github_links() can be called to just add the relevant links to DESCRIPTION.\n\n10.3.3 Other fields\nA few other DESCRIPTION fields are heavily used and worth knowing about:\n\nEncoding describes the character encoding of files throughout your package. Our package development workflow always assumes that this is set to Encoding: UTF-8 as this now the most commonly used text encoding, and we are not aware of any reasons to use a different value.\nCollate controls the order in which R files are sourced. This only matters if your code has side-effects; most commonly because you’re using S4.\nVersion is really important as a way of communicating where your package is in its lifecycle and how it is evolving over time. Learn more in Chapter 23.\nLazyData is relevant if your package makes data available to the user. If you specify LazyData: true, the datasets are lazy-loaded, which makes them more immediately available, i.e. users don’t have to use data(). The addition of LazyData: true is handled automatically by usethis::use_data(). More detail is given when we talk about external data in Chapter 8.\n\nThere are actually many other rarely, if ever, used fields. A complete list can be found in the “The DESCRIPTION file” section of the R extensions manual.\n\n10.3.4 Custom fields\nThere is also some flexibility to create your own fields to add additional metadata. In the narrowest sense, the only restriction is that you shouldn’t re-purpose the official field names used by R. You should also limit yourself to valid English words, so the field names aren’t flagged by the spell-check.\nIn practice, if you plan to submit to CRAN, we recommend that any custom field name should start with Config/. We featured an example of this earlier, where Config/Needs/website is used to record additional packages needed to build a package’s website.\nYou might notice that create_package() writes two more fields we haven’t discussed yet, relating to the use of the roxygen2 package for documentation:\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.1\nYou will learn more about these in Chapter 16. The use of these specific field names is basically an accident of history and, if it were re-done today, they would follow the Config/* pattern recommended above."
  },
  {
    "objectID": "Metadata.html#sec-namespace",
    "href": "Metadata.html#sec-namespace",
    "title": "10  DESCRIPTION and NAMESPACE",
    "section": "\n10.4 The NAMESPACE file",
    "text": "10.4 The NAMESPACE file\nThe following code is an excerpt of the NAMESPACE file from the testthat package.\n# Generated by roxygen2 (4.0.2): do not edit by hand\nS3method(as.character,expectation)\nS3method(compare,character)\nexport(auto_test)\nexport(auto_test_package)\nexport(colourise)\nexport(context)\nexportClasses(ListReporter)\nexportClasses(MinimalReporter)\nimportFrom(methods,setRefClass)\nuseDynLib(testthat,duplicate_)\nuseDynLib(testthat,reassign_function)\nYou can see that the NAMESPACE file looks a bit like R code. Each line contains a directive: S3method(), export(), exportClasses(), and so on. Each directive describes an R object, and says whether it’s exported from this package to be used by others, or it’s imported from another package to be used locally.\nIn total, there are eight namespace directives. Four describe exports:\n\n\nexport(): export functions (including S3 and S4 generics).\n\nexportPattern(): export all functions that match a pattern.\n\nexportClasses(), exportMethods(): export S4 classes and methods.\n\nS3method(): export S3 methods.\n\nAnd four describe imports:\n\n\nimport(): import all functions from a package.\n\nimportFrom(): import selected functions (including S4 generics).\n\nimportClassesFrom(), importMethodsFrom(): import S4 classes and methods.\n\nI don’t recommend writing these directives by hand. Instead, in this chapter you’ll learn how to generate the NAMESPACE file with roxygen2. There are three main advantages to using roxygen2:\n\nNamespace definitions live next to its associated function, so when you read the code it’s easier to see what’s being imported and exported.\nRoxygen2 abstracts away some of the details of NAMESPACE. You only need to learn one tag, @export, which will automatically generate the right directive for functions, S3 methods, S4 methods and S4 classes.\nRoxygen2 makes NAMESPACE tidy. No matter how many times you use @importFrom foo bar you’ll only get one importFrom(foo, bar) in your NAMESPACE. This makes it easy to attach import directives to every function that need them, rather than trying to manage in one central place.\n\nNote that you can choose to use roxygen2 to generate just NAMESPACE, just man/*.Rd, or both. If you don’t use any namespace related tags, roxygen2 won’t touch NAMESPACE. If you don’t use any documentation related tags, roxygen2 won’t touch man/."
  },
  {
    "objectID": "dependencies.html",
    "href": "dependencies.html",
    "title": "11  Dependencies: What does your package need?",
    "section": "",
    "text": "Second edition\n\n\n\nYou are reading the work-in-progress second edition of R Packages. This chapter is undergoing heavy restructuring and may be confusing or incomplete."
  },
  {
    "objectID": "dependencies.html#introduction",
    "href": "dependencies.html#introduction",
    "title": "11  Dependencies: What does your package need?",
    "section": "\n11.1 Introduction",
    "text": "11.1 Introduction\nA dependency is a code that your package needs to run. Dependencies are managed by two files. The DESCRIPTION manages dependencies at the package level; i.e. what packages needs to be installed for your package to work. R has a rich set of ways to describe different types of dependencies. A key point is whether a dependency is needed by regular users or is only needed for development tasks or optional functionality.\nThe flip-side is that your package will also be a dependency of other people’s scripts and packages. The job of the NAMESPACE is to define what functions you make available for others to use. The package namespace (as recorded in the NAMESPACE file) is one of the more confusing parts of building a package. It’s a fairly advanced topic, and by-and-large, not that important if you’re only developing packages for yourself. However, understanding namespaces is vital if you plan to submit your package to CRAN. This is because CRAN requires that your package plays nicely with other packages. When you first start using namespaces, it’ll seem like a lot of work for little gain. However, having a high quality namespace helps encapsulate your package and makes it self-contained. This ensures that other packages won’t interfere with your code, that your code won’t interfere with other packages, and that your package works regardless of the environment in which it’s run."
  },
  {
    "objectID": "dependencies.html#run-time-vs-develop-time-dependencies",
    "href": "dependencies.html#run-time-vs-develop-time-dependencies",
    "title": "11  Dependencies: What does your package need?",
    "section": "\n11.2 Run-time vs develop-time dependencies",
    "text": "11.2 Run-time vs develop-time dependencies\nPackages listed in Imports are needed by your users at runtime. The following lines indicate that your package absolutely needs both dplyr and tidyr to work.\nImports:\n    dplyr,\n    tidyr\nPackages listed in Suggests are either needed for development tasks or might unlock optional functionality for your users. The lines below indicate that, while your package can take advantage of ggplot2 and testthat, they’re not absolutely required:\nSuggests:\n    ggplot2,\n    testthat\nFor example, the withr package is very useful for writing tests that clean up after themselves. Such usage is compatible with listing withr in Suggests, since regular users don’t need to run the tests. But sometimes a package might also use withr in its own functions, perhaps to offer its own with_*() and local_*() functions. In that case, withr should be listed in Imports.\nBoth Imports and Suggests take a comma-separated list of package names. We recommend putting one package on each line, and keeping them in alphabetical order. A non-haphazard order makes it easier for humans to parse this field and appreciate changes. The easiest way to add a package to Imports or Suggests is with usethis::use_package(). If the dependencies are already in alphabetical order, use_package() will keep it that way. In general, it can be nice to run usethis::use_tidy_description() regularly, which orders and formats DESCRIPTION fields according to a fixed standard.\nImports and Suggests differ in the strength and nature of dependency:\n\n\nImports: packages listed here must be present for your package to work. Any time your package is installed, those packages will also be installed, if not already present. devtools::load_all() also checks that all packages in Imports are installed.\nAdding a package to Imports ensures it will be installed, but it does not mean that it will be attached along with your package, i.e. it does not do the equivalent of library(otherpkg)1. Inside your package, the best practice is to explicitly refer to external functions using the syntax package::function(). This makes it very easy to identify which functions live outside of your package. This is especially useful when you read your code in the future.\nIf you use a lot of functions from another package, this is rather verbose. There’s also a minor performance penalty associated with :: (on the order of 5µs, so it will only matter if you call the function millions of times). You’ll learn about alternative ways to make functions in other packages available inside your package in Section 11.5.4.\n\n\nSuggests: your package can use these packages, but doesn’t require them. You might use suggested packages for example datasets, to run tests, build vignettes, or maybe there’s only one function that needs the package.\nPackages listed in Suggests are not automatically installed along with your package. This means that you can’t assume the package is available unconditionally. Below we show various ways to handle these checks.\n\n\nIf you add packages to DESCRIPTION with usethis::use_package(), it will also remind you of the recommended way to call them.\n\nusethis::use_package(\"dplyr\") # Default is \"Imports\"\n#> ✔ Adding 'dplyr' to Imports field in DESCRIPTION\n#> • Refer to functions with `dplyr::fun()`\n\nusethis::use_package(\"ggplot2\", \"Suggests\")\n#> ✔ Adding 'ggplot2' to Suggests field in DESCRIPTION\n#> • Use `requireNamespace(\"ggplot2\", quietly = TRUE)` to test if package is installed\n#> • Then directly refer to functions with `ggplot2::fun()`\n\n\n11.2.1 Guarding the use of a suggested package\nInside a function in your own package, check for the availability of a suggested package with requireNamespace(\"pkg\", quietly = TRUE). There are two basic scenarios:\n\n# the suggested package is required \nmy_fun <- function(a, b) {\n  if (!requireNamespace(\"pkg\", quietly = TRUE)) {\n    stop(\n      \"Package \\\"pkg\\\" must be installed to use this function.\",\n      call. = FALSE\n    )\n  }\n  # code that includes calls such as pkg::f()\n}\n\n# the suggested package is optional; a fallback method is available\nmy_fun <- function(a, b) {\n  if (requireNamespace(\"pkg\", quietly = TRUE)) {\n    pkg::f()\n  } else {\n    g()\n  }\n}\n\nThe rlang package has some useful functions for checking package availability. Here’s how the checks around a suggested package could look if you use rlang:\n\n# the suggested package is required \nmy_fun <- function(a, b) {\n  rlang::check_installed(\"pkg\", reason = \"to use `my_fun()`\")\n  # code that includes calls such as pkg::f()\n}\n\n# the suggested package is optional; a fallback method is available\nmy_fun <- function(a, b) {\n  if (rlang::is_installed(\"pkg\")) {\n    pkg::f()\n  } else {\n    g()\n  }\n}\n\nThese rlang functions have handy features for programming, such as vectorization over pkg, classed errors with a data payload, and, for check_installed(), an offer to install the needed package in an interactive session.\nSuggests isn’t terribly relevant for packages where the user base is approximately equal to the development team or for packages that are used in a very predictable context. In that case, it’s reasonable to just use Imports for everything. Using Suggests is mostly a courtesy to external users or to accommodate very lean installations. It can free users from downloading rarely needed packages (especially those that are tricky to install) and lets them get started with your package as quickly as possible.\nAnother common place to use a suggested package is in an example and here we often guard with require() (but you’ll also see requireNamespace() used for this). This example is from ggplot2::coord_map().\n\n#' @examples\n#' if (require(\"maps\")) {\n#'   nz <- map_data(\"nz\")\n#'   # Prepare a map of NZ\n#'   nzmap <- ggplot(nz, aes(x = long, y = lat, group = group)) +\n#'     geom_polygon(fill = \"white\", colour = \"black\")\n#'  \n#'   # Plot it in cartesian coordinates\n#'   nzmap\n#' }\n\nAn example is basically the only place where we would use require() inside a package.\nAnother place you might use a suggested package is in a vignette. The tidyverse team generally writes vignettes as if all suggested packages are available. But if you choose to use suggested packages conditionally in your vignettes, the knitr chunk options purl and eval may be useful for achieving this. See Chapter 17 for more discussion of vignettes.\n\n11.2.1.1 Whether and how to guard in a test\nAs with vignettes, the tidyverse team does not usually guard the use of a suggested package in a test. In general, for vignettes and tests, we assume all suggested packages are available. The motivation for this posture is self-consistency and pragmatism. The key packages needed to run tests or build vignettes (e.g. testthat or knitr) appear in Suggests, not in Imports or Depends. Therefore, if the tests are actually executing or the vignettes are being built, that implies that an expansive notion of package dependencies has been applied. Also, empirically, in every important scenario of running R CMD check, the suggested packages are installed. This is generally true for CRAN and we ensure that it’s true in our own automated checks. However, it’s important to note that other package maintainers take a different stance and choose to protect all usage of suggested packages in their tests and vignettes.\nSometimes even the tidyverse team makes an exception and guards the use of a suggested package in a test. Here’s a test from ggplot2, which uses testthat::skip_if_not_installed() to skip execution if the suggested sf package is not available.\n\ntest_that(\"basic plot builds without error\", {\n  skip_if_not_installed(\"sf\")\n\n  nc_tiny_coords <- matrix(\n    c(-81.473, -81.741, -81.67, -81.345, -81.266, -81.24, -81.473,\n      36.234, 36.392, 36.59, 36.573, 36.437, 36.365, 36.234),\n    ncol = 2\n  )\n\n  nc <- sf::st_as_sf(\n    data_frame(\n      NAME = \"ashe\",\n      geometry = sf::st_sfc(sf::st_polygon(list(nc_tiny_coords)), crs = 4326)\n    )\n  )\n\n  expect_doppelganger(\"sf-polygons\", ggplot(nc) + geom_sf() + coord_sf())\n})\n\nWhat might justify the use of skip_if_not_installed()? In this case, the sf package can be nontrivial to install and it is conceivable that a contributor would want to run the remaining tests, even if sf is not available.\nFinally, note that testthat::skip_if_not_installed(pkg, minimum_version = \"x.y.z\") can be used to conditionally skip a test based on the version of the other package.\n\n11.2.2 Minimum versions\nIf you need a specific version of a package, specify it in parentheses after the package name:\nImports:\n    dplyr (>= 1.0.0),\n    tidyr (>= 1.1.0)\nYou always want to specify a minimum version (dplyr (>= 1.0.0)) rather than an exact version (dplyr (== 1.0.0)). Since R can’t have multiple versions of the same package loaded at the same time, specifying an exact dependency dramatically increases the chance of conflicting versions2.\nVersioning is most important if you will release your package for use by others. Usually people don’t have exactly the same versions of packages installed that you do. If someone has an older package that doesn’t have a function your package needs, they’ll get an unhelpful error message if your package does not advertise the minimum version it needs. However, if you state a minimum version, they’ll learn about this problem clearly, probably at the time of installing your package.\nThink carefully if you declare a minimum version for a dependency. In some sense, the safest thing to do is to require a version greater than or equal to the package’s current version. For public work, this is most naturally defined as the current CRAN version of a package; private or personal projects may adopt some other convention. But it’s important to appreciate the implications for people who try to install your package: if their local installation doesn’t fulfill all of your requirements around versions, installation will either fail or force upgrades of these dependencies. This is desirable if your minimum version requirements are genuine, i.e. your package would be broken otherwise. But if your stated requirements have a less solid rationale, this may be unnecessarily conservative and inconvenient.\nIn the absence of clear, hard requirements, you should set minimum versions (or not) based on your expected user base, the package versions they are likely to have, and a cost-benefit analysis of being too lax versus too conservative. The de facto policy of the tidyverse team is to specify a minimum version when using a known new feature or when someone encounters a version problem in authentic use. This isn’t perfect, but we don’t currently have the tooling to do better, and it seems to work fairly well in practice.\n\n11.2.3 Other dependencies\nThere are three other fields that allow you to express more specialised dependencies:\n\n\nDepends: Prior to the roll-out of namespaces in R 2.14.0 in 2011, Depends was the only way to “depend” on another package. Now, despite the name, you should almost always use Imports, not Depends. You’ll learn why, and when you should still use Depends, in Chapter 11.\nYou can also use Depends to state a minimum version for R itself, e.g. Depends: R (>= 4.0.0). Again, think carefully if you do this. This raises the same issues as setting a minimum version for a package you depend on, except the stakes are much higher when it comes to R itself. Users can’t simply consent to the necessary upgrade, so, if other packages depend on yours, your minimum version requirement for R can cause a cascade of package installation failures.\n\nThe backports package is useful if you want to use a function like tools::R_user_dir(), which was introduced in 4.0.0 in 2020, while still supporting older R versions.\nThe tidyverse packages officially support the current R version, the devel version, and four previous versions. We proactively test this support in the standard build matrix we use for continuous integration.\nPackages with a lower level of use may not need this level of rigour. The main takeaway is: if you state a minimum of R, you should have a reason and you should take reasonable measures to test your claim regularly.\n\n\nLinkingTo: if your package uses C or C++ code from another package, you need to list it here.\nEnhances: packages listed here are “enhanced” by your package. Typically, this means you provide methods for classes defined in another package (a sort of reverse Suggests). But it’s hard to define what that means, so we don’t recommend using Enhances.\n\nYou can also list things that your package needs outside of R in the SystemRequirements field. But this is just a plain text field and is not automatically checked. Think of it as a quick reference; you’ll also need to include detailed system requirements (and how to install them) in your README.\n\n\n11.2.3.1 An R version gotcha\nBefore we leave this topic, we give a concrete example of how easily an R version dependency can creep in and have a broader impact than you might expect. The saveRDS() function writes a single R object as an .rds file, an R-specific format. For almost 20 years, .rds files used the “version 2” serialization format. “Version 3” became the new default in R 3.6.0 (released April 2019) and cannot be read by R versions prior to 3.5.0 (released April 2018).\nMany R packages have at least one .rds file lurking within and, if that gets re-generated with a modern R version, by default, the new .rds file will have the “version 3” format. When that R package is next built, such as for a CRAN submission, the required R version is automatically bumped to 3.5.0, signaled by this message:\nNB: this package now depends on R (>= 3.5.0)\n  WARNING: Added dependency on R >= 3.5.0 because serialized objects in\n  serialize/load version 3 cannot be read in older versions of R.\n  File(s) containing such objects:\n    'path/to/some_file.rds'\nLiterally, the DESCRIPTION file in the bundled package says Depends: R (>= 3.5.0), even if DESCRIPTION in the source package says differently3.\nWhen such a package is released on CRAN, the new minimum R version is viral, in the sense that all packages listing the original package in Imports or even Suggests have, to varying degrees, inherited the new dependency on R >= 3.5.0.\nThe immediate take-away is to be very deliberate about the version of .rds files until R versions prior to 3.5.0 have fallen off the edge of what you intend to support. This particular .rds issue won’t be with us forever, but similar issues crop up elsewhere, such as in the standards implicit in compiled C or C++ source code. The broader message is that the more reverse dependencies your package has, the more thought you need to give to your package’s stated minimum versions, especially for R itself.\n\n\n11.2.4 Nonstandard dependencies\nIn packages developed with devtools, you may see DESCRIPTION files that use a couple other nonstandard fields for package dependencies specific to development tasks.\nThe Remotes field can be used when you need to install a dependency from a nonstandard place, i.e. from somewhere besides CRAN or Bioconductor. One common example of this is when you’re developing against a development version of one of your dependencies. During this time, you’ll want to install the dependency from its development repository, which is often GitHub. The way to specify various remote sources is described in a devtools vignette.\n\nThe dependency and any minimum version requirement still need to be declared in the normal way in, e.g., Imports. usethis::use_dev_package() helps to make the necessary changes in DESCRIPTION. If your package temporarily relies on a development version of usethis, the affected DESCRIPTION fields might evolve like this:\n\nStable -->               Dev -->                       Stable again\n----------------------   ---------------------------   ----------------------\nPackage: yourpkg         Package: yourpkg              Package: yourpkg\nVersion: 1.0.0           Version: 1.0.0.9000           Version: 1.1.0\nImports:                 Imports:                      Imports: \n    usethis (>= 2.1.3)       usethis (>= 2.1.3.9000)       usethis (>= 2.2.0)\n                         Remotes:   \n                             r-lib/usethis \nIt’s important to note that you should not submit your package to CRAN in the intermediate state, meaning with a Remotes field and with a dependency required at a version that’s not available from CRAN or Bioconductor. For CRAN packages, this can only be a temporary development state, eventually resolved when the dependency updates on CRAN and you can bump your minimum version accordingly.\nYou may also see devtools-developed packages with packages listed in DESCRIPTION fields in the form of Config/Needs/*. This pattern takes advantage of the fact that fields prefixed with Config/ are ignored by CRAN and also do not trigger a NOTE about “Unknown, possibly mis-spelled, fields in DESCRIPTION”.\n\nThe use of Config/Needs/* is not directly related to devtools. It’s more accurate to say that it’s associated with continuous integration workflows made available to the community at https://github.com/r-lib/actions/ and exposed via functions such as usethis::use_github_actions(). A Config/Needs/* field tells the setup-r-dependencies GitHub Action about extra packages that need to be installed.\nConfig/Needs/website is the most common and it provides a place to specify packages that aren’t a formal dependency, but that must be present in order to build the package’s website. On the left is an example of what might appear in DESCRIPTION for a package that uses various tidyverse packages in the non-vignette articles on its website, which is also formatted with styling that lives in the tidyverse/template GitHub repo. On the right is the corresponding excerpt from the configuration of the workflow that builds and deploys the website.\nin DESCRIPTION                  in .github/workflows/pkgdown.yaml\n--------------------------      ---------------------------------\nConfig/Needs/website:           - uses: r-lib/actions/setup-r-dependencies@v1\n    tidyverse,                    with:\n    tidyverse/tidytemplate          extra-packages: pkgdown\n                                    needs: website\nContinuous integration and package websites are discussed more in ?? and ??, respectively. These chapters are a yet-to-be-(re)written task for the 2nd edition.\n\nThe Config/Needs/* convention is handy because it allows a developer to use DESCRIPTION as their definitive record of package dependencies, while maintaining a clean distinction between true runtime dependencies versus those that are only needed for specialized development tasks."
  },
  {
    "objectID": "dependencies.html#exports",
    "href": "dependencies.html#exports",
    "title": "11  Dependencies: What does your package need?",
    "section": "\n11.3 Exports",
    "text": "11.3 Exports\nFor a function to be usable outside of your package, you must export it. When you create a new package with usethis::create_package(), nothing is exported at first. You can still experiment interactively with load_all() (since that loads all functions, not just those that are exported). But if you install and reload the package, you’ll notice that no functions are available.\nTo export an object, put @export in its roxygen block. For example:\n\n#' @export\nfoo <- function(x, y, z) {\n  ...\n}\n\nThis will then generate export(), exportMethods(), exportClass() or S3method() depending on the type of the object.\nYou export functions that you want other people to use. Exported functions must be documented, and you must be cautious when changing their interface — other people are using them! Generally, it’s better to export too little than too much. It’s easy to export things that you didn’t before; it’s hard to stop exporting a function because it might break existing code. Always err on the side of caution, and simplicity. It’s easier to give people more functionality than it is to take away stuff they’re used to.\nI believe that packages that have a wide audience should strive to do one thing and do it well. All functions in a package should be related to a single problem (or a set of closely related problems). Any functions not related to that purpose should not be exported. For example, most of my packages have a utils.R file that contains many small functions that are useful for me, but aren’t part of the core purpose of those packages. I never export these functions.\n\n# Defaults for NULL values\n`%||%` <- function(a, b) if (is.null(a)) b else a\n\n# Remove NULLs from a list\ncompact <- function(x) {\n  x[!vapply(x, is.null, logical(1))]\n}\n\nThat said, if you’re creating a package for yourself, it’s far less important to be so disciplined. Because you know what’s in your package, it’s fine to have a local “misc” package that contains a passel of functions that you find useful. But I don’t think you should release such a package.\nThe following sections describe what you should export if you’re using S3, S4 or RC."
  },
  {
    "objectID": "dependencies.html#when-should-you-take-a-dependency",
    "href": "dependencies.html#when-should-you-take-a-dependency",
    "title": "11  Dependencies: What does your package need?",
    "section": "\n11.4 When should you take a dependency?",
    "text": "11.4 When should you take a dependency?\nhttps://www.tidyverse.org/blog/2019/05/itdepends/\n\n11.4.1 Tidyverse dependencies\nHere’s our policies about the role of different packages. use_tidy_dependencies(). “Free” dependencies."
  },
  {
    "objectID": "dependencies.html#namespace",
    "href": "dependencies.html#namespace",
    "title": "11  Dependencies: What does your package need?",
    "section": "\n11.5 Namespace",
    "text": "11.5 Namespace\n\n11.5.1 Motivation\nAs the name suggests, namespaces provide “spaces” for “names”. They provide a context for looking up the value of an object associated with a name.\nWithout knowing it, you’ve probably already used namespaces. For example, have you ever used the :: operator? It disambiguates functions with the same name. For example, both plyr and Hmisc provide a summarize() function. If you load plyr, then Hmisc, summarize() will refer to the Hmisc version. But if you load the packages in the opposite order, summarize() will refer to the plyr version. This can be confusing. Instead, you can explicitly refer to specific functions: Hmisc::summarize() and plyr::summarize(). Then the order in which the packages are loaded won’t matter.\nNamespaces make your packages self-contained in two ways: the imports and the exports. The imports defines how a function in one package finds a function in another. To illustrate, consider what happens when someone changes the definition of a function that you rely on: for example, the simple nrow() function in base R:\n\nnrow\n#> function (x) \n#> dim(x)[1L]\n#> <bytecode: 0x559f328d8458>\n#> <environment: namespace:base>\n\nIt’s defined in terms of dim(). So what will happen if we override dim() with our own definition? Does nrow() break?\n\ndim <- function(x) c(1, 1)\ndim(mtcars)\n#> [1] 1 1\nnrow(mtcars)\n#> [1] 32\n\nSurprisingly, it does not! That’s because when nrow() looks for an object called dim(), it uses the package namespace, so it finds dim() in the base environment, not the dim() we created in the global environment.\nThe exports helps you avoid conflicts with other packages by specifying which functions are available outside of your package (internal functions are available only within your package and can’t easily be used by another package). Generally, you want to export a minimal set of functions; the fewer you export, the smaller the chance of a conflict. While conflicts aren’t the end of the world because you can always use :: to disambiguate, they’re best avoided where possible because it makes the lives of your users easier.\n\n11.5.2 Search path\nTo understand why namespaces are important, you need a solid understanding of search paths. To call a function, R first has to find it. R does this by first looking in the global environment. If R doesn’t find it there, it looks in the search path, the list of all the packages you have attached. You can see this list by running search(). For example, here’s the search path for the code in this book:\n\nsearch()\n#>  [1] \".GlobalEnv\"        \"tools:quarto\"      \"package:stats\"    \n#>  [4] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n#>  [7] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \n#> [10] \"package:base\"\n\nThere’s an important difference between loading and attaching a package. Normally when you talk about loading a package you think of library(), but that actually attaches the package.\nIf a package is installed,\n\nLoading will load code, data and any DLLs; register S3 and S4 methods; and run the .onLoad() function. After loading, the package is available in memory, but because it’s not in the search path, you won’t be able to access its components without using ::. Confusingly, :: will also load a package automatically if it isn’t already loaded. It’s rare to load a package explicitly, but you can do so with requireNamespace() or loadNamespace().\nAttaching puts the package in the search path. You can’t attach a package without first loading it, so both library() or require() load then attach the package. You can see the currently attached packages with search().\n\nIf a package isn’t installed, loading (and hence attaching) will fail with an error.\nTo see the differences more clearly, consider two ways of running expect_that() from the testthat package. If we use library(), testthat is attached to the search path. If we use ::, it’s not.\n\nold <- search()\ntestthat::expect_equal(1, 1)\nsetdiff(search(), old)\n#> character(0)\nexpect_true(TRUE)\n#> Error in expect_true(TRUE): could not find function \"expect_true\"\n    \nlibrary(testthat)\nexpect_equal(1, 1)\nsetdiff(search(), old)\n#> [1] \"package:testthat\"\nexpect_true(TRUE)\n\nThere are four functions that make a package available. They differ based on whether they load or attach, and what happens if the package is not found (i.e., throws an error or returns FALSE).\n\n\n\n\n\n\n\n\nThrows error\nReturns FALSE\n\n\n\n\nLoad\nloadNamespace(\"x\")\nrequireNamespace(\"x\", quietly = TRUE)\n\n\nAttach\nlibrary(x)\nrequire(x, quietly = TRUE)\n\n\n\nOf the four, you should only ever use two:\n\nUse library(x) in data analysis scripts. It will throw an error if the package is not installed, and will terminate the script. You want to attach the package to save typing. Never use library() in a package.\nUse requireNamespace(\"x\", quietly = TRUE) inside a package if you want a specific action (e.g. throw an error) depending on whether or not a suggested package is installed.\n\nYou never need to use require() (requireNamespace() is almost always better), or loadNamespace() (which is only needed for internal R code). You should never use require() or library() in a package: instead, use the Depends or Imports fields in the DESCRIPTION.\nNow’s a good time to come back to an important issue which we glossed over earlier. What’s the difference between Depends and Imports in the DESCRIPTION? When should you use one or the other?\nListing a package in either Depends or Imports ensures that it’s installed when needed. The main difference is that where Imports just loads the package, Depends attaches it. There are no other differences. The rest of the advice in this chapter applies whether or not the package is in Depends or Imports.\nUnless there is a good reason otherwise, you should always list packages in Imports not Depends. That’s because a good package is self-contained, and minimises changes to the global environment (including the search path). The only exception is if your package is designed to be used in conjunction with another package. For example, the analogue package builds on top of vegan. It’s not useful without vegan, so it has vegan in Depends instead of Imports. Similarly, ggplot2 should really Depend on scales, rather than Importing it.\nNow that you understand the importance of the namespace, let’s dive into the nitty gritty details. The two sides of the package namespace, imports and exports, are both described by the NAMESPACE. You’ll learn what this file looks like in the next section. In the section after that, you’ll learn the details of exporting and importing functions and other objects.\n\n11.5.3 Workflow\nGenerating the namespace with roxygen2 is just like generating function documentation with roxygen2. You use roxygen2 blocks (starting with #') and tags (starting with @). The workflow is the same:\n\nAdd roxygen comments to your .R files.\nRun devtools::document() (or press Ctrl/Cmd + Shift + D in RStudio) to convert roxygen comments to .Rd files.\nLook at NAMESPACE and run tests to check that the specification is correct.\nRinse and repeat until the correct functions are exported.\n\n11.5.4 Imports\nNAMESPACE also controls which external functions can be used by your package without having to use ::.\nIt’s confusing that both DESCRIPTION (through the Imports field) and NAMESPACE (through import directives) seem to be involved in imports. This is just an unfortunate choice of names. The Imports field really has nothing to do with functions imported into the namespace: it just makes sure the package is installed when your package is. It doesn’t make functions available. You need to import functions in exactly the same way regardless of whether or not the package is attached.\nDepends is just a convenience for the user: if your package is attached, it also attaches all packages listed in Depends. If your package is loaded, packages in Depends are loaded, but not attached, so you need to qualify function names with :: or specifically import them.\nIt’s common for packages to be listed in Imports in DESCRIPTION, but not in NAMESPACE. The converse is not true. Every package mentioned in NAMESPACE must also be present in the Imports or Depends fields.\n\n11.5.5 R functions\nIf you are using just a few functions from another package, my recommendation is to note the package name in the Imports: field of the DESCRIPTION file and call the function(s) explicitly using ::, e.g., pkg::fun().\nIf you are using functions repeatedly, you can avoid :: by importing the function with @importFrom pkg fun. Operators can also be imported in a similar manner, e.g., @importFrom magrittr %>%.\nAlternatively, if you are repeatedly using many functions from another package, you can import all of them using @import package. This is the least recommended solution because it makes your code harder to read (you can’t tell where a function is coming from), and if you @import many packages, it increases the chance of conflicting function names."
  },
  {
    "objectID": "license.html#introduction",
    "href": "license.html#introduction",
    "title": "12  Licensing",
    "section": "\n12.1 Introduction",
    "text": "12.1 Introduction\nThe goal of this chapter is to give you the basic tools to manage licensing for your R package. Software licensing is a large and complicated field, made particularly complex because it lies at the intersection of programming and law. Fortunately, you don’t need to be an expert to do the right thing: respecting how an author wants their code to be treated as indicated by the license they’ve picked.\nTo understand the author’s wishes, it’s useful to understand the two major camps of open source licenses:\n\nPermissive licenses are very easy going. Code with a permissive license can be freely copied, modified, and published, and the only restriction is that the license must be preserved. The MIT and Apache licenses are the most common modern permissive licenses; older permissive licenses include the various forms of the BSD license.\nCopyleft licenses are stricter. The most common copyleft license is the GPL which allows you to freely copy and modify the code for personal use, but if you publish modified versions or bundle with other code, the modified version or complete bundle must also be licensed with the GPL.\n\nTo get a high-level view of the open source licensing space, and the details of individual licenses, I highly recommend https://choosealicense.com, which I’ve used in the links above.\nWhen you look across all programming languages, permissive licenses are the most common. For example, a 2015 survey of GitHub repositories found that ~55% used a permissive license and ~20% used a copyleft license. The R community is a little different: as of 2020, my analysis (following Sean Kross’s blog post) found that ~70% of CRAN packages use a copyleft license and ~15% use a permissive license.\nThis chapter will start with licensing your own code, and then cover the most important details of receiving code from other people (e.g. in a PR) and bundling other people’s code into your package. Note that simply using a package or R itself doesn’t require that you comply with the license; this is why you can write proprietary R code and why R packages can have any license you choose.\nFor more details about licensing R packages, I recommend Licensing R by Colin Fay.\n(If you run the code in this chapter, please make sure that you’re using usethis 2.0.0 or greater; writing this chapter prompted a number of changes in the package.)"
  },
  {
    "objectID": "license.html#code-you-write",
    "href": "license.html#code-you-write",
    "title": "12  Licensing",
    "section": "\n12.2 Code you write",
    "text": "12.2 Code you write\nWe’ll start by talking about code that you write, and how to license it to make clear how you want people to treat it. In brief:\n\nIf you want a permissive license so people can use your code with minimal restrictions, choose the MIT license with use_mit_license().\nIf you want a copyleft license so that all derivatives and bundles of your code are also open source, choose the GPLv3 license with use_gpl_license().\nIf your package primarily contains data, not code, and you want minimal restrictions, choose the CC0 license with use_cc0_license(). Or if you want to require attribution when your data is used, choose the CC BY license by calling use_ccby_license().\nIf you don’t want to make your code open source call use_proprietary_license(). Such packages can not be distributed by CRAN.\n\nWe’ll come back to more details and present a few other licenses in Section 12.3.2.\nIt’s important to use a license because if you don’t the default copyright laws apply which mean that no one is allowed to make a copy of your code without your express permission.\n(It is possible to license a CRAN package with a non-open source license like the ACM license but we don’t recommend it.)"
  },
  {
    "objectID": "license.html#copyright-holder",
    "href": "license.html#copyright-holder",
    "title": "12  Licensing",
    "section": "\n12.3 Copyright holder",
    "text": "12.3 Copyright holder\nBefore we go any further, it’s important to introduce an important term: copyright holder. The copyright holder (or holders) are the people who own the underlying copyright on the code, and are hence the only people who are allowed to choose (or later change) the license. There are three main cases1:\n\nIf you wrote the code in your own time, you’re the copyright holder.\nIf you wrote the code for your employer, your employer is the copyright holder.\nIf you wrote the code for contract work, you’re the copyright holder unless the contract specifically describes otherwise.\n\nThis means that if you’re writing a package for your job, you’ll need to get your employer to approve the open source license you use. Some employers (particularly universities) have standard policies so you don’t need to ask for permission every time; you’ll need to investigate what your company’s policy is.\nNote that if multiple people have contributed to the package, there will be multiple copyright holders: each person or company will hold the copyright for their specific contribution. We’ll come back to this topic in Section 12.5.\n\n12.3.1 Key files\nThere are three key files used to record your licensing decision:\n\n\nEvery license sets the License field in the DESCRIPTION. This contains the name of the license in a standard form so that R CMD check and CRAN can automatically verify it. It comes in four main forms:\n\nA name and version specification, e.g. GPL (>= 2), or Apache License (== 2.0).\nA standard abbreviation, e.g. GPL-2, LGPL-2.1, Artistic-2.0.\nA name of a license “template” and a file containing specific variables. The most common case is MIT + file LICENSE, where the LICENSE file needs to contain two fields: the year and copyright holder.\nPointer to the full text of a non-standard license, file LICENSE.\n\nMore complicated licensing structures are possible but outside the scope of this text. See the Licensing section of R-exts for details.\n\nAs described above, the LICENSE file is used in one of two ways. Some licenses are templates that require additional details to be complete in the LICENSE file. The LICENSE file can also contain the full text of non-standard and non-open source licenses. You are not permitted to include the full text of standard licenses.\nLICENSE.md includes a copy of the full text of the license. All open source licenses require a copy of the license to be included, but CRAN does not permit you to include a copy of standard licenses in your package, so we also use .Rbuildignore to make sure this file is not sent to CRAN.\n\nThere is one other file that we’ll come back to in Section 12.5.2: LICENSE.note. This is used when you have bundled code written by other people, and parts of your package have more permissive licenses than the whole.\n\n12.3.2 More licenses\nI gave you the absolute minimum you need to know above. But it’s worth mentioning a few more important licenses roughly ordered from most permissive to least permissive:\n\nuse_mit_license(): the MIT license is the most permissive license only requiring that you keep the copyright and license notice.\nuse_apache_license(): the Apache License is similar to the MIT license but it also includes an explicit patent grant. Patents are another component of intellectual property distinct from copyrights, and some organisations also care about protection from patent claims.\nuse_lgpl_license(): the LGPL is a little weaker than the GPL, allowing you to bundle LPGL code using any license for the larger work.\nuse_gpl_license(): We’ve discussed the GPL already, but there’s one important wrinkle to note — the GPL has two major versions, GPLv2 and GPLv3, and they’re not compatible (i.e. you can’t bundle GPLv2 and GPLv3 code in the same project). To avoid this problem it’s generally recommended to license your package as GPL >=2 or GPL >= 3 so that future versions of the GPL license also apply to your code. This is what use_gpl_license() does by default.\nuse_agpl_license(): The AGPL defines distribution to include providing a service over a network, so that if you use AGPL code to provide a web service, all bundled code must also be open-sourced. Because this is a considerably broader claim than the GPL, many companies expressly forbid the use of AGPL software.\n\nThere are many other licenses available. You can see a list of the most popular at https://choosealicense.com/licenses/ and a full list at https://opensource.org/licenses/alphabetical. The primary downside of choosing a license not in the bullet list above is that fewer R users will understand what it means.\n\n12.3.3 Relicensing\nIt’s important to spend a little time thinking about your initial license because it can be hard to change it later because it requires the permission of all copyright holders. Unless you’ve done something special (which we’ll discuss in Section 12.4), the copyright holders include everyone who has contributed a non-trivial amount of code.\nIf you do need to re-license a package, we recommend the following steps:\n\nCheck the Authors@R field in the DESCRIPTION to confirm that the package doesn’t contain bundled code (which we’ll talk about in Section 12.5).\nFind all contributors by looking at the Git history or the contributors display on GitHub.\nOptionally, inspect the specific contributions and remove people who only contributed typo fixes and similar2.\nAsk every contributor if they’re OK with changing the license. If every contributor is on GitHub, the easiest way to do this is to create an issue where you list all contributors and ask them to confirm that they’re OK with the change. Two examples where the tidyverse team has relicensed code include generics and covr.\nOnce all copyright holders have approved, make the change by calling the appropriate license function.\n\nYou can read about how the tidyverse followed this process to unify on the MIT license at https://www.tidyverse.org/blog/2021/12/relicensing-packages/.\n\n12.3.4 Data\nOpen source licenses are designed specifically to apply to source code, so if you’re releasing a package that primarily contains data, you should use a different type of license. We recommend one of two Creative Commons licenses:\n\nIf you want to make the data as freely available as possible, you use the CC0 license with use_cc0_license(). This is a permissive license that’s equivalent to the MIT license (but applies to data, not code).\nIf you want to require attribution when someone else uses your data, you can use the CC-BY license, with use_ccby_license()."
  },
  {
    "objectID": "license.html#sec-code-given-to-you",
    "href": "license.html#sec-code-given-to-you",
    "title": "12  Licensing",
    "section": "\n12.4 Code given to you",
    "text": "12.4 Code given to you\nMany packages include code not written by the author. There are two main ways this happens: other people might choose to contribute to your package using a pull request or similar, or you might find some code and choose to bundle it. This section will discuss code that others give to you, and the next section will discuss code that you bundle.\nWhen someone contributes code to your package using a pull request or similar, you can assume that the author is happy for their code to use your license. This is explicit in the GitHub terms of service, but is generally considered to be true regardless of how the code is contributed3.\nNote, however, that the author retains copyright of their code, unless you use a “contributor license agreement” or CLA for short. The primary advantage of a CLA is that it makes the copyright of the code very simple, and hence makes it easy to relicense code if needed. This is most important for dual open-source/commercial projects because it easily allows for dual licensing where the code is made available to the world with copyleft license, and made available to paying customers with a different, more permissive, license.\nIt’s also important to acknowledge the contribution, and it’s good practice to be generous with thanks and attribution. In the tidyverse, we ask that all code contributors include a bullet in NEWS.md with their GitHub username, and we thank all contributors in release announcements. We only add core developers4 to the DESCRIPTION file; but some projects choose to add all contributors no matter how small."
  },
  {
    "objectID": "license.html#sec-code-you-bundle",
    "href": "license.html#sec-code-you-bundle",
    "title": "12  Licensing",
    "section": "\n12.5 Code you bundle",
    "text": "12.5 Code you bundle\nThere are three common reasons that you might choose to bundle code written by someone else:\n\nYou’re including someone else’s CSS or JS library in order to create a useful and attractive web page or HTML widgets.\nYou’re providing an R wrapper for a simple C or C++ library. (For complex C/C++ libraries, you don’t usually bundle the code in your package, but instead link to a copy installed elsewhere on the system).\nYou’ve copied a small amount of R code from another package to avoid taking a dependency. Generally, taking a dependency on another package is the right thing to do because you don’t need to worry about licensing, and you’ll automatically get bug fixes. But sometimes you only need a very small amount of code from a big package, and copying and pasting it into your package is the right thing to do.\n\nNote that R is rather different to languages like C where the most common way that code is bundled together is by compiling it into a single executable.\n\n12.5.1 License compatibility\nBefore you bundle someone else’s code into your package, you need to first check that the bundled license is compatible with your license. When distributing code, you can add additional restrictions, but you can not remove restrictions, which means that license compatibility is not symmetric. For example, you can bundle MIT licensed code in a GPL licensed package, but you can not bundle GPL licensed code in an MIT licensed package.\nThere are five main cases to consider:\n\nIf your license and their license are the same: it’s OK to bundle.\nIf their license is MIT or BSD, it’s OK to bundle.\nIf their code has a copyleft license and your code has a permissive license, you can’t bundle their code. You’ll need to consider an alternative approach, either looking for code with a more permissive license, or putting the external code in a separate package.\nIf the code comes from Stack Overflow, it’s licensed5 with the Creative Common CC BY-SA license, which is only compatible with GPLv36 . This means that you need to take extra care when using Stack Overflow code in open source packages . Learn more at https://empirical-software.engineering/blog/so-snippets-in-gh-projects.\nOtherwise, you’ll need to do a little research. Wikipedia has a useful diagram and Google is your friend. It’s important to note that different versions of the same license are not necessarily compatible, e.g. GPLv2 and GPLv3 are not compatible.\n\nIf your package isn’t open source, things are more complicated. Permissive licenses are still easy, and copyleft licenses generally don’t restrict use as long as you don’t distribute the package outside your company. But this is a complex issue and opinions differ, and should check with your legal department first.\n\n12.5.2 How to include\nOnce you’ve determined that the licenses are compatible, you can bring the code in your package. When doing so, you need to preserve all existing license and copyright statements, and make it as easy as possible for future readers to understanding the licensing situation:\n\nIf you’re including a fragment of another project, generally best to put in its own file and ensure that file has copyright statements and license description at the top.\nIf you’re including multiple files, put in a directory, and put a LICENSE file in that directory.\n\nYou also need to include some standard metadata in Authors@R. You should use role = \"cph\" to declare that the author is a copyright holder, with a comment describing what they’re the author of.\nIf you’re submitting to CRAN and the bundled code has a different (but compatible) license, you also need to include a LICENSE.note file that describes the overall license of the package, and the specific licenses of each individual component. For example, the diffviewer package bundles six javascript libraries all of which use a permissive license. The DESCRIPTION lists all copyright holders, and the LICENSE.note describes their licenses. (Other packages use other techniques, but I think this is the simplest approach that will fly with CRAN.)"
  },
  {
    "objectID": "testing-basics.html#introduction",
    "href": "testing-basics.html#introduction",
    "title": "13  Testing basics",
    "section": "\n13.1 Introduction",
    "text": "13.1 Introduction\nTesting is a vital part of package development. It ensures that your code does what you want it to do. Testing, however, adds an additional step to your development workflow. The goal of this chapter is to show you how to make this task easier and more effective by doing formal automated testing using the testthat package.\nThe first stage of your testing journey is to become convinced that testing has enough benefits to justify the work. For some of us, this is easy to accept. Others must learn the hard way.\nOnce you’ve decided to embrace automated testing, it’s time to learn some mechanics and figure out where testing fits into your development workflow.\nAs you and your R packages evolve, you’ll start to encounter testing situations where it’s fruitful to use techniques that are somewhat specific to testing and differ from what we do below R/."
  },
  {
    "objectID": "testing-basics.html#why-is-formal-testing-worth-the-trouble",
    "href": "testing-basics.html#why-is-formal-testing-worth-the-trouble",
    "title": "13  Testing basics",
    "section": "\n13.2 Why is formal testing worth the trouble?",
    "text": "13.2 Why is formal testing worth the trouble?\nUp until now, your workflow probably looks like this:\n\nWrite a function.\nLoad it with devtools::load_all(), maybe via Ctrl/Cmd + Shift + L.\nExperiment with it in the console to see if it works.\nRinse and repeat.\n\nWhile you are testing your code in this workflow, you’re only doing it informally. The problem with this approach is that when you come back to this code in 3 months time to add a new feature, you’ve probably forgotten some of the informal tests you ran the first time around. This makes it very easy to break code that used to work.\nMany of us embrace automated testing when we realize we’re re-fixing a bug for the second or fifth time. While writing code or fixing bugs, we might perform some interactive tests to make sure the code we’re working on does what we want. But it’s easy to forget all the different use cases you need to check, if you don’t have a system for storing and re-running the tests. This is a common practice among R programmers. The problem is not that you don’t test your code, it’s that you don’t automate your tests.\nIn this chapter you’ll learn how to transition from informal ad hoc testing, done interactively in the console, to automated testing (also known as unit testing). While turning casual interactive tests into formal tests requires a little more work up front, it pays off in four ways:\n\n\nFewer bugs. Because you’re explicit about how your code should behave, you will have fewer bugs. The reason why is a bit like the reason double entry book-keeping works: because you describe the behaviour of your code in two places, both in your code and in your tests, you are able to check one against the other.\nWith informal testing, it’s tempting to just explore typical and authentic usage, similar to writing examples. However, when writing formal tests, it’s natural to adopt a more adversarial mindset and to anticipate how unexpected inputs could break your code.\nIf you always introduce new tests when you add a new feature or function, you’ll prevent many bugs from being created in the first place, because you will proactively address pesky edge cases. Tests also keep you from (re-)breaking one feature, when you’re tinkering with another.\n\nBetter code structure. Code that is well designed tends to be easy to test and you can turn this to your advantage. If you are struggling to write tests, consider if the problem is actually the design of your function(s). The process of writing tests is a great way to get free, private, and personalized feedback on how well-factored your code is. If you integrate testing into your development workflow (versus planning to slap tests on “later”), you’ll subject yourself to constant pressure to break complicated operations into separate functions that work in isolation. Functions that are easier to test are usually easier to understand and re-combine in new ways.\nCall to action. When we start to fix a bug, we first like to convert it into a (failing) test. This is wonderfully effective at making your goal very concrete: make this test pass. This is basically a special case of a general methodology known as test driven development.\nRobust code. If you know that all the major functionality of your package is well covered by the tests, you can confidently make big changes without worrying about accidentally breaking something. This provides a great reality check when you think you’ve discovered some brilliant new way to simplify your package. Sometimes such “simplifications” fail to account for some important use case and your tests will save you from yourself."
  },
  {
    "objectID": "testing-basics.html#introducing-testthat",
    "href": "testing-basics.html#introducing-testthat",
    "title": "13  Testing basics",
    "section": "\n13.3 Introducing testthat",
    "text": "13.3 Introducing testthat\nThis chapter describes how to test your R package using the testthat package: https://testthat.r-lib.org\nIf you’re familiar with frameworks for unit testing in other languages, you should note that there are some fundamental differences with testthat. This is because R is, at heart, more a functional programming language than an object-oriented programming language. For instance, because R’s main object-oriented systems (S3 and S4) are based on generic functions (i.e., methods belong to functions not classes), testing approaches built around objects and methods don’t make much sense.\ntestthat 3.0.0 (released 2020-10-31) introduced the idea of an edition of testthat, specifically the third edition of testthat, which we refer to as testthat 3e. An edition is a bundle of behaviors that you have to explicitly choose to use, allowing us to make otherwise backward incompatible changes. This is particularly important for testthat since it has a very large number of packages that use it (almost 5,000 at last count). To use testthat 3e, you must have a version of testthat >= 3.0.0 and explicitly opt-in to the third edition behaviors. This allows testthat to continue to evolve and improve without breaking historical packages that are in a rather passive maintenance phase. You can learn more in the testthat 3e article and the blog post Upgrading to testthat edition 3.\nWe recommend testthat 3e for all new packages and we recommend updating existing, actively maintained packages to use testthat 3e. Unless we say otherwise, this chapter describes testthat 3e."
  },
  {
    "objectID": "testing-basics.html#sec-tests-mechanics-workflow",
    "href": "testing-basics.html#sec-tests-mechanics-workflow",
    "title": "13  Testing basics",
    "section": "\n13.4 Test mechanics and workflow",
    "text": "13.4 Test mechanics and workflow\n\n13.4.1 Initial setup\nTo setup your package to use testthat, run:\n\nusethis::use_testthat(3)\n\nThis will:\n\nCreate a tests/testthat/ directory.\n\nAdd testthat to the Suggests field in the DESCRIPTION and specify testthat 3e in the Config/testthat/edition field. The affected DESCRIPTION fields might look like:\nSuggests: testthat (>= 3.0.0)\nConfig/testthat/edition: 3\n\n\nCreate a file tests/testthat.R that runs all your tests when R CMD check runs. (You’ll learn more about that in Chapter 20.) For a package named “pkg”, the contents of this file will be something like:\n\nlibrary(testthat)\nlibrary(pkg)\n\ntest_check(\"pkg\")\n\n\n\nThis initial setup is usually something you do once per package. However, even in a package that already uses testthat, it is safe to run use_testthat(3), when you’re ready to opt-in to testthat 3e.\nDo not edit tests/testthat.R! It is run during R CMD check (and, therefore, devtools::check()), but is not used in most other test-running scenarios (such as devtools::test() or devtools::test_active_file()). If you want to do something that affects all of your tests, there is almost always a better way than modifying the boilerplate tests/testthat.R script. This chapter details many different ways to make objects and logic available during testing.\n\n13.4.2 Create a test\nAs you define functions in your package, in the files below R/, you add the corresponding tests to .R files in tests/testthat/. We strongly recommend that the organisation of test files match the organisation of R/ files, discussed in Section 7.2: The foofy() function (and its friends and helpers) should be defined in R/foofy.R and their tests should live in tests/testthat/test-foofy.R.\nR                                     tests/testthat\n└── foofy.R                           └── test-foofy.R\n    foofy <- function(...) {...}          test_that(\"foofy does this\", {...})\n                                          test_that(\"foofy does that\", {...})\nEven if you have different conventions for file organisation and naming, note that testthat tests must live in files below tests/testthat/ and these file names must begin with test. The test file name is displayed in testthat output, which provides helpful context1.\n\nusethis offers a helpful pair of functions for creating or toggling between files:\n\nusethis::use_r()\nusethis::use_test()\n\nEither one can be called with a file (base) name, in order to create a file de novo and open it for editing:\n\nuse_r(\"foofy\")    # creates and opens R/foofy.R\nuse_test(\"blarg\") # creates and opens tests/testthat/test-blarg.R\n\nThe use_r() / use_test() duo has some convenience features that make them “just work” in many common situations:\n\nWhen determining the target file, they can deal with the presence or absence of the .R extension and the test- prefix.\n\nEquivalent: use_r(\"foofy.R\"), use_r(\"foofy\")\n\nEquivalent: use_test(\"test-blarg.R\"), use_test(\"blarg.R\"), use_test(\"blarg\")\n\n\n\nIf the target file already exists, it is opened for editing. Otherwise, the target is created and then opened for editing.\n\n\n\n\n\n\n\nRStudio\n\n\n\nIf R/foofy.R is the active file in your source editor, you can even call use_test() with no arguments! The target test file can be inferred: if you’re editing R/foofy.R, you probably want to work on the companion test file, tests/testthat/test-foofy.R. If it doesn’t exist yet, it is created and, either way, the test file is opened for editing. This all works the other way around also. If you’re editing tests/testthat/test-foofy.R, a call to use_r() (optionally, creates and) opens R/foofy.R.\n\n\nBottom line: use_r() / use_test() are handy for initially creating these file pairs and, later, for shifting your attention from one to the other.\nWhen use_test() creates a new test file, it inserts an example test:\n\ntest_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})\n\nYou will replace this with your own description and logic, but it’s a nice reminder of the basic form:\n\nA test file holds one or more test_that() tests.\nEach test describes what it’s testing: e.g. “multiplication works”.\nEach test has one or more expectations: e.g. expect_equal(2 * 2, 4).\n\nBelow we go into much more detail about how to test your own functions.\n\n13.4.3 Run tests\nDepending on where you are in the development cycle, you’ll run your tests at various scales. When you are rapidly iterating on a function, you might work at the level of individual tests. As the code settles down, you’ll run entire test files and eventually the entire test suite.\nMicro-iteration: This is the interactive phase where you initiate and refine a function and its tests in tandem. Here you will run devtools::load_all() often, and then execute individual expectations or whole tests interactively in the console. Note that load_all() attaches testthat, so it puts you in the perfect position to test drive your functions and to execute individual tests and expectations.\n\n# tweak the foofy() function and re-load it\ndevtools::load_all()\n\n# interactively explore and refine expectations and tests\nexpect_equal(foofy(...), EXPECTED_FOOFY_OUTPUT)\n\ntestthat(\"foofy does good things\", {...})\n\nMezzo-iteration: As one file’s-worth of functions and their associated tests start to shape up, you will want to execute the entire file of associated tests, perhaps with testthat::test_file():\n\n\ntestthat::test_file(\"tests/testthat/test-foofy.R\")\n\n\n\n\n\n\n\nRStudio\n\n\n\nIn RStudio, you have a couple shortcuts for running a single test file.\nIf the target test file is the active file, you can use the “Run Tests” button in the upper right corner of the source editor.\nThere is also a useful function, devtools::test_active_file(). It infers the target test file from the active file and, similar to how use_r() and use_test() work, it works regardless of whether the active file is a test file or a companion R/*.R file. You can invoke this via “Run a test file” in the Addins menu. However, for heavy users (like us!), we recommend binding this to a keyboard shortcut; we use Ctrl/Cmd + T.\n\n\nMacro-iteration: As you near the completion of a new feature or bug fix, you will want to run the entire test suite.\nMost frequently, you’ll do this with devtools::test():\n\ndevtools::test()\n\nThen eventually, as part of R CMD check with devtools::check():\n\ndevtools::check()\n\n\n\n\n\n\n\nRStudio\n\n\n\ndevtools::test() is mapped to Ctrl/Cmd + Shift + T. devtools::check() is mapped to Ctrl/Cmd + Shift + E.\n\n\n\nThe output of devtools::test() looks like this:\ndevtools::test()\nℹ Loading usethis\nℹ Testing usethis\n✓ | F W S  OK | Context\n✓ |         1 | addin [0.1s]\n✓ |         6 | badge [0.5s]\n   ...\n✓ |        27 | github-actions [4.9s]\n   ...\n✓ |        44 | write [0.6s]\n\n══ Results ═════════════════════════════════════════════════════════════════\nDuration: 31.3 s\n\n── Skipped tests  ──────────────────────────────────────────────────────────\n• Not on GitHub Actions, Travis, or Appveyor (3)\n\n[ FAIL 1 | WARN 0 | SKIP 3 | PASS 728 ]\nTest failure is reported like this:\nFailure (test-release.R:108:3): get_release_data() works if no file found\nres$Version (`actual`) not equal to \"0.0.0.9000\" (`expected`).\n\n`actual`:   \"0.0.0.1234\"\n`expected`: \"0.0.0.9000\"\nEach failure gives a description of the test (e.g., “get_release_data() works if no file found”), its location (e.g., “test-release.R:108:3”), and the reason for the failure (e.g., “res$Version (actual) not equal to”0.0.0.9000” (expected)“).\nThe idea is that you’ll modify your code (either the functions defined below R/ or the tests in tests/testthat/) until all tests are passing."
  },
  {
    "objectID": "testing-basics.html#test-organisation",
    "href": "testing-basics.html#test-organisation",
    "title": "13  Testing basics",
    "section": "\n13.5 Test organisation",
    "text": "13.5 Test organisation\nA test file lives in tests/testthat/. Its name must start with test. We will inspect and execute a test file from the stringr package.\n\nBut first, for the purposes of rendering this book, we must attach stringr and testthat. Note that in real-life test-running situations, this is taken care of by your package development tooling:\n\nDuring interactive development, devtools::load_all() makes testthat and the package-under-development available (both its exported and unexported functions).\nDuring arms-length test execution, this is taken care of by devtools::test_active_file(), devtools::test(), and tests/testthat.R.\n\n\n\n\n\n\n\nImportant\n\n\n\nYour test files should not include these library() calls. We also explicitly request testthat edition 3, but in a real package this will be declared in DESCRIPTION.\n\nlibrary(testthat)\nlibrary(stringr)\nlocal_edition(3)\n\n\n\n\nHere are the contents of tests/testthat/test-dup.r from stringr2:\n\ntest_that(\"basic duplication works\", {\n  expect_equal(str_dup(\"a\", 3), \"aaa\")\n  expect_equal(str_dup(\"abc\", 2), \"abcabc\")\n  expect_equal(str_dup(c(\"a\", \"b\"), 2), c(\"aa\", \"bb\"))\n  expect_equal(str_dup(c(\"a\", \"b\"), c(2, 3)), c(\"aa\", \"bbb\"))\n})\n#> Test passed 🌈\n\ntest_that(\"0 duplicates equals empty string\", {\n  expect_equal(str_dup(\"a\", 0), \"\")\n  expect_equal(str_dup(c(\"a\", \"b\"), 0), rep(\"\", 2))\n})\n#> Test passed 🥳\n\ntest_that(\"uses tidyverse recycling rules\", {\n  expect_error(str_dup(1:2, 1:3), class = \"vctrs_error_incompatible_size\")\n})\n#> Test passed 🥳\n\nThis file shows a typical mix of tests:\n\n“basic duplication works” tests typical usage of str_dup().\n“0 duplicates equals empty string” probes a specific edge case.\n“uses tidyverse recycling rules” checks that malformed input results in a specific kind of error.\n\nTests are organised hierarchically: expectations are grouped into tests which are organised in files:\n\nA file holds multiple related tests. In this example, the file tests/testthat/test-dup.r has all of the tests for the code in R/dup.r.\n\nA test groups together multiple expectations to test the output from a simple function, a range of possibilities for a single parameter from a more complicated function, or tightly related functionality from across multiple functions. This is why they are sometimes called unit tests. Each test should cover a single unit of functionality. A test is created with test_that(desc, code).\nIt’s common to write the description (desc) to create something that reads naturally, e.g. test_that(\"basic duplication works\", { ... }). A test failure report includes this description, which is why you want a concise statement of the test’s purpose, e.g. a specific behaviour.\n\nAn expectation is the atom of testing. It describes the expected result of a computation: Does it have the right value and right class? Does it produce an error when it should? An expectation automates visual checking of results in the console. Expectations are functions that start with expect_.\n\nYou want to arrange things such that, when a test fails, you’ll know what’s wrong and where in your code to look for the problem. This motivates all our recommendations regarding file organisation, file naming, and the test description. Finally, try to avoid putting too many expectations in one test - it’s better to have more smaller tests than fewer larger tests."
  },
  {
    "objectID": "testing-basics.html#expectations",
    "href": "testing-basics.html#expectations",
    "title": "13  Testing basics",
    "section": "\n13.6 Expectations",
    "text": "13.6 Expectations\nAn expectation is the finest level of testing. It makes a binary assertion about whether or not an object has the properties you expect. This object is usually the return value from a function in your package.\nAll expectations have a similar structure:\n\nThey start with expect_.\nThey have two main arguments: the first is the actual result, the second is what you expect.\nIf the actual and expected results don’t agree, testthat throws an error.\nSome expectations have additional arguments that control the finer points of comparing an actual and expected result.\n\nWhile you’ll normally put expectations inside tests inside files, you can also run them directly. This makes it easy to explore expectations interactively. There are more than 40 expectations in the testthat package, which can be explored in testthat’s reference index. We’re only going to cover the most important expectations here.\n\n13.6.1 Testing for equality\nexpect_equal() checks for equality, with some reasonable amount of numeric tolerance:\n\nexpect_equal(10, 10)\nexpect_equal(10, 10L)\nexpect_equal(10, 10 + 1e-7)\nexpect_equal(10, 11)\n#> Error: 10 (`actual`) not equal to 11 (`expected`).\n#> \n#>   `actual`: 10\n#> `expected`: 11\n\nIf you want to test for exact equivalence, use expect_identical().\n\nexpect_equal(10, 10 + 1e-7)\nexpect_identical(10, 10 + 1e-7)\n#> Error: 10 (`actual`) not identical to 10 + 1e-07 (`expected`).\n#> \n#>   `actual`: 10.0000000\n#> `expected`: 10.0000001\n\nexpect_equal(2, 2L)\nexpect_identical(2, 2L)\n#> Error: 2 (`actual`) not identical to 2L (`expected`).\n#> \n#> `actual` is a double vector (2)\n#> `expected` is an integer vector (2)\n\n\n13.6.2 Testing errors\nUse expect_error() to check whether an expression throws an error. It’s the most important expectation in a trio that also includes expect_warning() and expect_message(). We’re going to emphasize errors here, but most of this also applies to warnings and messages.\nUsually you care about two things when testing an error:\n\nDoes the code fail? Specifically, does it fail for the right reason?\nDoes the accompanying message make sense to the human who needs to deal with the error?\n\nThe entry-level solution is to expect a specific type of condition:\n\n1 / \"a\"\n#> Error in 1/\"a\": non-numeric argument to binary operator\nexpect_error(1 / \"a\") \n\nlog(-1)\n#> Warning in log(-1): NaNs produced\n#> [1] NaN\nexpect_warning(log(-1))\n\nThis is a bit dangerous, though, especially when testing an error. There are lots of ways for code to fail! Consider the following test:\n\nexpect_error(str_duq(1:2, 1:3))\n\nThis expectation is intended to test the recycling behaviour of str_dup(). But, due to a typo, it tests behaviour of a non-existent function, str_duq(). The code throws an error and, therefore, the test above passes, but for the wrong reason. Due to the typo, the actual error thrown is about not being able to find the str_duq() function:\n\nstr_duq(1:2, 1:3)\n#> Error in str_duq(1:2, 1:3): could not find function \"str_duq\"\n\nHistorically, the best defense against this was to assert that the condition message matches a certain regular expression, via the second argument, regexp.\n\nexpect_error(1 / \"a\", \"non-numeric argument\")\nexpect_warning(log(-1), \"NaNs produced\")\n\nThis does, in fact, force our typo problem to the surface:\n\nexpect_error(str_duq(1:2, 1:3), \"recycle\")\n#> Error in str_duq(1:2, 1:3): could not find function \"str_duq\"\n\nRecent developments in both base R and rlang make it increasingly likely that conditions are signaled with a class, which provides a better basis for creating precise expectations. That is exactly what you’ve already seen in this stringr example. This is what the class argument is for:\n\n# fails, error has wrong class\nexpect_error(str_duq(1:2, 1:3), class = \"vctrs_error_incompatible_size\")\n#> Error in str_duq(1:2, 1:3): could not find function \"str_duq\"\n\n# passes, error has expected class\nexpect_error(str_dup(1:2, 1:3), class = \"vctrs_error_incompatible_size\")\n\n\nIf you have the choice, express your expectation in terms of the condition’s class, instead of its message. Often this is under your control, i.e. if your package signals the condition. If the condition originates from base R or another package, proceed with caution. This is often a good reminder to re-consider the wisdom of testing a condition that is not fully under your control in the first place.\nTo check for the absence of an error, warning, or message, pass NA to the regexp argument:\n\nexpect_error(1 / 2, NA)\n\nOf course, this is functionally equivalent to simply executing 1 / 2 inside a test, but some developers find the explicit expectation expressive.\nIf you genuinely care about the condition’s message, testthat 3e’s snapshot tests are the best approach, which we describe next.\n\n13.6.3 Snapshot tests\nSometimes it’s difficult or awkward to describe an expected result with code. Snapshot tests are a great solution to this problem and this is one of the main innovations in testthat 3e. The basic idea is that you record the expected result in a separate, human-readable file. Going forward, testthat alerts you when a newly computed result differs from the previously recorded snapshot. Snapshot tests are particularly suited to monitoring your package’s user interface, such as its informational messages and errors. Other use cases include testing images or other complicated objects.\nWe’ll illustrate snapshot tests using the waldo package. Under the hood, testthat 3e uses waldo to do the heavy lifting of “actual vs. expected” comparisons, so it’s good for you to know a bit about waldo anyway. One of waldo’s main design goals is to present differences in a clear and actionable manner, as opposed to a frustrating declaration that “this differs from that and I know exactly how, but I won’t tell you”. Therefore, the formatting of output from waldo::compare() is very intentional and is well-suited to a snapshot test. The binary outcome of TRUE (actual == expected) vs. FALSE (actual != expected) is fairly easy to check and could get its own test. Here we’re concerned with writing a test to ensure that differences are reported to the user in the intended way.\nwaldo uses a few different layouts for showing diffs, depending on various conditions. Here we deliberately constrain the width, in order to trigger a side-by-side layout.3 (We’ll talk more about the withr package below.)\n\nwithr::with_options(\n  list(width = 20),\n  waldo::compare(c(\"X\", letters), c(letters, \"X\"))\n)\n#>     old | new    \n#> [1] \"X\" -        \n#> [2] \"a\" | \"a\" [1]\n#> [3] \"b\" | \"b\" [2]\n#> [4] \"c\" | \"c\" [3]\n#> \n#>      old | new     \n#> [25] \"x\" | \"x\" [24]\n#> [26] \"y\" | \"y\" [25]\n#> [27] \"z\" | \"z\" [26]\n#>          - \"X\" [27]\n\nThe two primary inputs differ at two locations: once at the start and once at the end. This layout presents both of these, with some surrounding context, which helps the reader orient themselves.\nHere’s how this would look as a snapshot test:\n\n\ntest_that(\"side-by-side diffs work\", {\n  withr::local_options(width = 20)\n  expect_snapshot(\n    waldo::compare(c(\"X\", letters), c(letters, \"X\"))\n  )\n})\n\nIf you execute expect_snapshot() or a test containing expect_snapshot() interactively, you’ll see this:\nCan't compare snapshot to reference when testing interactively\nℹ Run `devtools::test()` or `testthat::test_file()` to see changes\nfollowed by a preview of the snapshot output.\nThis reminds you that snapshot tests only function when executed non-interactively, i.e. while running an entire test file or the entire test suite. This applies both to recording snapshots and to checking them.\nThe first time this test is executed via devtools::test() or similar, you’ll see something like this (assume the test is in tests/testthat/test-diff.R):\n── Warning (test-diff.R:63:3): side-by-side diffs work ─────────────────────\nAdding new snapshot:\nCode\n  waldo::compare(c(\n    \"X\", letters), c(\n    letters, \"X\"))\nOutput\n      old | new    \n  [1] \"X\" -        \n  [2] \"a\" | \"a\" [1]\n  [3] \"b\" | \"b\" [2]\n  [4] \"c\" | \"c\" [3]\n  \n       old | new     \n  [25] \"x\" | \"x\" [24]\n  [26] \"y\" | \"y\" [25]\n  [27] \"z\" | \"z\" [26]\n           - \"X\" [27]\nThere is always a warning upon initial snapshot creation. The snapshot is added to tests/testthat/_snaps/diff.md, under the heading “side-by-side diffs work”, which comes from the test’s description. The snapshot looks exactly like what a user sees interactively in the console, which is the experience we want to check for. The snapshot file is also very readable, which is pleasant for the package developer. This readability extends to snapshot changes, i.e. when examining Git diffs and reviewing pull requests on GitHub, which helps you keep tabs on your user interface. Going forward, as long as your package continues to re-capitulate the expected snapshot, this test will pass.\nIf you’ve written a lot of conventional unit tests, you can appreciate how well-suited snapshot tests are for this use case. If we were forced to inline the expected output in the test file, there would be a great deal of quoting, escaping, and newline management. Ironically, with conventional expectations, the output you expect your user to see tends to get obscured by a heavy layer of syntactical noise.\nWhat about when a snapshot test fails? Let’s imagine a hypothetical internal change where the default labels switch from “old” and “new” to “OLD” and “NEW”. Here’s how this snapshot test would react:\n── Failure (test-diff.R:63:3): side-by-side diffs work──────────────────────────\nSnapshot of code has changed:\nold[3:15] vs new[3:15]\n  \"    \\\"X\\\", letters), c(\"\n  \"    letters, \\\"X\\\"))\"\n  \"Output\"\n- \"      old | new    \"\n+ \"      OLD | NEW    \"\n  \"  [1] \\\"X\\\" -        \"\n  \"  [2] \\\"a\\\" | \\\"a\\\" [1]\"\n  \"  [3] \\\"b\\\" | \\\"b\\\" [2]\"\n  \"  [4] \\\"c\\\" | \\\"c\\\" [3]\"\n  \"  \"\n- \"       old | new     \"\n+ \"       OLD | NEW     \"\nand 3 more ...\n\n* Run `snapshot_accept('diff')` to accept the change\n* Run `snapshot_review('diff')` to interactively review the change\nThis diff is presented more effectively in most real-world usage, e.g. in the console, by a Git client, or via a Shiny app (see below). But even this plain text version highlights the changes quite clearly. Each of the two loci of change is indicated with a pair of lines marked with - and +, showing how the snapshot has changed.\nYou can call testthat::snapshot_review('diff') to review changes locally in a Shiny app, which lets you skip or accept individual snapshots. Or, if all changes are intentional and expected, you can go straight to testthat::snapshot_accept('diff'). Once you’ve re-synchronized your actual output and the snapshots on file, your tests will pass once again. In real life, snapshot tests are a great way to stay informed about changes to your package’s user interface, due to your own internal changes or due to changes in your dependencies or even R itself.\nexpect_snapshot() has a few arguments worth knowing about:\n\ncran = FALSE: By default, snapshot tests are skipped if it looks like the tests are running on CRAN’s servers. This reflects the typical intent of snapshot tests, which is to proactively monitor user interface, but not to check for correctness, which presumably is the job of other unit tests which are not skipped. In typical usage, a snapshot change is something the developer will want to know about, but it does not signal an actual defect.\n\nerror = FALSE: By default, snapshot code is not allowed to throw an error. See expect_error(), described above, for one approach to testing errors. But sometimes you want to assess “Does this error message make sense to a human?” and having it laid out in context in a snapshot is a great way to see it with fresh eyes. Specify error = TRUE in this case:\n\nexpect_snapshot(error = TRUE,\n  str_dup(1:2, 1:3)\n)\n\n\ntransform: Sometimes a snapshot contains volatile, insignificant elements, such as a temporary filepath or a timestamp. The transform argument accepts a function, presumably written by you, to remove or replace such changeable text. Another use of transform is to scrub sensitive information from the snapshot.\nvariant: Sometimes snapshots reflect the ambient conditions, such as the operating system or the version of R or one of your dependencies, and you need a different snapshot for each variant. This is an experimental and somewhat advanced feature, so if you can arrange things to use a single snapshot, you probably should.\n\nIn typical usage, testthat will take care of managing the snapshot files below tests/testthat/_snaps/. This happens in the normal course of you running your tests and, perhaps, calling testthat::snapshot_accept().\n\n13.6.4 Shortcuts for other common patterns\nWe conclude this section with a few more expectations that come up frequently. But remember that testthat has many more pre-built expectations than we can demonstrate here.\nSeveral expectations can be described as “shortcuts”, i.e. they streamline a pattern that comes up often enough to deserve its own wrapper.\n\n\nexpect_match(object, regexp, ...) is a shortcut that wraps grepl(pattern = regexp, x = object, ...). It matches a character vector input against a regular expression regexp. The optional all argument controls whether all elements or just one element needs to match. Read the expect_match() documentation to see how additional arguments, like ignore.case = FALSE or fixed = TRUE, can be passed down to grepl().\n\nstring <- \"Testing is fun!\"\n\nexpect_match(string, \"Testing\") \n\n# Fails, match is case-sensitive\nexpect_match(string, \"testing\")\n#> Error: `string` does not match \"testing\".\n#> Actual value: \"Testing is fun!\"\n\n# Passes because additional arguments are passed to grepl():\nexpect_match(string, \"testing\", ignore.case = TRUE)\n\n\nexpect_length(object, n) is a shortcut for expect_equal(length(object), n).\nexpect_setequal(x, y) tests that every element of x occurs in y, and that every element of y occurs in x. But it won’t fail if x and y happen to have their elements in a different order.\n\nexpect_s3_class() and expect_s4_class() check that an object inherit()s from a specified class. expect_type()checks the typeof() an object.\n\nmodel <- lm(mpg ~ wt, data = mtcars)\nexpect_s3_class(model, \"lm\")\nexpect_s3_class(model, \"glm\")\n#> Error: `model` inherits from 'lm' not 'glm'.\n\n\n\nexpect_true() and expect_false() are useful catchalls if none of the other expectations does what you need."
  },
  {
    "objectID": "testing-design.html#what-to-test",
    "href": "testing-design.html#what-to-test",
    "title": "14  Designing your test suite",
    "section": "\n14.1 What to test",
    "text": "14.1 What to test\n\nWhenever you are tempted to type something into a print statement or a debugger expression, write it as a test instead. — Martin Fowler\n\nThere is a fine balance to writing tests. Each test that you write makes your code less likely to change inadvertently; but it also can make it harder to change your code on purpose. It’s hard to give good general advice about writing tests, but you might find these points helpful:\n\nFocus on testing the external interface to your functions - if you test the internal interface, then it’s harder to change the implementation in the future because as well as modifying the code, you’ll also need to update all the tests.\nStrive to test each behaviour in one and only one test. Then if that behaviour later changes you only need to update a single test.\nAvoid testing simple code that you’re confident will work. Instead focus your time on code that you’re not sure about, is fragile, or has complicated interdependencies. That said, I often find I make the most mistakes when I falsely assume that the problem is simple and doesn’t need any tests.\nAlways write a test when you discover a bug. You may find it helpful to adopt the test-first philosophy. There you always start by writing the tests, and then write the code that makes them pass. This reflects an important problem solving strategy: start by establishing your success criteria, how you know if you’ve solved the problem.\n\n\n14.1.1 Test coverage\nAnother concrete way to direct your test writing efforts is to examine your test coverage. The covr package (https://covr.r-lib.org) can be used to determine which lines of your package’s source code are (or are not!) executed when the test suite is run. This is most often presented as a percentage. Generally speaking, the higher the better.\nIn some technical sense, 100% test coverage is the goal, however, this is rarely achieved in practice and that’s often OK. Going from 90% or 99% coverage to 100% is not always the best use of your development time and energy. In many cases, that last 10% or 1% often requires some awkward gymnastics to cover. Sometimes this forces you to introduce mocking or some other new complexity. Don’t sacrifice the maintainability of your test suite in the name of covering some weird edge case that hasn’t yet proven to be a problem. Also remember that not every line of code or every function is equally likely to harbor bugs. Focus your testing energy on code that is tricky, based on your expert opinion and any empirical evidence you’ve accumulated about bug hot spots.\nWe use covr regularly, in two different ways:\n\nLocal, interactive use. We mostly use devtools::test_coverage_active_file() and devtools::test_coverage(), for exploring the coverage of an individual file or the whole package, respectively.\nAutomatic, remote use via GitHub Actions (GHA). We cover continuous integration and GHA more thoroughly elsewhere, but we will at least mention here that usethis::use_github_action(\"test-coverage\") configures a GHA workflow that constantly monitors your test coverage. Test coverage can be an especially helpful metric when evaluating a pull request (either your own or from an external contributor). A proposed change that is well-covered by tests is less risky to merge."
  },
  {
    "objectID": "testing-design.html#high-level-principles-for-testing",
    "href": "testing-design.html#high-level-principles-for-testing",
    "title": "14  Designing your test suite",
    "section": "\n14.2 High-level principles for testing",
    "text": "14.2 High-level principles for testing\nIn later sections, we offer concrete strategies for how to handle common testing dilemmas in R. Here we lay out the high-level principles that underpin these recommendations:\n\nA test should ideally be self-sufficient and self-contained.\nThe interactive workflow is important, because you will mostly interact with your tests when they are failing.\nIt’s more important that test code be obvious than, e.g., as DRY as possible.\nHowever, the interactive workflow shouldn’t “leak” into and undermine the test suite.\n\nWriting good tests for a code base often feels more challenging than writing the code in the first place. This can come as a bit of a shock when you’re new to package development and you might be concerned that you’re doing it wrong. Don’t worry, you’re not! Testing presents many unique challenges and maneuvers, which tend to get much less air time in programming communities than strategies for writing the “main code”, i.e. the stuff below R/. As a result, it requires more deliberate effort to develop your skills and taste around testing.\nMany of the packages maintained by our team violate some of the advice you’ll find here. There are (at least) two reasons for that:\n\ntestthat has been evolving for more than twelve years and this chapter reflects the cumulative lessons learned from that experience. The tests in many packages have been in place for a long time and reflect typical practices from different eras and different maintainers.\nThese aren’t hard and fast rules, but are, rather, guidelines. There will always be specific situations where it makes sense to bend the rule.\n\nThis chapter can’t address all possible testing situations, but hopefully these guidelines will help your future decision-making.\n\n14.2.1 Self-sufficient tests\n\nAll tests should strive to be hermetic: a test should contain all of the information necessary to set up, execute, and tear down its environment. Tests should assume as little as possible about the outside environment ….\nFrom the book Software Engineering at Google, Chapter 11\n\nRecall this advice found in Section 7.6, which covers your package’s “main code”, i.e. everything below R/:\n\nThe .R files below R/ should consist almost entirely of function definitions. Any other top-level code is suspicious and should be carefully reviewed for possible conversion into a function.\n\nWe have analogous advice for your test files:\n\nThe test-*.R files below tests/testthat/ should consist almost entirely of calls to test_that(). Any other top-level code is suspicious and should be carefully considered for relocation into calls to test_that() or to other files that get special treatment inside an R package or from testthat.\n\nEliminating (or at least minimizing) top-level code outside of test_that() will have the beneficial effect of making your tests more hermetic. This is basically the testing analogue of the general programming advice that it’s wise to avoid unstructured sharing of state.\nLogic at the top-level of a test file has an awkward scope: Objects or functions defined here have what you might call “test file scope”, if the definitions appear before the first call to test_that(). If top-level code is interleaved between test_that() calls, you can even create “partial test file scope”.\nWhile writing tests, it can feel convenient to rely on these file-scoped objects, especially early in the life of a test suite, e.g. when each test file fits on one screen. But we find that implicitly relying on objects in a test’s parent environment tends to make a test suite harder to understand and maintain over time.\nConsider a test file with top-level code sprinkled around it, outside of test_that():\n\ndat <- data.frame(x = c(\"a\", \"b\", \"c\"), y = c(1, 2, 3))\n\nskip_if(today_is_a_monday())\n\ntest_that(\"foofy() does this\", {\n  expect_equal(foofy(dat), ...)\n})\n\ndat2 <- data.frame(x = c(\"x\", \"y\", \"z\"), y = c(4, 5, 6))\n\nskip_on_os(\"windows\")\n\ntest_that(\"foofy2() does that\", {\n  expect_snapshot(foofy2(dat, dat2)\n})\n\nWe recommend relocating file-scoped logic to either a narrower scope or to a broader scope. Here’s what it would look like to use a narrow scope, i.e. to inline everything inside test_that() calls:\n\ntest_that(\"foofy() does this\", {\n  skip_if(today_is_a_monday())\n  \n  dat <- data.frame(x = c(\"a\", \"b\", \"c\"), y = c(1, 2, 3))\n  \n  expect_equal(foofy(dat), ...)\n})\n\ntest_that(\"foofy() does that\", {\n  skip_if(today_is_a_monday())\n  skip_on_os(\"windows\")\n  \n  dat <- data.frame(x = c(\"a\", \"b\", \"c\"), y = c(1, 2, 3))\n  dat2 <- data.frame(x = c(\"x\", \"y\", \"z\"), y = c(4, 5, 6))\n  \n  expect_snapshot(foofy(dat, dat2)\n})\n\nBelow we will discuss techniques for moving file-scoped logic to a broader scope.\n\n14.2.2 Self-contained tests\nEach test_that() test has its own execution environment, which makes it somewhat self-contained. For example, an R object you create inside a test does not exist after the test exits:\n\nexists(\"thingy\")\n#> [1] FALSE\n\ntest_that(\"thingy exists\", {\n  thingy <- \"thingy\"\n  expect_true(exists(thingy))\n})\n#> Test passed 🎊\n\nexists(\"thingy\")\n#> [1] FALSE\n\nThe thingy object lives and dies entirely within the confines of test_that(). However, testthat doesn’t know how to cleanup after actions that affect other aspects of the R landscape:\n\nThe filesystem: creating and deleting files, changing the working directory, etc.\nThe search path: library(), attach().\nGlobal options, like options() and par(), and environment variables.\n\nWatch how calls like library(), options(), and Sys.setenv() have a persistent effect after a test, even when they are executed inside test_that():\n\ngrep(\"jsonlite\", search(), value = TRUE)\n#> character(0)\ngetOption(\"opt_whatever\")\n#> NULL\nSys.getenv(\"envvar_whatever\")\n#> [1] \"\"\n\ntest_that(\"landscape changes leak outside the test\", {\n  library(jsonlite)\n  options(opt_whatever = \"whatever\")\n  Sys.setenv(envvar_whatever = \"whatever\")\n  \n  expect_match(search(), \"jsonlite\", all = FALSE)\n  expect_equal(getOption(\"opt_whatever\"), \"whatever\")\n  expect_equal(Sys.getenv(\"envvar_whatever\"), \"whatever\")\n})\n#> Test passed 🎉\n\ngrep(\"jsonlite\", search(), value = TRUE)\n#> [1] \"package:jsonlite\"\ngetOption(\"opt_whatever\")\n#> [1] \"whatever\"\nSys.getenv(\"envvar_whatever\")\n#> [1] \"whatever\"\n\nThese changes to the landscape even persist beyond the current test file, i.e. they carry over into all subsequent test files.\nIf it’s easy to avoid making such changes in your test code, that is the best strategy! But if it’s unavoidable, then you have to make sure that you clean up after yourself. This mindset is very similar to one we advocated for in Section 7.6, when discussing how to design well-mannered functions.\nWe like to use the withr package (https://withr.r-lib.org) to make temporary changes in global state, because it automatically captures the initial state and arranges the eventual restoration. You’ve already seen an example of its usage, when we explored snapshot tests:\n\ntest_that(\"side-by-side diffs work\", {\n  withr::local_options(width = 20)             # <-- (°_°) look here!\n  expect_snapshot(\n    waldo::compare(c(\"X\", letters), c(letters, \"X\"))\n  )\n})\n\nThis test requires the display width to be set at 20 columns, which is considerably less than the default width. withr::local_options(width = 20) sets the width option to 20 and, at the end of the test, restores the option to its original value. withr is also pleasant to use during interactive development: deferred actions are still captured on the global environment and can be executed explicitly via withr::deferred_run() or implicitly by restarting R.\nWe recommend including withr in Suggests, if you’re only going to use it in your tests, or in Imports, if you also use it below R/. Call withr functions as we do above, e.g. like withr::local_whatever(), in either case. See Section 11.2.1.1 for a full discussion.\n\n\n\n\n\n\nTip\n\n\n\nThe easiest way to add a package to DESCRIPTION is with, e.g., usethis::use_package(\"withr\", type = \"Suggests\"). For tidyverse packages, withr is considered a “free dependency”, i.e. the tidyverse uses withr so extensively that we don’t hesitate to use it whenever it would be useful.\n\n\nwithr has a large set of pre-implemented local_*() / with_*() functions that should handle most of your testing needs, so check there before you write your own. If nothing exists that meets your need, withr::defer() is the general way to schedule some action at the end of a test.1\nHere’s how we would fix the problems in the previous example using withr: Behind the scenes, we reversed the landscape changes, so we can try this again.\n\ngrep(\"jsonlite\", search(), value = TRUE)\n#> character(0)\ngetOption(\"opt_whatever\")\n#> NULL\nSys.getenv(\"envvar_whatever\")\n#> [1] \"\"\n\ntest_that(\"withr makes landscape changes local to a test\", {\n  withr::local_package(\"jsonlite\")\n  withr::local_options(opt_whatever = \"whatever\")\n  withr::local_envvar(envvar_whatever = \"whatever\")\n  \n  expect_match(search(), \"jsonlite\", all = FALSE)\n  expect_equal(getOption(\"opt_whatever\"), \"whatever\")\n  expect_equal(Sys.getenv(\"envvar_whatever\"), \"whatever\")\n})\n#> Test passed 🎉\n\ngrep(\"jsonlite\", search(), value = TRUE)\n#> character(0)\ngetOption(\"opt_whatever\")\n#> NULL\nSys.getenv(\"envvar_whatever\")\n#> [1] \"\"\n\ntestthat leans heavily on withr to make test execution environments as reproducible and self-contained as possible. In testthat 3e, testthat::local_reproducible_output() is implicitly part of each test_that() test.\n\ntest_that(\"something specific happens\", {\n  local_reproducible_output()     # <-- this happens implicitly\n  \n  # your test code, which might be sensitive to ambient conditions, such as\n  # display width or the number of supported colors\n})\n\nlocal_reproducible_output() temporarily sets various options and environment variables to values favorable for testing, e.g. it suppresses colored output, turns off fancy quotes, sets the console width, and sets LC_COLLATE = \"C\". Usually, you can just passively enjoy the benefits of local_reproducible_output(). But you may want to call it explicitly when replicating test results interactively or if you want to override the default settings in a specific test.\n\n14.2.3 Plan for test failure\nWe regret to inform you that most of the quality time you spend with your tests will be when they are inexplicably failing.\n\nIn its purest form, automating testing consists of three activities: writing tests, running tests, and reacting to test failures….\nRemember that tests are often revisited only when something breaks. When you are called to fix a broken test that you have never seen before, you will be thankful someone took the time to make it easy to understand. Code is read far more than it is written, so make sure you write the test you’d like to read!\nFrom the book Software Engineering at Google, Chapter 11\n\nMost of us don’t work on a code base the size of Google. But even in a team of one, tests that you wrote six months ago might as well have been written by someone else. Especially when they are failing.\nWhen we do reverse dependency checks, often involving hundreds or thousands of CRAN packages, we have to inspect test failures to determine if changes in our packages are to blame. As a result, we regularly engage with failing tests in other people’s packages, which leaves us with lots of opinions about practices that create unnecessary testing pain.\nTest troubleshooting nirvana looks like this: In a fresh R session, you can do devtools::load_all() and immediately run an individual test or walk through it line-by-line. There is no need to hunt around for setup code that has to be run manually first, that is found elsewhere in the test file or perhaps in a different file altogether. Test-related code that lives in an unconventional location causes extra self-inflicted pain when you least need it.\nConsider this extreme and abstract example of a test that is difficult to troubleshoot due to implicit dependencies on free-range code:\n\n# dozens or hundreds of lines of top-level code, interspersed with other tests,\n# which you must read and selectively execute\n\ntest_that(\"f() works\", {\n  x <- function_from_some_dependency(object_with_unknown_origin)\n  expect_equal(f(x), 2.5)\n})\n\nThis test is much easier to drop in on if dependencies are invoked in the normal way, i.e. via ::, and test objects are created inline:\n\n# dozens or hundreds of lines of self-sufficient, self-contained tests,\n# all of which you can safely ignore!\n\ntest_that(\"f() works\", {\n  useful_thing <- ...\n  x <- somePkg::someFunction(useful_thing)\n  expect_equal(f(x), 2.5)\n})\n\nThis test is self-sufficient. The code inside { ... } explicitly creates any necessary objects or conditions and makes explicit calls to any helper functions. This test doesn’t rely on objects or dependencies that happen to be be ambiently available.\nSelf-sufficient, self-contained tests are a win-win: It is literally safer to design tests this way and it also makes tests much easier for humans to troubleshoot later.\n\n14.2.4 Repetition is OK\nOne obvious consequence of our suggestion to minimize code with “file scope” is that your tests will probably have some repetition. And that’s OK! We’re going to make the controversial recommendation that you tolerate a fair amount of duplication in test code, i.e. you can relax some of your DRY (“don’t repeat yourself”) tendencies.\n\nKeep the reader in your test function. Good production code is well-factored; good test code is obvious. … think about what will make the problem obvious when a test fails.\nFrom the blog post Why Good Developers Write Bad Unit Tests\n\nHere’s a toy example to make things concrete.\n\ntest_that(\"multiplication works\", {\n  useful_thing <- 3\n  expect_equal(2 * useful_thing, 6)\n})\n#> Test passed 😀\n\ntest_that(\"subtraction works\", {\n  useful_thing <- 3\n  expect_equal(5 - useful_thing, 2)\n})\n#> Test passed 🥳\n\nIn real life, useful_thing is usually a more complicated object that somehow feels burdensome to instantiate. Notice how useful_thing <- 3 appears in more than once place. Conventional wisdom says we should DRY this code out. It’s tempting to just move useful_thing’s definition outside of the tests:\n\nuseful_thing <- 3\n\ntest_that(\"multiplication works\", {\n  expect_equal(2 * useful_thing, 6)\n})\n#> Test passed 😸\n\ntest_that(\"subtraction works\", {\n  expect_equal(5 - useful_thing, 2)\n})\n#> Test passed 🥇\n\nBut we really do think the first form, with the repetition, is often the better choice.\nAt this point, many readers might be thinking “but the code I might have to repeat is much longer than 1 line!”. Below we describe the use of test fixtures. This can often reduce complicated situations back to something that resembles this simple example.\n\n14.2.5 Remove tension between interactive and automated testing\nYour test code will be executed in two different settings:\n\nInteractive test development and maintenance, which includes tasks like:\n\nInitial test creation\nModifying tests to adapt to change\nDebugging test failure\n\n\nAutomated test runs, which is accomplished with functions such as:\n\nSingle file: devtools::test_active_file(), testthat::test_file()\n\nWhole package: devtools::test(), devtools::check()\n\n\n\n\nAutomated testing of your whole package is what takes priority. This is ultimately the whole point of your tests. However, the interactive experience is clearly important for the humans doing this work. Therefore it’s important to find a pleasant workflow, but also to ensure that you don’t rig anything for interactive convenience that actually compromises the health of the test suite.\nThese two modes of test-running should not be in conflict with each other. If you perceive tension between these two modes, this can indicate that you’re not taking full advantage of some of testthat’s features and the way it’s designed to work with devtools::load_all().\nWhen working on your tests, use load_all(), just like you do when working below R/. By default, load_all() does all of these things:\n\nSimulates re-building, re-installing, and re-loading your package.\nMakes everything in your package’s namespace available, including unexported functions and objects and anything you’ve imported from another package.\nAttaches testthat, i.e. does library(testthat).\nRuns test helper files, i.e. executes test/testthat/helper.R (more on that below).\n\nThis eliminates the need for any library() calls below tests/testthat/, for the vast majority of R packages. Any instance of library(testthat) is clearly no longer necessary. Likewise, any instance of attaching one of your dependencies via library(somePkg) is unnecessary. In your tests, if you need to call functions from somePkg, do it just as you do below R/. If you have imported the function into your namespace, use fun(). If you have not, use somePkg::fun(). It’s fair to say that library(somePkg) in the tests should be about as rare as taking a dependency via Depends, i.e. there is almost always a better alternative.\nUnnecessary calls to library(somePkg) in test files have a real downside, because they actually change the R landscape. library() alters the search path. This means the circumstances under which you are testing may not necessarily reflect the circumstances under which your package will be used. This makes it easier to create subtle test bugs, which you will have to unravel in the future.\nOne other function that should almost never appear below tests/testhat/ is source(). There are several special files with an official role in testthat workflows (see below), not to mention the entire R package machinery, that provide better ways to make functions, objects, and other logic available in your tests."
  },
  {
    "objectID": "testing-design.html#sec-tests-files-overview",
    "href": "testing-design.html#sec-tests-files-overview",
    "title": "14  Designing your test suite",
    "section": "\n14.3 Files relevant to testing",
    "text": "14.3 Files relevant to testing\nHere we review which package files are especially relevant to testing and, more generally, best practices for interacting with the file system from your tests.\n\n14.3.1 Hiding in plain sight: files below R/\n\nThe most important functions you’ll need to access from your tests are clearly those in your package! Here we’re talking about everything that’s defined below R/. The functions and other objects defined by your package are always available when testing, regardless of whether they are exported or not. For interactive work, devtools::load_all() takes care of this. During automated testing, this is taken care of internally by testthat.\nThis implies that test helpers can absolutely be defined below R/ and used freely in your tests. It might make sense to gather such helpers in a clearly marked file, such as one of these:\n.                              \n├── ...\n└── R\n    ├── ...\n    ├── test-helpers.R\n    ├── test-utils.R\n    ├── utils-testing.R\n    └── ...\n\n14.3.2 tests/testthat.R\n\nRecall the initial testthat setup described in Section 13.4: The standard tests/testthat.R file looks like this:\n\nlibrary(testthat)\nlibrary(pkg)\n\ntest_check(\"pkg\")\n\nWe repeat the advice to not edit tests/testthat.R. It is run during R CMD check (and, therefore, devtools::check()), but is not used in most other test-running scenarios (such as devtools::test() or devtools::test_active_file() or during interactive development). Do not attach your dependencies here with library(). Call them in your tests in the same manner as you do below R/.\n\n14.3.3 Testthat helper files\nAnother type of file that is always executed by load_all() and at the beginning of automated testing is a helper file, defined as any file below tests/testthat/ that begins with helper. Helper files are a mighty weapon in the battle to eliminate code floating around at the top-level of test files. Helper files are a prime example of what we mean when we recommend moving such code into a broader scope. Objects or functions defined in a helper file are available to all of your tests.\nIf you have just one such file, you should probably name it helper.R. If you organize your helpers into multiple files, you could include a suffix with additional info. Here are examples of how such files might look:\n.                              \n├── ...\n└── tests\n    ├── testthat\n    │   ├── helper.R\n    │   ├── helper-blah.R\n    │   ├── helper-foo.R    \n    │   ├── test-foofy.R\n    │   └── (more test files)\n    └── testthat.R\nMany developers use helper files to define custom test helper functions, which we describe in detail below. Compared to defining helpers below R/, some people find that tests/testthat/helper.R makes it more clear that these utilities are specifically for testing the package. This location also feels more natural if your helpers rely on testthat functions.\nA helper file is also a good location for setup code that is needed for its side effects. This is a case where tests/testthat/helper.R is clearly more appropriate than a file below R/. For example, in an API-wrapping package, helper.R is a good place to (attempt to) authenticate with the testing credentials.\n\n14.3.4 Testthat setup files\nTestthat has one more special file type: setup files, defined as any file below test/testthat/ that begins with setup. Here’s an example of how that might look:\n.                              \n├── ...\n└── tests\n    ├── testthat\n    │   ├── helper.R\n    │   ├── setup.R\n    │   ├── test-foofy.R\n    │   └── (more test files)\n    └── testthat.R\nA setup file is handled almost exactly like a helper file, but with two big differences:\n\nSetup files are not executed by devtools::load_all().\nSetup files often contain the corresponding teardown code.\n\nSetup files are good for global test setup that is tailored for test execution in non-interactive or remote environments. For example, you might turn off behaviour that’s aimed at an interactive user, such as messaging or writing to the clipboard.\nIf any of your setup should be reversed after test execution, you should also include the necessary teardown code in setup.R2. We recommend maintaining teardown code alongside the setup code, in setup.R, because this makes it easier to ensure they stay in sync. The artificial environment teardown_env() exists as a magical handle to use in withr::defer() and withr::local_*() / withr::with_*().\nHere’s a setup.R example from the reprex package, where we turn off clipboard and HTML preview functionality during testing:\n\nop <- options(reprex.clipboard = FALSE, reprex.html_preview = FALSE)\n\nwithr::defer(options(op), teardown_env())\n\nSince we are just modifying options here, we can be even more concise and use the pre-built function withr::local_options() and pass teardown_env() as the .local_envir:\n\nwithr::local_options(\n  list(reprex.clipboard = FALSE, reprex.html_preview = FALSE),\n  .local_envir = teardown_env()\n)\n\n\n14.3.5 Files ignored by testthat\ntestthat only automatically executes files where these are both true:\n\nFile is a direct child of tests/testthat/\n\nFile name starts with one of the specific strings:\n\nhelper\nsetup\ntest\n\n\n\nIt is fine to have other files or directories in tests/testthat/, but testthat won’t automatically do anything with them (other than the _snaps directory, which holds snapshots).\n\n14.3.6 Storing test data\nMany packages contain files that hold test data. Where should these be stored? The best location is somewhere below tests/testthat/, often in a subdirectory, to keep things neat. Below is an example, where useful_thing1.rds and useful_thing2.rds hold objects used in the test files.\n.\n├── ...\n└── tests\n    ├── testthat\n    │   ├── fixtures\n    │   │   ├── make-useful-things.R\n    │   │   ├── useful_thing1.rds\n    │   │   └── useful_thing2.rds\n    │   ├── helper.R\n    │   ├── setup.R\n    │   └── (all the test files)\n    └── testthat.R\nThen, in your tests, use testthat::test_path() to build a robust filepath to such files.\n\ntest_that(\"foofy() does this\", {\n  useful_thing <- readRDS(test_path(\"fixtures\", \"useful_thing1.rds\"))\n  # ...\n})\n\ntestthat::test_path() is extremely handy, because it produces the correct path in the two important modes of test execution:\n\nInteractive test development and maintenance, where working directory is presumably set to the top-level of the package.\nAutomated testing, where working directory is usually set to something below tests/.\n\n14.3.7 Where to write files during testing\nIf it’s easy to avoid writing files from you tests, that is definitely the best plan. But there are many times when you really must write files.\nYou should only write files inside the session temp directory. Do not write into your package’s tests/ directory. Do not write into the current working directory. Do not write into the user’s home directory. Even though you are writing into the session temp directory, you should still clean up after yourself, i.e. delete any files you’ve written.\nMost package developers don’t want to hear this, because it sounds like a hassle. But it’s not that burdensome once you get familiar with a few techniques and build some new habits. A high level of file system discipline also eliminates various testing bugs and will absolutely make your CRAN life run more smoothly.\nThis test is from roxygen2 and demonstrates everything we recommend:\n\ntest_that(\"can read from file name with utf-8 path\", {\n  path <- withr::local_tempfile(\n    pattern = \"Universit\\u00e0-\",\n    lines = c(\"#' @include foo.R\", NULL)\n  )\n  expect_equal(find_includes(path), \"foo.R\")\n})\n\nwithr::local_tempfile() creates a file within the session temp directory whose lifetime is tied to the “local” environment – in this case, the execution environment of an individual test. It is a wrapper around base::tempfile() and passes, e.g., the pattern argument through, so you have some control over the file name. You can optionally provide lines to populate the file with at creation time or you can write to the file in all the usual ways in subsequent steps. Finally, with no special effort on your part, the temporary file will automatically be deleted at the end of the test.\nSometimes you need even more control over the file name. In that case, you can use withr::local_tempdir() to create a self-deleting temporary directory and write intentionally-named files inside this directory."
  },
  {
    "objectID": "testing-advanced.html#test-fixtures",
    "href": "testing-advanced.html#test-fixtures",
    "title": "15  Advanced testing techniques",
    "section": "\n15.1 Test fixtures",
    "text": "15.1 Test fixtures\nWhen it’s not practical to make your test entirely self-sufficient, prefer making the necessary object, logic, or conditions available in a structured, explicit way. There’s a pre-existing term for this in software engineering: a test fixture.\n\nA test fixture is something used to consistently test some item, device, or piece of software. — Wikipedia\n\nThe main idea is that we need to make it as easy and obvious as possible to arrange the world into a state that is conducive for testing. We describe several specific solutions to this problem:\n\nPut repeated code in a constructor-type helper function. Memoise it, if construction is demonstrably slow.\nIf the repeated code has side effects, write a custom local_*() function to do what’s needed and clean up afterwards.\nIf the above approaches are too slow or awkward and the thing you need is fairly stable, save it as a static file and load it.\n\n\n15.1.1 Create useful_things with a helper function\nIs it fiddly to create a useful_thing? Does it take several lines of code, but not much time or memory? In that case, write a helper function to create a useful_thing on-demand:\n\nnew_useful_thing <- function() {\n  # your fiddly code to create a useful_thing goes here\n}\n\nand call that helper in the affected tests:\n\ntest_that(\"foofy() does this\", {\n  useful_thing1 <- new_useful_thing()\n  expect_equal(foofy(useful_thing1, x = \"this\"), EXPECTED_FOOFY_OUTPUT)\n})\n\ntest_that(\"foofy() does that\", {\n  useful_thing2 <- new_useful_thing()\n  expect_equal(foofy(useful_thing2, x = \"that\"), EXPECTED_FOOFY_OUTPUT)\n})\n\nWhere should the new_useful_thing() helper be defined? This comes back to what we outlined in Section 14.3. Test helpers can be defined below R/, just like any other internal utility in your package. Another popular location is in a test helper file, e.g. tests/testthat/helper.R. A key feature of both options is that the helpers are made available to you during interactive maintenance via devtools::load_all().\nIf it’s fiddly AND costly to create a useful_thing, your helper function could even use memoisation to avoid unnecessary re-computation. Once you have a helper like new_useful_thing(), you often discover that it has uses beyond testing, e.g. behind-the-scenes in a vignette. Sometimes you even realize you should just define it below R/ and export and document it, so you can use it freely in documentation and tests.\n\n15.1.2 Create (and destroy) a “local” useful_thing\n\nSo far, our example of a useful_thing was a regular R object, which is cleaned-up automatically at the end of each test. What if the creation of a useful_thing has a side effect on the local file system, on a remote resource, R session options, environment variables, or the like? Then your helper function should create a useful_thing and clean up afterwards. Instead of a simple new_useful_thing() constructor, you’ll write a customized function in the style of withr’s local_*() functions:\n\nlocal_useful_thing <- function(..., env = parent.frame()) {\n  # your fiddly code to create a useful_thing goes here\n  withr::defer(\n    # your fiddly code to clean up after a useful_thing goes here\n    envir = env\n  )\n}\n\nUse it in your tests like this:\n\ntest_that(\"foofy() does this\", {\n  useful_thing1 <- local_useful_thing()\n  expect_equal(foofy(useful_thing1, x = \"this\"), EXPECTED_FOOFY_OUTPUT)\n})\n\ntest_that(\"foofy() does that\", {\n  useful_thing2 <- local_useful_thing()\n  expect_equal(foofy(useful_thing2, x = \"that\"), EXPECTED_FOOFY_OUTPUT)\n})\n\nWhere should the local_useful_thing() helper be defined? All the advice given above for new_useful_thing() applies: define it below R/ or in a test helper file.\nTo learn more about writing custom helpers like local_useful_thing(), see the testthat vignette on test fixtures.\n\n15.1.3 Store a concrete useful_thing persistently\nIf a useful_thing is costly to create, in terms of time or memory, maybe you don’t actually need to re-create it for each test run. You could make the useful_thing once, store it as a static test fixture, and load it in the tests that need it. Here’s a sketch of how this could look:\n\ntest_that(\"foofy() does this\", {\n  useful_thing1 <- readRDS(test_path(\"fixtures\", \"useful_thing1.rds\"))\n  expect_equal(foofy(useful_thing1, x = \"this\"), EXPECTED_FOOFY_OUTPUT)\n})\n\ntest_that(\"foofy() does that\", {\n  useful_thing2 <- readRDS(test_path(\"fixtures\", \"useful_thing2.rds\"))\n  expect_equal(foofy(useful_thing2, x = \"that\"), EXPECTED_FOOFY_OUTPUT)\n})\n\nNow we can revisit a file listing from earlier, which addressed exactly this scenario:\n.\n├── ...\n└── tests\n    ├── testthat\n    │   ├── fixtures\n    │   │   ├── make-useful-things.R\n    │   │   ├── useful_thing1.rds\n    │   │   └── useful_thing2.rds\n    │   ├── helper.R\n    │   ├── setup.R\n    │   └── (all the test files)\n    └── testthat.R\nThis shows static test files stored in tests/testthat/fixtures/, but also notice the companion R script, make-useful-things.R. From data analysis, we all know there is no such thing as a script that is run only once. Refinement and iteration is inevitable. This also holds true for test objects like useful_thing1.rds. We highly recommend saving the R code used to create your test objects, so that they can be re-created as needed."
  },
  {
    "objectID": "testing-advanced.html#building-your-own-testing-tools",
    "href": "testing-advanced.html#building-your-own-testing-tools",
    "title": "15  Advanced testing techniques",
    "section": "\n15.2 Building your own testing tools",
    "text": "15.2 Building your own testing tools\nLet’s return to the topic of duplication in your test code. We’ve encouraged you to have a higher tolerance for repetition in test code, in the name of making your tests obvious. But there’s still a limit to how much repetition to tolerate. We’ve covered techniques such as loading static objects with test_path(), writing a constructor like new_useful_thing(), or implementing a test fixture like local_useful_thing(). There are even more types of test helpers that can be useful in certain situations.\n\n15.2.1 Helper defined inside a test\nConsider this test for the str_trunc() function in stringr:\n\n# from stringr (hypothetically)\ntest_that(\"truncations work for all sides\", {\n  expect_equal(\n    str_trunc(\"This string is moderately long\", width = 20, side = \"right\"),\n    \"This string is mo...\"\n  )\n  expect_equal(\n    str_trunc(\"This string is moderately long\", width = 20, side = \"left\"),\n    \"...s moderately long\"\n  )\n  expect_equal(\n    str_trunc(\"This string is moderately long\", width = 20, side = \"center\"),\n    \"This stri...ely long\"\n  )\n})\n\nThere’s a lot of repetition here, which increases the chance of copy / paste errors and generally makes your eyes glaze over. Sometimes it’s nice to create a hyper-local helper, inside the test. Here’s how the test actually looks in stringr\n\n# from stringr (actually)\ntest_that(\"truncations work for all sides\", {\n\n  trunc <- function(direction) str_trunc(\n    \"This string is moderately long\",\n    direction,\n    width = 20\n  )\n\n  expect_equal(trunc(\"right\"),   \"This string is mo...\")\n  expect_equal(trunc(\"left\"),    \"...s moderately long\")\n  expect_equal(trunc(\"center\"),  \"This stri...ely long\")\n})\n\nA hyper-local helper like trunc() is particularly useful when it allows you to fit all the important business for each expectation on one line. Then your expectations can be read almost like a table of actual vs. expected, for a set of related use cases. Above, it’s very easy to watch the result change as we truncate the input from the right, left, and in the center.\nNote that this technique should be used in extreme moderation. A helper like trunc() is yet another place where you can introduce a bug, so it’s best to keep such helpers extremely short and simple.\n\n15.2.2 Custom expectations\nIf a more complicated helper feels necessary, it’s a good time to reflect on why that is. If it’s fussy to get into position to test a function, that could be a sign that it’s also fussy to use that function. Do you need to refactor it? If the function seems sound, then you probably need to use a more formal helper, defined outside of any individual test, as described earlier.\nOne specific type of helper you might want to create is a custom expectation. Here are two very simple ones from usethis:\n\nexpect_usethis_error <- function(...) {\n  expect_error(..., class = \"usethis_error\")\n}\n\nexpect_proj_file <- function(...) {\n  expect_true(file_exists(proj_path(...)))\n}\n\nexpect_usethis_error() checks that an error has the \"usethis_error\" class. expect_proj_file() is a simple wrapper around file_exists() that searches for the file in the current project. These are very simple functions, but the sheer amount of repetition and the expressiveness of their names makes them feel justified.\nIt is somewhat involved to make a proper custom expectation, i.e. one that behaves like the expectations built into testthat. We refer you to the Custom expectations vignette if you wish to learn more about that.\nFinally, it can be handy to know that testthat makes specific information available when it’s running:\n\n\nThe environment variable TESTTHAT is set to \"true\". testthat::is_testing() is a shortcut:\n\nis_testing <- function() {\n  Sys.getenv(\"TESTTHAT\")\n}\n\n\n\nThe package-under-test is available as the environment variable TESTTHAT_PKG and testthat::testing_package() is a shortcut:\n\ntesting_package <- function() {\n  Sys.getenv(\"TESTTHAT_PKG\")\n}\n\n\n\nIn some situations, you may want to exploit this information without taking a run-time dependency on testthat. In that case, just inline the source of these functions directly into your package."
  },
  {
    "objectID": "testing-advanced.html#when-testing-gets-hard",
    "href": "testing-advanced.html#when-testing-gets-hard",
    "title": "15  Advanced testing techniques",
    "section": "\n15.3 When testing gets hard",
    "text": "15.3 When testing gets hard\nDespite all the techniques we’ve covered so far, there remain situations where it still feels very difficult to write tests. In this section, we review more ways to deal with challenging situations:\n\nSkipping a test in certain situations\nMocking an external service\nDealing with secrets\n\n\n15.3.1 Skipping a test\nSometimes it’s impossible to perform a test - you may not have an internet connection or you may not have access to the necessary credentials. Unfortunately, another likely reason follows from this simple rule: the more platforms you use to test your code, the more likely it is that you won’t be able to run all of your tests, all of the time. In short, there are times when, instead of getting a failure, you just want to skip a test.\n\n15.3.1.1 testthat::skip()\n\nHere we use testthat::skip() to write a hypothetical custom skipper, skip_if_no_api():\n\nskip_if_no_api() <- function() {\n  if (api_unavailable()) {\n    skip(\"API not available\")\n  }\n}\n\ntest_that(\"foo api returns bar when given baz\", {\n  skip_if_no_api()\n  ...\n})\n\nskip_if_no_api() is a yet another example of a test helper and the advice already given about where to define it applies here too.\nskip()s and the associated reasons are reported inline as tests are executed and are also indicated clearly in the summary:\n\ndevtools::test()\n#> ℹ Loading abcde\n#> ℹ Testing abcde\n#> ✔ | F W S  OK | Context\n#> ✔ |         2 | blarg\n#> ✔ |     1   2 | foofy\n#> ────────────────────────────────────────────────────────────────────────────────\n#> Skip (test-foofy.R:6:3): foo api returns bar when given baz\n#> Reason: API not available\n#> ────────────────────────────────────────────────────────────────────────────────\n#> ✔ |         0 | yo                                                              \n#> ══ Results ═════════════════════════════════════════════════════════════════════\n#> ── Skipped tests  ──────────────────────────────────────────────────────────────\n#> • API not available (1)\n#> \n#> [ FAIL 0 | WARN 0 | SKIP 1 | PASS 4 ]\n#> \n#> 🥳\n\nSomething like skip_if_no_api() is likely to appear many times in your test suite. This is another occasion where it is tempting to DRY things out, by hoisting the skip() to the top-level of the file. However, we still lean towards calling skip_if_no_api() in each test where it’s needed.\n\n# we prefer this:\ntest_that(\"foo api returns bar when given baz\", {\n  skip_if_no_api()\n  ...\n})\n\ntest_that(\"foo api returns an errors when given qux\", {\n  skip_if_no_api()\n  ...\n})\n\n# over this:\nskip_if_no_api()\n\ntest_that(\"foo api returns bar when given baz\", {...})\n\ntest_that(\"foo api returns an errors when given qux\", {...})\n\nWithin the realm of top-level code in test files, having a skip() at the very beginning of a test file is one of the more benign situations. But once a test file does not fit entirely on your screen, it creates an implicit yet easy-to-miss connection between the skip() and individual tests.\n\n15.3.1.2 Built-in skip() functions\nSimilar to testthat’s built-in expectations, there is a family of skip() functions that anticipate some common situations. These functions often relieve you of the need to write a custom skipper. Here are some examples of the most useful skip() functions:\n\ntest_that(\"foo api returns bar when given baz\", {\n  skip_if(api_unavailable(), \"API not available\")\n  ...\n})\ntest_that(\"foo api returns bar when given baz\", {\n  skip_if_not(api_available(), \"API not available\")\n  ...\n})\n\nskip_if_not_installed(\"sp\")\nskip_if_not_installed(\"stringi\", \"1.2.2\")\n\nskip_if_offline()\nskip_on_cran()\nskip_on_os(\"windows\")\n\n\n15.3.1.3 Dangers of skipping\nOne challenge with skips is that they are currently completely invisible in CI — if you automatically skip too many tests, it’s easy to fool yourself that all your tests are passing when in fact they’re just being skipped! In an ideal world, your CI/CD would make it easy to see how many tests are being skipped and how that changes over time.\n2022-06-01: Recent changes to GitHub Actions mean that we will likely have better test reporting before the second edition of this book is published. Stay tuned!\nIt’s a good practice to regularly dig into the R CMD check results, especially on CI, and make sure the skips are as you expect. But this tends to be something you have to learn through experience.\n\n15.3.2 Mocking\nThe practice known as mocking is when we replace something that’s complicated or unreliable or out of our control with something simpler, that’s fully within our control. Usually we are mocking an external service, such as a REST API, or a function that reports something about session state, such as whether the session is interactive.\nThe classic application of mocking is in the context of a package that wraps an external API. In order to test your functions, technically you need to make a live call to that API to get a response, which you then process. But what if that API requires authentication or what if it’s somewhat flaky and has occasional downtime? It can be more productive to just pretend to call the API but, instead, to test the code under your control by processing a pre-recorded response from the actual API.\nOur main advice about mocking is to avoid it if you can. This is not an indictment of mocking, but just a realistic assessment that mocking introduces new complexity that is not always justified by the payoffs.\nSince most R packages do not need full-fledged mocking, we do not cover it here. Instead we’ll point you to the packages that represent the state-of-the-art for mocking in R today:\n\nmockery: https://github.com/r-lib/mockery\n\nmockr: https://krlmlr.github.io/mockr/\n\nhttptest: https://enpiar.com/r/httptest/\n\nhttptest2: https://enpiar.com/httptest2/\n\nwebfakes: https://webfakes.r-lib.org\n\n\n15.3.3 Secrets\nAnother common challenge for packages that wrap an external service is the need to manage credentials. Specifically, it is likely that you will need to provide a set of test credentials to fully test your package.\nOur main advice here is to design your package so that large parts of it can be tested without live, authenticated assess to the external service.\nOf course, you will still want to be able to test your package against the actual service that it wraps, in environments that support secure environment variables. Since this is also a very specialized topic, we won’t go into more detail here. Instead we refer you to the Wrapping APIs vignette in the httr2 package, which offers substantial support for secret management."
  },
  {
    "objectID": "testing-advanced.html#special-considerations-for-cran-packages",
    "href": "testing-advanced.html#special-considerations-for-cran-packages",
    "title": "15  Advanced testing techniques",
    "section": "\n15.4 Special considerations for CRAN packages",
    "text": "15.4 Special considerations for CRAN packages\n\n15.4.1 CRAN check flavors and related services\nThis section will likely move to a different location, once we revise and expand the content elsewhere in the book on R CMD check and package release. But it can gestate here.\nCRAN runs R CMD check on all contributed packages on a regular basis, on multiple platforms or what they call “flavors”. This check includes, but is not limited to, your testthat tests. CRAN’s check flavors almost certainly include platforms other than your preferred development environment(s), so you must proactively plan ahead if you want your tests to pass there.\nYou can see CRAN’s current check flavors here: https://cran.r-project.org/web/checks/check_flavors.html. There are various combinations of:\n\nOperating system and CPU: Windows, macOS (x86_64, arm64), Linux (various distributions)\nR version: r-devel, r-release, r-oldrel\nC, C++, FORTRAN compilers\nLocale, in the sense of the LC_CTYPE environment variable (this is about which human language is in use and character encoding)\n\nIt would be impractical for individual package developers to personally maintain all of these testing platforms. Instead, we turn to various community- and CRAN-maintained resources to test our packages. In order of how often we use them:\n\n\nGitHub Actions (GHA). Many R package developers host their source code on GitHub and use GHA to check their package, e.g., every time they push.\nThe usethis package offers several functions to help you configure GHA workflows for checking your package. The most appropriate level of checking depends on the nature of your user base and how likely it is that your package could behave differently across the flavors (e.g. does it contain compiled code?)\n\n\nusethis::use_github_action_check_release(): an entry-level, bare-minimum workflow that checks with the latest release of R on Linux.\n\nusethis::use_github_action_check_standard(): Covers the three major operating systems and both the released and development versions of R. This is a good choice for a package that is (or aspires to be) on CRAN or Bioconductor.\nThe tidyverse/r-lib team uses an even more extensive check matrix, which would be overkill for most other packages. It’s necessary in this case in order to meet our commitment to support the current version, the development version, and four previous versions of R.\n\n\n\nR-hub builder (R-hub). This is a service supported by the R Consortium where package developers can submit their package for checks that replicate various CRAN check flavors. This is useful when you’re doing the due diligence leading up to a CRAN submission.\nYou can use R-hub via a web interface (https://builder.r-hub.io) or, as we recommend, through the rhub R package.\nThe rhub::check_for_cran() function is morally similar to the GHA workflow configured by usethis::use_github_action_check_standard(), i.e. it’s a good solution for a typical package heading to CRAN. rhub has many other functions for accessing individual check flavors.\n\n\nWin-Builder is a service maintained by the CRAN personnel who build the Windows binaries for CRAN packages. You use it in a similar way as R-hub, i.e. it’s a good check to run when preparing a CRAN submission. (Win-Builder is basically the inspiration for R-hub, i.e. Win-builder is such a convenient service that it makes sense to extend it for more flavors.)\nThe Win-Builder homepage (https://win-builder.r-project.org) explains how to upload a package via ftp, but we recommend using the convenience functions devtools::check_win_devel() and friends.\n\n\nmacOS builder is a service maintained by the CRAN personnel who build the macOS binaries for CRAN packages. This is a relatively new addition to the list and checks packages with “the same setup and available packages as the CRAN M1 build machine”.\nYou can submit your package using the web interface (https://mac.r-project.org/macbuilder/submit.html) or with devtools::check_mac_release().\n\n\n15.4.2 Testing on CRAN\nThe need to pass tests on all of CRAN’s flavors is not the only thing you need to think about. There are other considerations that will influence how you write your tests and how (or whether) they run on CRAN. When a package runs afoul of the CRAN Repository Policy (https://cran.r-project.org/web/packages/policies.html), the test suite is very often the culprit (although not always).\nIf a specific test simply isn’t appropriate to be run by CRAN, include skip_on_cran() at the very start.\n\ntest_that(\"some long-running thing works\", {\n  skip_on_cran()\n  # test code that can potentially take \"a while\" to run  \n})\n\nUnder the hood, skip_on_cran() consults the NOT_CRAN environment variable. Such tests will only run when NOT_CRAN has been explicitly defined as \"true\". This variable is set by devtools and testthat, allowing those tests to run in environments where you expect success (and where you can tolerate and troubleshoot occasional failure).\nIn particular, the GitHub Actions workflows that we recommend elsewhere will run tests with NOT_CRAN = \"true\" call. For certain types of functionality, there is no practical way to test it on CRAN and your own checks, on GHA or an equivalent continuous integration service, are your best method of quality assurance.\nThere are even rare cases where it makes sense to maintain tests outside of your package altogether. The tidymodels team uses this strategy for integration-type tests of their whole ecosystem that would be impossible to host inside an individual CRAN package.\nThe following subsections enumerate other thing to keep in mind for maximum success when testing on CRAN.\n\n15.4.2.1 Speed\nYour tests need to run relatively quickly - ideally, less than a minute, in total. Use skip_on_cran() in a test that is unavoidably long-running.\n\n15.4.2.2 Reproducibility\nBe careful about testing things that are likely to be variable on CRAN machines. It’s risky to test how long something takes (because CRAN machines are often heavily loaded) or to test parallel code (because CRAN runs multiple package tests in parallel, multiple cores will not always be available). Numerical precision can also vary across platforms, so use expect_equal() unless you have a specific reason for using expect_identical().\n\n15.4.2.3 Flaky tests\nDue to the scale at which CRAN checks packages, there is basically no latitude for a test that’s “just flaky”, i.e. sometimes fails for incidental reasons. CRAN does not process your package’s test results the way you do, where you can inspect each failure and exercise some human judgment about how concerning it is.\nIt’s probably a good idea to eliminate flaky tests, just for your own sake! But if you have valuable, well-written tests that are prone to occasional nuisance failure, definitely put skip_on_cran() at the start.\nThe classic example is any test that accesses a website or web API. Given that any web resource in the world will experience occasional downtime, it’s best to not let such tests run on CRAN. The CRAN Repository Policy says:\n\nPackages which use Internet resources should fail gracefully with an informative message if the resource is not available or has changed (and not give a check warning nor error).\n\nOften making such a failure “graceful” would run counter to the behaviour you actually want in practice, i.e. you would want your user to get an error if their request fails. This is why it is usually more practical to test such functionality elsewhere.\nRecall that snapshot tests, by default, are also skipped on CRAN. You typically use such tests to monitor, e.g., how various informational messages look. Slight changes in message formatting are something you want to be alerted to, but do not indicate a major defect in your package. This is the motivation for the default skip_on_cran() behaviour of snapshot tests.\nFinally, flaky tests cause problems for the maintainers of your dependencies. When the packages you depend on are updated, CRAN runs R CMD check on all reverse dependencies, including your package. If your package has flaky tests, your package can be the reason another package does not clear CRAN’s incoming checks and can delay its release.\n\n15.4.2.4 Process and file system hygiene\nIn Section 14.3.7, we urged you to only write into the session temp directory and to clean up after yourself. This practice makes your test suite much more maintainable and predictable. For packages that are (or aspire to be) on CRAN, this is absolutely required per the CRAN repository policy:\n\nPackages should not write in the user’s home filespace (including clipboards), nor anywhere else on the file system apart from the R session’s temporary directory (or during installation in the location pointed to by TMPDIR: and such usage should be cleaned up)…. Limited exceptions may be allowed in interactive sessions if the package obtains confirmation from the user.\n\nSimilarly, you should make an effort to be hygienic with respect to any processes you launch:\n\nPackages should not start external software (such as PDF viewers or browsers) during examples or tests unless that specific instance of the software is explicitly closed afterwards.\n\nAccessing the clipboard is the perfect storm that potentially runs afoul of both of these guidelines, as the clipboard is considered part of the user’s home filespace and, on Linux, can launch an external process (e.g. xsel or xclip). Therefore it is best to turn off any clipboard functionality in your tests (and to ensure that, during authentic usage, your user is clearly opting-in to that)."
  },
  {
    "objectID": "man.html",
    "href": "man.html",
    "title": "16  Function documentation",
    "section": "",
    "text": "Second edition\n\n\n\nYou are reading the work-in-progress second edition of R Packages. This chapter is undergoing heavy restructuring and may be confusing or incomplete."
  },
  {
    "objectID": "man.html#introduction",
    "href": "man.html#introduction",
    "title": "16  Function documentation",
    "section": "\n16.1 Introduction",
    "text": "16.1 Introduction\nDocumentation is one of the most important aspects of a good package: without it, users won’t know how to use your package! Documentation is also useful for future-you (so you remember what your functions were supposed to do) and for developers extending your package.\nIn this chapter, you’ll learn about function documentation, as accessed by ? or help(). Function documentation works like a dictionary: it’s helpful if you want to know what a function does, but it won’t help you find the right function for a new situation. That’s one of the jobs of vignettes, which you’ll learn about in the next chapter. In this chapter we’ll focus on documenting functions, but the same ideas apply to documenting datasets, classes and generics, and packages. You can learn more about those important topics in vignette(\"rd-other\", package = \"roxygen2\").\nBase R provides a standard way of documenting a package where each documentation topic corresponds to an .Rd file in the man/ directory. These files use a custom syntax, loosely based on LaTeX, that are rendered to HTML, plain text, or pdf, as needed, for viewing. We are not going to use these files directly. Instead, we’ll use the roxygen2 package to generate them from specially formatted comments. There are a few advantages to using roxygen2:\n\nCode and documentation are intermingled so that when you modify your code, it’s easy to remember to also update your documentation.\nYou can with using markdown, rather learning a new text formatting syntax.\n.Rd boilerplate is automated away.\nIt provides a number of tools for sharing content between documentation topics and even between topics and vignettes.\n\nYou’ll see these files when you work with them in git, but you’ll otherwise rarely need to look at them."
  },
  {
    "objectID": "man.html#roxygen2-basics",
    "href": "man.html#roxygen2-basics",
    "title": "16  Function documentation",
    "section": "\n16.2 roxygen2 basics",
    "text": "16.2 roxygen2 basics\nTo get started, we’ll work through the basic roxygen2 workflow and discuss the overall structure of roxygen2 comments which are organised into blocks and tags.\n\n16.2.1 The documentation workflow\nThe documentation workflow starts when you add roxygen comments, comments that start with ', to your source file. Here’s a simple example:\n\n#' Add together two numbers\n#' \n#' @param x A number.\n#' @param y A number.\n#' @return The sum of `x` and `y`.\n#' @examples\n#' add(1, 1)\n#' add(10, 1)\nadd <- function(x, y) {\n  x + y\n}\n\nThen you’ll press Ctrl/Cmd + Shift + D or type devtools::document() which then runs roxygen2::roxygenise() which generates a man/add.Rd that looks like this:\n% Generated by roxygen2: do not edit by hand\n% Please edit documentation in R/across.R\n\\name{add}\n\\alias{add}\n\\title{Add together two numbers}\n\\usage{\nadd(x, y)\n}\n\\arguments{\n\\item{x}{A number.}\n\n\\item{y}{A number.}\n}\n\\value{\nThe sum of \\code{x} and \\code{y}.\n}\n\\description{\nAdd together two numbers\n}\n\\examples{\nadd(1, 1)\nadd(10, 1)\n}\nIf you’ve used LaTeX before, this should look familiar since the .Rd format is loosely based on it, and if you’re interested you can read more about it in R extensions. Otherwise you won’t need to look at it except to check it in to git.\nWhen you use ?add, help(\"add\"), or example(\"add\"), R looks for an .Rd file containing \\alias{add}. It then parses the file, converts it into HTML, and displays it. Here’s what the result looks like in RStudio:\n\n\n\n\n\nTo preview the development documentation, devtools uses some tricks to override the usual help functions so they know where to look in your source packages. To activate these tricks, you need to run devtools::load_all() once. So if the development documentation doesn’t appear, you may need to load your package first.\nTo summarize, there are four steps in the basic roxygen2 workflow:\n\nAdd roxygen2 comments to your .R files.\nPress Ctrl/Cmd + Shift + D or type devtools::document() to convert roxygen2 comments to .Rd files.\nPreview documentation with ?.\nRinse and repeat until the documentation looks the way you want.\n\n16.2.2 roxygen2 comments, blocks, and tags\nNow that you understand the basic workflow, lets talk a little more about the syntax. roxygen2 comments start with #' and the set of all roxygen2 comments preceding a function is called a block. Blocks are broken up by tags, which look like @tagName tagValue. The content of a tag extends from the end of the tag name to the start of the next tag1. A block can contain text before the first tag which is called the introduction. By default, each roxygen2 block will generate a single documentation topic, i.e. one .Rd file2 in the man/ directory .\nThroughout this chapter I’m going to show you roxygen2 comments from real tidyverse packages, focusing on stringr since the functions there tend to be fairly straightforward leading to documentation that is easier to excerpt. Here’s a simple first example: the documentation for stringr::str_unique().\n\n#' Remove duplicated strings\n#'\n#' `str_unique()` removes duplicated values, with optional control over\n#' how duplication is measured.\n#'\n#' @param string A character vector to return unique entries.\n#' @param ... Other options used to control matching behavior between duplicate\n#'   strings. Passed on to [stringi::stri_opts_collator()].\n#' @returns A character vector.\n#' @seealso [unique()], [stringi::stri_unique()] which this function wraps.\n#' @examples\n#' str_unique(c(\"a\", \"b\", \"c\", \"b\", \"a\"))\n#'\n#' # Use ... to pass additional arguments to stri_unique()\n#' str_unique(c(\"motley\", \"mötley\", \"pinguino\", \"pingüino\"))\n#' str_unique(c(\"motley\", \"mötley\", \"pinguino\", \"pingüino\"), strength = 1)\n#' @export\nstr_unique <- function(string, ...) {\n  ...\n}\n\nHere the introduction includes the title (“Remove duplicated strings”) and a basic description of what the function does. It’s followed by five tags, two @params, one @returns, one @seealso, one @examples, and one @export.\nNote that I’ve wrapped each line of the roxygen2 block 80 characters wide, to match the wrapping of my code, and I’ve indented the second and subsequent lines of the long @param tag so it’s easier to scan. You can get more roxygen2 style advice in the tidyverse style guide.\nThe following sections will work through the most important tags. We’ll start with the introduction which provides the title, description, and details, then we’ll cover the inputs (the function arguments), outputs (the return value), and examples. We’ll then discuss links and cross-references, and finish off with some techniques to share documentation between topics."
  },
  {
    "objectID": "man.html#title-description-details",
    "href": "man.html#title-description-details",
    "title": "16  Function documentation",
    "section": "\n16.3 Title, description, details",
    "text": "16.3 Title, description, details\nThe block introduction provides a title, description, and, optionally, details, for the function:\n\nThe title is taken from the first sentence. It should be written in sentence case, not end in a full stop, and be followed by a blank line. The title is shown in various function indexes and what the user will see when browsing functions.\nThe description is taken from the next paragraph. It comes first in the documentation and should briefly describe the most important features of the function.\nAdditional details are anything after the description. Details are optional, but can be any length so are useful if want to dig deep into some important aspect of the function.\n\nThe following sections describe each component in more detail, and then discuss a few useful related tags.\n\n16.3.1 Title\nWhen figuring out what to use as a title, I think it’s most important to consider the functions in your package holistically. When the user is skimming the index, how will they find the function to solve their current problem? What do functions have in common that doesn’t need to be repeated in every title? What is unique to that function and should be highlighted?\nAs an example, take the titles of some of the key dplyr functions3:\n\n\nmutate(): Create, modify, and delete columns.\n\nsummarise(): Summarise each group to fewer rows.\n\nfilter(): Subset rows using column values.\n\nselect(): Subset columns using their names and types.\n\narrange(): Arrange rows by column values.\n\nHere we’ve tried to succinctly describe what the function does, making sure to describe whether it affects rows, columns, or groups. We do our best to use synonyms, instead of repeating the function name, to hopefully give folks another chance to understand the intent of the function.\nAt the time we wrote this, I don’t think the function titles for stringr were that successful. But they provide a useful negative case study:\n\n\nstr_detect(): Detect the presence or absence of a pattern in a string.\n\nstr_extract(): Extract matching patterns from a string.\n\nstr_locate(): Locate the position of patterns in a string.\n\nstr_match(): Extract matched groups from a string.\n\nThere’s a lot of repetition (“pattern”, “from a string”) and the verb used for the function name is repeated in the title, so if you don’t understand the function already, the title seems unlikely to help much. (In hindsight, it also seems like the function names could have been better chosen.) Hopefully we’ll have improved those titles by the time you read this.\n\n16.3.2 Description\nThe purpose of the description is to summarize the goal of the function, usually in under a paragraph. This can be challenging for simple functions, because it might feel like you’re repeating the title of the function. But it’s okay for the description to be a little duplicative of the rest of the documentation; it’s often useful for the reader to see the same thing expressed in two different ways. It’s a little extra work keeping it all up to date, but the extra effort is often worth it.\n\n#' Detect the presence/absence of a pattern\n#'\n#' `str_detect()` returns a logical vector `TRUE` if `pattern` is found within\n#' each element of `string` or a `FALSE` if not. It's equivalent\n#' `grepl(pattern, string)`.\n\nIf you want to use multiple paragraphs or a bulleted list, you can use the explicit @description tag4. Here’s an example from stringr::str_like(), which mimics the LIKE operator from SQL:\n\n#' Detect the a pattern in the same way as `SQL`'s `LIKE` operator.\n#'\n#' @description\n#' `str_like()` follows the conventions of the SQL `LIKE` operator:\n#'\n#' * Must match the entire string.\n#' * `_` matches a single character (like `.`).\n#' * `%` matches any number of characters (like `.*`).\n#' * `\\%` and `\\_` match literal `%` and `_`.\n#' * The match is case insensitive by default.\n\nFinally, it’s often particularly hard to write a good description if you’ve just written the function because the purpose seems so intuitively obvious. Do your best, and then come back in a couple of months when you’ve forgotten exactly what the function does, and re-write the description to jog your memory.\n\n16.3.3 Details\nThe “details” are just any additional details or explanation that you think your function needs. Most functions don’t need details, but some functions need a lot. If you have a lot of information to convey, I recommend using markdown headings to break the documentation up into sections. Here’s a example from dplyr::mutate(). We’ve elided some of the details to keep this example short, but you should still get a sense of how we used headings to break up the content in to skimmable chunks:\n\n#' Create, modify, and delete columns\n#'\n#' `mutate()` adds new variables and preserves existing ones;\n#' `transmute()` adds new variables and drops existing ones.\n#' New variables overwrite existing variables of the same name.\n#' Variables can be removed by setting their value to `NULL`.\n#'\n#' # Useful mutate functions\n#'\n#' * [`+`], [`-`], [log()], etc., for their usual mathematical meanings\n#'\n#' ...\n#'\n#' # Grouped tibbles\n#'\n#' Because mutating expressions are computed within groups, they may\n#' yield different results on grouped tibbles. This will be the case\n#' as soon as an aggregating, lagging, or ranking function is\n#' involved. Compare this ungrouped mutate:\n#'\n#' ...\n\nNote that even though these headings come immediately after the description they are shown much later (after the function arguments and return value) in the rendered documentation.\nIn older code, you might also see the use of @section title: which was used to create sections before roxygen2 fully supported RMarkdown. You can now move these below the description and turn them into markdown headings."
  },
  {
    "objectID": "man.html#arguments",
    "href": "man.html#arguments",
    "title": "16  Function documentation",
    "section": "\n16.4 Arguments",
    "text": "16.4 Arguments\nFor most functions, the bulk of your work will go towards documenting how each argument affects the output of the function. For this purpose, you’ll use @param (short for parameter, a synonym of argument) followed by the argument name and a description of its action.\nThe most important job of the description is to provide a succinct summary of the allowed inputs and what the parameter does. For example, here’s str_detect():\n\n#' @param string Input vector. Either a character vector, or something\n#'  coercible to one.\n\nAnd here are three of the arguments to str_flatten():\n\n#' @param collapse String to insert between each piece. Defaults to `\"\"`.\n#' @param last Optional string use in place of final separator.\n#' @param na.rm Remove missing values? If `FALSE` (the default), the result \n#'   will be `NA` if any element of `string` is `NA`.\n\nNote that @param collapse and @param na.rm describe their default arguments. This is good practice because the function usage (which shows the default values) and the argument description are often quite far apart. The primary downside is that introducing this duplication means that you’ll need to update the docs if you change the default value; we believe this small amount of extra work is worth it to make the life of the user easier.\nIf an argument has a fixed set of possible parameters, you should list them. If they’re simple, you can just list them in a sentence, like in str_trim():\n\n#' @param side Side on which to remove whitespace: `\"left\"`, `\"right\"`, or\n#'   `\"both\"` (the default).\n\nIf they need more explanation, you might use a bulleted list, as in str_wrap():\n\n#' @param whitespace_only A boolean.\n#'   * `TRUE` (the default): wrapping will only occur at whitespace.\n#'   * `FALSE`: can break on any non-word character (e.g. `/`, `-`).\n\nThe documentation for most arguments tends to be relatively short, often one or two sentences. But you should take as much space as you need, and you’ll see some examples of multi-paragraph argument documentation shortly.\n\n16.4.1 Multiple arguments\nIf the behavior of multiple arguments is tightly coupled, you can document them together by separating the names with commas (with no spaces). For example, in str_equal() x and y are interchangeable, so they’re documented together:\n\n#' @param x,y A pair of character vectors.\n\nIn str_sub() start and end define the range of characters to replace, and you can use just start if you pass in a two-column matrix. So it makes sense to document them together:\n\n#' @param start,end Two integer vectors. `start` gives the position\n#'   of the first character (defaults to first), `end` gives the position\n#'   of the last (defaults to last character). Alternatively, pass a two-column\n#'   matrix to `start`.\n#'\n#'   Negative values count backwards from the last character.\n\nIn str_wrap() indent and exdent define the indentation for the first line and all subsequent lines respectively:\n\n#' @param indent,exdent A non-negative integer giving the indent for the\n#'   first line (`indent`) and all subsequent lines (`exdent`).\n\n\n16.4.2 Inheriting arguments\nIf your package contains many closely related functions, it’s common for them to have arguments that share the same name and meaning. It would be annoying and error prone to copy and paste the same @param documentation to every function so roxygen2 provides @inheritParams which allows you to inherit argument documentation from another package.\nstringr uses @inheritParams extensively because most functions have string and pattern arguments. So str_detect() documents them in detail:\n\n#' @param string Input vector. Either a character vector, or something\n#'  coercible to one.\n#' @param pattern Pattern to look for.\n#'\n#'   The default interpretation is a regular expression, as described\n#'   `vignette(\"regular-expressions\")`. Control options with [regex()].\n#'\n#'   Match a fixed string (i.e. by comparing only bytes), using\n#'   [fixed()]. This is fast, but approximate. Generally,\n#'   for matching human text, you'll want [coll()] which\n#'   respects character matching rules for the specified locale.\n#'\n#'   Match character, word, line and sentence boundaries with\n#'   [boundary()]. An empty pattern, \"\", is equivalent to\n#'   `boundary(\"character\")`.\n\nThen the other stringr functions use @inheritParams str_detect to get a detailed documentation for string and pattern without having to duplicate that text.\n@inheritParams only inherits docs for arguments that aren’t already documented, so you can document some arguments and inherit others. str_match() uses this to inherit its standard string argument but document its unusual pattern argument:\n\n#' @inheritParams str_detect\n#' @param pattern Unlike other stringr functions, `str_match()` only supports\n#'   regular expressions, as described `vignette(\"regular-expressions\")`. \n#'   The pattern should contain at least one capturing group.\n\nYou can inherit documentation from a function in another package by using the standard :: notation, i.e. @inheritParams anotherpackage::function. This does introduce one small annoyance: now the documentation for your package is no longer self-contained and the version of anotherpackage can affect the generated docs. Beware of spurious diffs caused by contributors with different installed versions."
  },
  {
    "objectID": "man.html#return-value",
    "href": "man.html#return-value",
    "title": "16  Function documentation",
    "section": "\n16.5 Return value",
    "text": "16.5 Return value\nAs important as a function’s inputs are its outputs. Documenting the outputs is the job of the @returns5 tag. Here the goal of the docs is not to describe exactly how the values are computed (which is the job of the description and details), but to roughly describe the overall “shape” of the output, i.e. what sort of object it is, and its dimensions (if that makes sense). For example, if your function returns a vector you might describe its type and length, or if your function returns a data frame you might describe the names and types of the columns and the expected number of rows.\nThe return documentation for functions in stringr are straightforward because almost all functions return some type of vector with the same length as one of the inputs. For example, here’s str_like():\n\n#' @returns A logical vector the same length as `string`.\n\nA more complicated case is the joint documentation for str_locate() and str_locate_all()6. str_locate() returns an integer matrix, and str_locate_all() returns a list of matrices, so the text needs to describe what defines the rows and columns.\n\n#' @return `str_locate()` returns an integer matrix with two columns and\n#'   one row for each element of `string`. The first column, `start`,\n#'   gives the position at the start of the match, and second column, `end`,\n#'   gives the position of the end.\n#'\n#'   `str_locate_all()` returns a list of integer matrices as above, but\n#'   the matrices have one row for each match in the corresponding element\n#'   in `string`.\n\nIn other cases it can be easier to figure out what to describe by thinking about the set of functions and how they differ. For example, most dplyr functions return data frames, so just saying @return A data frame is not very useful. Instead we sat down and thought about exactly what makes each function different. We decided it makes sense to describe each function in terms of how it affects the rows, the columns, the groups, and the attributes. For example, here’s dplyr::filter():\n\n#' @returns\n#' An object of the same type as `.data`. The output has the following properties:\n#'\n#' * Rows are a subset of the input, but appear in the same order.\n#' * Columns are not modified.\n#' * The number of groups may be reduced (if `.preserve` is not `TRUE`).\n#' * Data frame attributes are preserved.\n\n@returns is also a good place to describe any important warnings or errors that the user might see here. For example readr::read_csv():\n\n#' @returns A [tibble()]. If there are parsing problems, a warning will alert you.\n#'   You can retrieve the full details by calling [problems()] on your dataset.\n\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nFor your initial CRAN submission, all functions must document their return value. This is not required for subsequent submission, but it’s still good practice. There’s currently no way to check that you’ve documented the return value of every function (we’re working on it) which is why you’ll notice some tidyverse functions lack output documentation."
  },
  {
    "objectID": "man.html#links-and-cross-references",
    "href": "man.html#links-and-cross-references",
    "title": "16  Function documentation",
    "section": "\n16.6 Links and cross-references",
    "text": "16.6 Links and cross-references\n\nRegular markdown to link to web pages: https://r-project.org or [The R Project](https://r-project.org).\nTo link to a function we slightly abuse markdown syntax: [function()] or [pkg::function()]. To link to non-function documentation just omit the (): [topic], [pkg::topic].\n\nUseful tags\n\n@seealso allows you to point to other useful resources, either on the web, in your package [functioname()], or another package [pkg::function()].\nIf you have a family of related functions where every function should link to every other function in the family, use @family. The value of @family should be plural.\n\nWhen you start using links (and images), you’ll also need to use a new documentation workflow, as the workflow described above does not show images or links between topics or. If you’d like to also see links, you can use this slower but more comprehensive workflow:\n\nRe-document you package Cmd + Shift + D.\nBuild and install your package by clicking  in the build pane or by pressing Ctrl/Cmd + Shift + B. This installs it in your regular library, then restarts R and reloads your package.\nPreview documentation with ?."
  },
  {
    "objectID": "man.html#sec-examples",
    "href": "man.html#sec-examples",
    "title": "16  Function documentation",
    "section": "\n16.7 Examples",
    "text": "16.7 Examples\nDescribing how a function works is great, but showing how it works is even better. That’s the role of the @examples tag, which uses executable R code to show what a function does. Unlike other parts of the documentation where we’ve focused mainly on what you should write, here we’ll briefly give some content advice and then focus mainly on the mechanics. The mechanics of examples are complex because they must not error, and they’re run in four different situations:\n\nInteractively using the example() function.\nDuring R CMD check on your computer, or another computer you control (e.g. GitHub action).\nDuring R CMD check run by CRAN.\nWhen building your pkgdown website.\n\nAfter discussing what to put in your examples, we’ll talk about keeping your examples self-contained, how to display errors if needed, handling dependencies, running examples conditionally, and\n\n16.7.1 Contents\nUse examples to show the basic operation of the function, and then to highlight any particularly important properties. For example, str_detect() starts by showing a few simple variations and then highlights a property you might easily miss from reading the docs: as well as passing a vector of strings and one pattern, you can also pass one string and vector of patterns.\n\n#' @examples\n#' fruit <- c(\"apple\", \"banana\", \"pear\", \"pineapple\")\n#' str_detect(fruit, \"a\")\n#' str_detect(fruit, \"^a\")\n#' str_detect(fruit, \"a$\")\n#' \n#' # Also vectorised over pattern\n#' str_detect(\"aecfg\", letters)\n\nTry to stay focused on the most important features without getting into the weeds of every last edge case: if you make the examples too long, it becomes hard for the user to find the key application that they’re looking for. If you find yourself writing very long examples, it may be a sign that you should write a vignette instead.\nThere aren’t any formal ways to break up your examples into sections but you can use sectioning comments that use many === or --- to create a visual breakdown. Here’s an example from tidyr::chop():\n\n#' @examples\n#' # Chop ==============================================================\n#' df <- tibble(x = c(1, 1, 1, 2, 2, 3), y = 1:6, z = 6:1)\n#' # Note that we get one row of output for each unique combination of\n#' # non-chopped variables\n#' df %>% chop(c(y, z))\n#' # cf nest\n#' df %>% nest(data = c(y, z))\n#'\n#' # Unchop ============================================================\n#' df <- tibble(x = 1:4, y = list(integer(), 1L, 1:2, 1:3))\n#' df %>% unchop(y)\n#' df %>% unchop(y, keep_empty = TRUE)\n#' \n#' #' # Incompatible types -------------------------------------------------\n#' # If the list-col contains types that can not be natively\n#' df <- tibble(x = 1:2, y = list(\"1\", 1:3))\n#' try(df %>% unchop(y))\n\nStrive to keep the examples focused on the specific function that you’re documenting. If you can make the point with a familiar built-in dataset, like iris, do so. If you find yourself needing to do a bunch of setup to create a dataset or object to use in the example, it may be a sign that you need to create a package dataset. See Chapter 8 for details.\n\n16.7.2 Pack it in; pack it out\nAs much as possible, keep your examples as self-contained as possible. For example, this means:\n\nIf you modify options(), reset them at the end of the example.\nIf you create a file, create it somewhere in tempdir() and make sure to delete it at the end of the example.\nDon’t change the working directory.\nDon’t write to the clipboard.\nAvoid accessing websites in examples. If the website is down, your example will fail and hence R CMD check will error.\n\nUnfortunately due to the way that examples are run during R CMD check there’s no way to use familiar tools like withr to enforce these constraints. Instead you’ll need to do it by hand.\nThese constraints are often in tension with good documentation if you’re trying to document a function that somehow changes the state of the world. So if you’re finding it really hard to follow these rules, this might be another sign to switch to a vignette.\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nMany of these constraints are also mentioned in the CRAN repository policy, which you must adhere to when submitting to CRAN. Use find in page to search for “malicious or anti-social” to see the details.\n\n\nAdditionally, you want your examples to send the user on a short walk, not a long hike. Examples need to execute relatively quickly so users can quickly see the results, it doesn’t take ages to build your website, automated checks happen quickly, and it doesn’t take up computing resources when submitting to CRAN.\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nAll examples must run in under 10 minutes.\n\n\n\n16.7.3 Errors\nWhat can you do if you want to include code that causes an error for the purposes of teaching. There are two basic options:\n\nYou can wrap the code in try() so that the error is shown, but doesn’t stop execution of the error.\nYou can wrap the code \\dontrun{}7 so it is never run by example().\n\n16.7.4 Dependencies and conditional execution\nYou can only use packages in examples that your package depends on (i.e. that appear in Imports or Suggests). Example code is run in the user’s environment, not the package environment, so you’ll have to either explicitly attach the package with library() or refer to each function with ::.\nIn the past, we recommended only using code from suggested packages inside an if block that used if (requireNamespace(\"suggested_package\", quietly = TRUE)). Today, we no longer recommend that technique because:\n\nWe expect that suggested packages are installed when running R CMD check8.\nThe cost of wrapping code in {} is high: you can no longer see intermediate results. The cost of a package not being installed is low: users can usually recognize the package not loaded error and can resolve it themselves.\n\nIn other cases, your example code may depend on something other than a package being installed. For example, if your examples talk to a web API, you probably only want to run them if the user is authenticated, and want to avoid such code being run on CRAN. In this case you can use @examplesIf instead of @examples. The code in an @examplesIf block will only be executed if some condition is TRUE:\n\n#' @examplesIf some_condition()\n#' some_other_function()\n#' some_more_functions()\n\ngoogledrive uses @examplesIf in almost every function because the examples can only work if you have an authenticated and active connection to Google Drive as judged by googledrive::drive_has_token(). For example, here’s googledrive::drive_publish():\n\n#' @examplesIf drive_has_token()\n#' # Create a file to publish\n#' file <- drive_example_remote(\"chicken_sheet\") %>%\n#'   drive_cp()\n#'\n#' # Publish file\n#' file <- drive_publish(file)\n#' file$published\n\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nFor initial CRAN submission of your package, all functions must contain some runnable examples (i.e. there must be examples and they must not all be wrapped in \\dontrun{}).\n\n\n\n16.7.5 Intermixing examples and text\nAn alternative to examples is to use RMarkdown’s code blocks, either ```R if you just want to show some code or ```{r} if you want the code to be run. These can be effective techniques but there are downsides to each:\n\nThe code in ```R blocks is never run; this means it’s easy to accidentally introduce syntax errors or to forget to update it when your package changes.\nThe code in ```{r} blocks is run every time you document the package. This has the nice advantage of including the output in the documentation (unlike examples), but the code can’t take very long to run or your iterative documentation workflow will become quite painful."
  },
  {
    "objectID": "man.html#re-using-documentation",
    "href": "man.html#re-using-documentation",
    "title": "16  Function documentation",
    "section": "\n16.8 Re-using documentation",
    "text": "16.8 Re-using documentation\nroxygen2 provides a number of features that allow you to reuse documentation across topics. They are documented in vignettes(\"reuse\", package = \"roxygen2\") so here we’ll focus on the three most important:\n\nDocumenting multiple functions in one topic.\nInheriting documentation from another topic.\nUse child documents to share prose between topics, or to share between documentation topics and vignettes.\n\n\n16.8.1 Multiple functions in one topic\nBy default, each function gets its own documentation topic, but if two functions are very closely connected you can combine the documentation for multiple functions into a single topic. For example, take str_length() and str_width() which provide two different ways of computing the size of a string. As you can see from the description, both functions are documented together, because this makes it easy to see how they differ:\n\n#' The length/width of a string\n#'\n#' @description\n#' `str_length()` returns the number of codepoints in a string. These are\n#' the individual elements (which are often, but not always letters) that\n#' can be extracted with [str_sub()].\n#'\n#' `str_width()` returns how much space the string will occupy when printed\n#' in a fixed width font (i.e. when printed in the console).\n#'\n#' ...\nstr_length <- function(string) {\n  ...\n}\n\nTo merge the two topics, str_width() uses @rdname str_length to add its documentation to an existing topic:\n\n#' @rdname str_length\nstr_width <- function(string) {\n  ...\n}\n\nThis technique is best used for functions that have not just similar arguments, but also similar return value and related examples, as discussed next.\n\n16.8.2 Inheriting documentation\nIn other cases, functions in a make might share many related behaviors, but aren’t closely enough connected that you want to document them together. Instead, you can use @inherits, which generalizes @inheritParams, to inherit any component of the document from one topic.\nThere are three useful inherit tags:\n\n\n@inherit source_function will inherit all supported components from source_function.\nYou can choose to only inherit selected components by listing them after the function name, e.g. @inherit source_function return details. The complete list of currently supported components are params, return, title, description, details, seealso, sections, references, examples, author, source, note.\n\n@inheritSection source_function Section title will inherit the single section with title “Section title” from source_function().\n\n@inheritDotParams automatically generates parameter documentation for ... for the common case where you pass ... on to another function. Because you often override some arguments, it comes with a flexible specification for argument selection:\n\n\n@inheritDotParams foo takes all parameters from foo().\n\n@inheritDotParams foo a b e:h takes parameters a, b, and all parameters between e and h.\n\n@inheritDotParams foo -x -y takes all parameters except for x and y.\n\n\n\nAll of these tags also work to inherit documentation from functions in another package by using pkg::source_function.\n\n16.8.3 Child documents\nFinally, you can use the same .Rmd or .md document in the documentation, README.Rmd, and vignettes by using RMarkdown child documents. The syntax looks like this:\n```{r child = \"common.Rmd\"}\n```\nThe included Rmd file can have roxygen Markdown-style links to other help topics. E.g. [roxygen2::roxygenize()] will link to the manual page of the roxygenize function in roxygen2. See vignette(\"rd-formatting\") for details.\nIf the Rmd file contains roxygen (Markdown-style) links to other help topics, then some care is needed, as those links will not work in Rmd files by default. A workaround is to specify external HTML links for them. These external locations will not be used for the manual which instead always links to the help topics in the manual. Example:\nSee also the [roxygen2::roxygenize()] function.\n\n[roxygen2::roxygenize()]: https://roxygen2.r-lib.org/reference/roxygenize.html\nThis example will link to the supplied URLs in HTML / Markdown files and it will link to the roxygenize help topic in the manual.\nNote that if you add external link targets like these, then roxygen will emit a warning about these link references being defined multiple times (once externally, and once to the help topic). This warning originates in Pandoc, and it is harmless."
  },
  {
    "objectID": "vignettes.html",
    "href": "vignettes.html",
    "title": "17  Vignettes",
    "section": "",
    "text": "Second edition\n\n\n\nYou are reading the work-in-progress second edition of R Packages. This chapter is undergoing heavy restructuring and may be confusing or incomplete."
  },
  {
    "objectID": "vignettes.html#introduction",
    "href": "vignettes.html#introduction",
    "title": "17  Vignettes",
    "section": "\n17.1 Introduction",
    "text": "17.1 Introduction\nA vignette is a long-form guide to your package. Function documentation is great if you know the name of the function you need, but it’s useless otherwise. A vignette is like a book chapter or an academic paper: it can describe the problem that your package is designed to solve, and then show the reader how to solve it.\nMany existing packages have vignettes. You can see all the installed vignettes with browseVignettes(). To see the vignette for a specific package, use the argument, browseVignettes(\"packagename\"). Each vignette provides three things: the original source file, a readable HTML page or PDF, and a file of R code. You can read a specific vignette with vignette(x), and see its code with edit(vignette(x)). To see vignettes for a package you haven’t installed, look at its CRAN page, e.g., https://cran.r-project.org/web/packages/dplyr.\nIn this chapter, we’re going to use RMarkdown to write our vignettes. If you’re not already familiar with RMarkdown you’ll need to learn the basics elsewhere; at good place to start is https://rmarkdown.rstudio.com/.\nOlder packages can include vignettes written with Sweave, a precursor to RMarkdown. If this describes, your package, I highly recommend switching to RMarkdown."
  },
  {
    "objectID": "vignettes.html#vignette-workflow",
    "href": "vignettes.html#vignette-workflow",
    "title": "17  Vignettes",
    "section": "\n17.2 Vignette workflow",
    "text": "17.2 Vignette workflow\nTo create your first vignette, run:\n\nusethis::use_vignette(\"my-vignette\")\n\nThis will:\n\nCreate a vignettes/ directory.\nAdd the necessary dependencies to DESCRIPTION (i.e. it adds knitr to the Suggests and VignetteBuilder fields).\nDraft a vignette, vignettes/my-vignette.Rmd.\n\nThe draft vignette has been designed to remind you of the important parts of an R Markdown file. It serves as a useful reference when you’re creating a new vignette. Once you have this file, the workflow is straightforward:\n\nModify the vignette.\nPress Ctrl/Cmd + Shift + K (or click ) to knit the vignette and preview the output.\nThis builds with the installed package — but you probably want the dev package. Use devtools::build_rmd().\n\nThe check workflow, Cmd + Shift + E, will run the code in all vignettes. This is a good way to verify that you’ve captured all the needed dependencies."
  },
  {
    "objectID": "vignettes.html#vignette-metadata",
    "href": "vignettes.html#vignette-metadata",
    "title": "17  Vignettes",
    "section": "\n17.3 Metadata",
    "text": "17.3 Metadata\nThe first few lines of the vignette contain important metadata. The default template contains the following information:\n---\ntitle: \"Vignette Title\"\noutput: rmarkdown::html_vignette\nvignette: >\n  %\\VignetteIndexEntry{Vignette Title}\n  %\\VignetteEngine{knitr::rmarkdown}\n  %\\VignetteEncoding{UTF-8}\n---\nThis metadata is written in yaml, a format designed to be both human and computer readable. The basics of the syntax is much like the DESCRIPTION file, where each line consists of a field name, a colon, then the value of the field. The one special YAML feature we’re using here is >. It indicates the following lines of text are plain text and shouldn’t use any special YAML features.\nThe fields are:\n\ntitle and description. If you change the title, you must also change the VignetteIndexEntry{} described below.\nauthor: we don’t use this unless the vignette author is different to the package author.\ndate: don’t recommend this either as it’s very easy to forget to update. You could use Sys.date(), but this shows when the vignette was built, which might be very different to when it was last updated.\nOutput: this tells rmarkdown which output formatter to use. There are many options that are useful for regular reports (including html, pdf, slideshows, …) but rmarkdown::html_vignette has been specifically designed to work well inside packages. See ?rmarkdown::html_vignette for more details.\nVignette: this contains a special block of metadata needed by R. Here, you can see the legacy of LaTeX vignettes: the metadata looks like LaTeX commands. You’ll need to modify the \\VignetteIndexEntry to provide the title of your vignette as you’d like it to appear in the vignette index. Leave the other two lines as is. They tell R to use knitr to process the file, and that the file is encoded in UTF-8 (the only encoding you should ever use to write vignettes).\n\nAlso includes block to set up some standard options:\n``` {r, echo = FALSE}\nknitr::opts_chunk$set(collapse = TRUE, comment = \"#>\")\n```\ncollapse = TRUE and comment = \"#>\" are my preferred way of displaying code output. I usually set these globally by putting the following knitr block at the start of my document."
  },
  {
    "objectID": "vignettes.html#controlling-evaluation",
    "href": "vignettes.html#controlling-evaluation",
    "title": "17  Vignettes",
    "section": "\n17.4 Controlling evaluation",
    "text": "17.4 Controlling evaluation\nYour vignettes will be evaluated in many different places, not just your computer — CI/CD, CRAN, and users can run on their computers (although not typical). Need to make sure they work everywhere which can be challenging if your\nAny packages used by your vignette must be listed in Imports or Suggests fields. Generally safe to assume that suggest packages will be installed when you vignette is executed. But if a package is particularly hard to install you might want to safeguard using one of the tools below.\nYou’re probably already familiar with the chunk option eval = FALSE. But can also set for all later chunks with knitr::opts_chunk$set(eval = FALSE). This is particularly useful for:\n\neval = requireNamespace(\"package\")\neval = !identical(Sys.getenv(\"foo\"), \"\")\neval = file.exists(\"special-key\")\n\nA final option if you want to don’t want to execute at all on CRAN. Another option is to create an “article”; an Rmd that appears only on the website that’s not embedded in the package. This makes it slightly less accessible, but it’s fine if you have a pkgdown website.\nMany other options are described at https://yihui.name/knitr/options.\nerror = TRUE captures any errors in the block and shows them inline. This is useful if you want to demonstrate what happens if code throws an error."
  },
  {
    "objectID": "vignettes.html#vignette-advice",
    "href": "vignettes.html#vignette-advice",
    "title": "17  Vignettes",
    "section": "\n17.5 Advice",
    "text": "17.5 Advice\n\nIf you’re thinking without writing, you only think you’re thinking. — Leslie Lamport\n\nWhen writing a vignette, you’re teaching someone how to use your package. You need to put yourself in the readers’ shoes, and adopt a “beginner’s mind”. This can be difficult because it’s hard to forget all of the knowledge that you’ve already internalised. For this reason, we find in-person teaching to be a really useful way to get feedback. You’re immediately confronted with what you’ve forgotten that only you know.\nA useful side effect of this approach is that it helps you improve your code. It forces you to re-see the initial onboarding process and to appreciate the parts that are hard. Our experience is that explaining how code works often reveals some problems that need fixing. (In fact, a key part of the tidyverse package release process is writing a blog post: we now do that before submitting to CRAN because of the number of times it’s revealed some subtle problem that requires a fix).\nIn the tidyverse, I think we’re generally always a little behind on vignettes and we need more than we currently have.\nWriting a vignette also makes a nice break from coding. Writing seems to use a different part of the brain from programming, so if you’re sick of programming, try writing for a bit.\n\n17.5.1 Writing\n\nI strongly recommend literally anything written by Kathy Sierra. Her old blog, Creating passionate users is full of advice about programming, teaching, and how to create valuable tools. I thoroughly recommend reading through all the older content. Her new blog, Serious Pony, doesn’t have as much content, but it has some great articles.\nIf you’d like to learn how to write better, I highly recommend Style: Lessons in Clarity and Grace by Joseph M. Williams and Joseph Bizup. It helps you understand the structure of writing so that you’ll be better able to recognise and fix bad writing.\n\n17.5.2 Diagrams\n\n\n\n\n\n\nSubmitting to CRAN\n\n\n\nYou’ll need to watch the file size. If you include a lot of graphics, it’s easy to create a very large file. Be on the look out for a NOTE that complains about an overly large directory.\n\n\n\n17.5.3 Organisation\nFor simpler packages, one vignette is often sufficient. Call it pkgname.Rmd; that takes advantage of a pkgdown convention which will automatically link “Getting Started” to your vignette.\nBut for more complicated packages you may actually need more than one. In fact, you can have as many vignettes as you like. I tend to think of them like chapters of a book – they should be self-contained, but still link together into a cohesive whole.\n\n17.5.4 Scientific publication\nVignettes can also be useful if you want to explain the details of your package. For example, if you have implemented a complex statistical algorithm, you might want to describe all the details in a vignette so that users of your package can understand what’s going on under the hood, and be confident that you’ve implemented the algorithm correctly. In this case, you might also consider submitting your vignette to the Journal of Statistical Software or The R Journal. Both journals are electronic only and peer-reviewed. Comments from reviewers can be very helpful for improving your package and vignette.\nIf you just want to provide something very lightweight so folks have an easy time citing your package you might also consider the Journal of Open Source Software. This journal has a particularly speedy submission and review process, and is where we published “Welcome to the Tidyverse”, a paper we wrote so that folks could have a single paper to cite and all the tidyverse authors would get some academic credit."
  },
  {
    "objectID": "other-markdown.html",
    "href": "other-markdown.html",
    "title": "18  Other markdown files",
    "section": "",
    "text": "Second edition\n\n\n\nYou are reading the work-in-progress second edition of R Packages. This chapter is currently a dumping ground for ideas, and we don’t recommend reading it."
  },
  {
    "objectID": "other-markdown.html#introduction",
    "href": "other-markdown.html#introduction",
    "title": "18  Other markdown files",
    "section": "\n18.1 Introduction",
    "text": "18.1 Introduction\nYou now have a package that’s ready to submit to CRAN. But before you do, there are two important files that you should update: README.md which describes what the package does, and NEWS.md which describes what’s changed since the previous version. I recommend using Markdown for these files, because it’s useful for them to be readable as both plain text (e.g. in emails) and HTML (e.g. on GitHub, in blog posts). The basic writing and formatting syntax is available at https://help.github.com/articles/basic-writing-and-formatting-syntax/."
  },
  {
    "objectID": "other-markdown.html#sec-readme",
    "href": "other-markdown.html#sec-readme",
    "title": "18  Other markdown files",
    "section": "\n18.2 README",
    "text": "18.2 README\n\n18.2.1 README.md\nThe goal of the README.md is to answer the following questions about your package:\n\nWhy should I use it?\nHow do I use it?\nHow do I get it?\n\nOn GitHub, the README.md will be rendered as HTML and displayed on the repository home page.\nI normally structure my README as follows:\n\nA paragraph that describes the high-level purpose of the package.\nAn example that shows how to use the package to solve a simple problem.\nInstallation instructions, giving code that can be copied and pasted into R.\nAn overview that describes the main components of the package. For more complex packages, this will point to vignettes for more details.\n\n18.2.2 README.Rmd\nIf you include an example in your README (a good idea!) you may want to generate it with R Markdown. The easiest way to get started is to use usethis::use_readme_rmd(). This creates a template README.Rmd and adds it to .Rbuildignore. The template looks like:\n---\noutput: github_document\n---\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n\n\n``` {r, include = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  fig.path = \"man/figures/README-\",\n  out.width = \"100%\"\n)\n```\nThis:\n\nOutputs Github flavoured Markdown.\nIncludes a comment in README.md to remind you to edit README.Rmd, not README.md.\nSets up my recommended knitr options, including saving images to man/figures/README- which ensures that they’re included in your built package (which is important so that your README works when it’s displayed by CRAN).\n\nYou’ll need to remember to re-knit README.Rmd each time you modify it. If you use git, use_readme_rmd() automatically adds the following “pre-commit” hook:\n#!/bin/bash\nif [[ README.Rmd -nt README.md ]]; then\n  echo \"README.md is out of date; please re-knit README.Rmd\"\n  exit 1\nfi \nThis prevents git commit from succeeding unless README.md is more recent than README.Rmd. If you get a false positive, you can ignore the check with git commit --no-verify. Note that git commit hooks are not stored in the repository, so every time you clone the repo, you’ll need to run usethis::use_readme_rmd() to set it up again.\n\n18.2.3 Development badges\nA number of usethis helpers automatically add development status badges to your readme so that readers can see:\n\nThe current version of your package on CRAN, usethis::use_cran_badge().\nThe amount of code coverage, added by use_coverage().\nThe R CMD check status of your development package, added by use_github_action_check_standard() and friends."
  },
  {
    "objectID": "other-markdown.html#news",
    "href": "other-markdown.html#news",
    "title": "18  Other markdown files",
    "section": "\n18.3 NEWS.md",
    "text": "18.3 NEWS.md\nThe README.md is aimed at new users. The NEWS.md is aimed at existing users: it should list all the API changes in each release. There are a number of formats you can use for package news, but I recommend NEWS.md. It’s well supported by GitHub, permitted by CRAN, and is easy to re-purpose for other formats.\nOrganise your NEWS.md as follows:\n\nUse a top-level heading for each version: e.g. # mypackage 1.0. The most recent version should go at the top. (pkgdown supports a few other formats, see more at https://pkgdown.r-lib.org/reference/build_news.html)\nEach change should be included in a bulleted list. If you have a lot of changes you might want to break them up using subheadings, ## Major changes, ## Bug fixes etc. I usually stick with a simple list until just before releasing the package when I’ll reorganise into sections, if needed. It’s hard to know in advance exactly what sections you’ll need.\nIf an item is related to an issue in GitHub, include the issue number in parentheses, e.g. (#​10). If an item is related to a pull request, include the pull request number and the author, e.g. (#​101, @hadley). Doing this makes it easy to navigate to the relevant issues on GitHub.\n\nThe main challenge with NEWS.md is getting into the habit of noting a change as you make a change."
  },
  {
    "objectID": "other-markdown.html#other-github-files",
    "href": "other-markdown.html#other-github-files",
    "title": "18  Other markdown files",
    "section": "\n18.4 Other GitHub files",
    "text": "18.4 Other GitHub files\nYou’ll also find a number of other files in the .github directory. These are mostly useful for GitHub but are also supported by pkgdown:\n\n\nLICENSE.md: complete text of the license of your package. Discussed in …\n\ncran-comments.md: sent to CRAN when you submit a package. Discussed in …\n\nSUPPORT.md:\n\nCONTRIBUTING.md:\n\nCODE_OF_CONDUCT.md:\n\nISSUE_TEMPLATE and PULL_REQUEST_TEMPLATE"
  },
  {
    "objectID": "website.html",
    "href": "website.html",
    "title": "19  Website",
    "section": "",
    "text": "Second edition\n\n\n\nYou are reading the work-in-progress second edition of R Packages. This chapter is currently a dumping ground for ideas, and we don’t recommend reading it."
  },
  {
    "objectID": "website.html#introduction",
    "href": "website.html#introduction",
    "title": "19  Website",
    "section": "\n19.1 Introduction",
    "text": "19.1 Introduction"
  },
  {
    "objectID": "website.html#logo",
    "href": "website.html#logo",
    "title": "19  Website",
    "section": "\n19.2 Logo",
    "text": "19.2 Logo\nWebsite is also a great way to show off your package logo.\nuse_logo()\nInclude hexagon spec.\nMention how to get as a sticker."
  },
  {
    "objectID": "check.html",
    "href": "check.html",
    "title": "20  R CMD check",
    "section": "",
    "text": "Second edition\n\n\n\nYou are reading the work-in-progress second edition of R Packages. This chapter is undergoing heavy restructuring and may be confusing or incomplete."
  },
  {
    "objectID": "check.html#introduction",
    "href": "check.html#introduction",
    "title": "20  R CMD check",
    "section": "\n20.1 Introduction",
    "text": "20.1 Introduction\nAn important part of the package development process is R CMD check. R CMD check automatically checks your code for common problems. It’s essential if you’re planning on submitting to CRAN, but it’s useful even if you’re not because it automatically detects many common problems that you’d otherwise discover the hard way.\nR CMD check will be frustrating the first time you run it - you’ll discover many problems that need to be fixed. The key to making R CMD check less frustrating is to actually run it more often: the sooner you find a problem, the easier it is to fix. The upper limit of this approach is to run R CMD check every time you make a change. If you use GitHub, you’ll learn precisely how to do that in Section 21.2."
  },
  {
    "objectID": "check.html#check-workflow",
    "href": "check.html#check-workflow",
    "title": "20  R CMD check",
    "section": "\n20.2 Workflow",
    "text": "20.2 Workflow\nR CMD check is the name of the command you run from the terminal. I don’t recommend calling it directly. Instead, run devtools::check(), or press Ctrl/Cmd + Shift + E in RStudio. In contrast to R CMD check, devtools::check():\n\nEnsures that the documentation is up-to-date by running devtools::document().\nBundles the package before checking it. This is the best practice for checking packages because it makes sure the check starts with a clean slate: because a package bundle doesn’t contain any of the temporary files that can accumulate in your source package, e.g. artifacts like .so and .o files which accompany compiled code, you can avoid the spurious warnings such files will generate.\nSets the NOT_CRAN environment variable to TRUE. This allows you to selectively skip tests on CRAN. (See ?testthat::skip_on_cran for details.)\n\nThe workflow for checking a package is simple, but tedious:\n\nRun devtools::check(), or press Ctrl/Cmd + Shift + E.\nFix the first problem.\nRepeat until there are no more problems.\n\nR CMD check returns three types of messages:\n\nERRORs: Severe problems that you should fix regardless of whether or not you’re submitting to CRAN.\nWARNINGs: Likely problems that you must fix if you’re planning to submit to CRAN (and a good idea to look into even if you’re not).\nNOTEs: Mild problems. If you are submitting to CRAN, you should strive to eliminate all NOTEs, even if they are false positives. If you have no NOTEs, human intervention is not required, and the package submission process will be easier. If it’s not possible to eliminate a NOTE, you’ll need describe why it’s OK in your submission comments, as described in Section 22.2. If you’re not submitting to CRAN, carefully read each NOTE, but don’t go out of your way to fix things that you don’t think are problems."
  },
  {
    "objectID": "check.html#sec-check-checks",
    "href": "check.html#sec-check-checks",
    "title": "20  R CMD check",
    "section": "\n20.3 Checks",
    "text": "20.3 Checks\nR CMD check is composed of over 50 individual checks, described in the following sections. For each check, I briefly describe what it does, what the most common problems are, and how to fix them. When you have a problem with R CMD check and can’t understand how to fix it, use this list to help figure out what you need to do. To make it easier to understand how the checks fit together, I’ve organised them into sections roughly corresponding to the chapters in this book. This means they will be in a somewhat different order to what you’ll see when you run check().\nThis list includes every check run in R 3.1.1. If you’re using a more recent version, you may want to consult the most recent online version of this chapter: https://r-pkgs.org/r-cmd-check.html. Please let me know if you encounter a problem that this chapter doesn’t help with.\n\n20.3.1 Check metadata\nR CMD check always starts by describing your current environment. I’m running R 3.1.1 on OS X with a UTF-8 charset:\n\n\nUsing log directory ‘/Users/hadley/Documents/web/httr.Rcheck’\n\nUsing R version 3.1.1 (2014-07-10)\n\nUsing platform: x86_64-apple-darwin13.1.0 (64-bit)\n\nUsing session charset: UTF-8\n\nNext the description is parsed and the package version is printed. Here I’m checking httr version 0.5.0.9000 (you’ll learn more about that weird version number in Section 23.3).\n\n\n\nChecking for file ‘httr/DESCRIPTION’\n\nThis is package ‘httr’ version ‘0.5.0.9000’\n\n20.3.2 Package structure\n\n\n\nChecking package directory. The directory you’re checking must exist - devtools::check() protects you against this problem.\n\n\n\nChecking if this is a source package. You must check a source package, not a binary or installed package. This should never fail if you usedevtools::check().\n\n\n\nChecking for executable files. You must not have executable files in your package: they’re not portable, they’re not open source, and they are a security risk. Delete any executable files from your package. (If you’re not submitting to CRAN, you can silence this warning by listing each executable file in the BinaryFiles field in your DESCRIPTION.)\n\n\n\nChecking for hidden files and directories. On Linux and OS X, files with a name starting with . are hidden by default, and you’ve probably included them in your package by mistake. Either delete them, or if they are important, use .Rbuildignore to remove them from the package bundle. R automatically removes some common directories like .git and .svn.\n\n\n\nChecking for portable file names. R packages must work on Windows, Linux and OS X, so you can only use file names that work on all platforms. The easiest way to do this is to stick to letters, numbers, underscores and dashes. Avoid non-English letters and spaces. Fix this check by renaming the listed files.\n\n\n\nChecking for sufficient/correct file permissions. If you can’t read a file, you can’t check it. This check detects the unlikely occurence that you have files in the package that you don’t have permission to read. Fix this problem by fixing the file permissions.\n\n\n\nChecking whether package ‘XYZ’ can be installed. R CMD check runs R CMD install to make sure that it’s possible to install your package. If this fails, you should run devtools::install() or RStudio’s Build & Reload and debug any problems before continuing.\n\n\n\nChecking installed package size. It’s easy to accidentally include large files that blow up the size of your package. This check ensures that the whole package is less than 5 MB and each subdirectory is less than 1 MB. If you see this message, check that you haven’t accidentally included a large file.\nIf submitting to CRAN, you’ll need to justify the size of your package. First, make sure the package is as small as it possibly can be: try recompressing the data, Section 8.6; and minimising vignettes, Chapter 17. If it’s still too large, consider moving data into its own package.\n\n\n\n\nChecking top-level files. Only specified files and directories are allowed at the top level of the package (e.g. DESCRIPTION, R/, src/). To include other files, you have two choices:\n\nIf they don’t need to be installed (i.e. they’re only used in the source package): add them to .Rbuildignore with usethis::use_build_ignore().\nIf they need to be installed: move them into inst/. They’ll be moved back to the top-level package directory when installed.\n\n\n\n\n\nChecking package subdirectories.\n\nDon’t include any empty directories. These are usually removed automatically by R CMD build so you shouldn’t see this error. If you do, just delete the directory.\nThe case of files and directories is important. All sub-directories should be lower-case, except for R/. A citation file, if present, should be in inst/CITATION. Rename as needed.\nThe contents of inst/ shouldn’t clash with top-level contents of the package (like build/, R/ etc). If they do, rename your files/directories.\n\n\n\n\n\nChecking for left-over files. Remove any files listed here. They’ve been included in your package by accident.\n\n20.3.3 Description\n\n\n\nChecking DESCRIPTION meta-information.\n\nThe DESCRIPTION must be valid. You are unlikely to see this error, because devtools::load_all() runs the same check each time you re-load the package.\nIf you use any non-ASCII characters in the DESCRIPTION, you must also specify an encoding. There are only three encodings that work on all platforms: latin1, latin2 and UTF-8. I strongly recommend UTF-8: Encoding: UTF-8.\nThe License must refer to either a known license (a complete list can be found at https://svn.r-project.org/R/trunk/share/licenses/license.db), or it must use file LICENSE and that file must exist. Errors here are most likely to be typos.\nYou should either provide Authors@R or Authors and Maintainer. You’ll get an error if you’ve specified both, which you can fix by removing the one you didn’t want.\n\n\n\n\n\nChecking package dependencies.\n\nAll packages listed in Depends, Imports and LinkingTo must be installed, and their version requirements must be met, otherwise your package can’t be checked. An easy way to install any missing or outdated dependencies is to run devtools::install_deps(dependencies = TRUE).\nPackages listed in Suggests must be installed, unless you’ve set the environment variable _R_CHECK_FORCE_SUGGESTS_ to a false value (e.g. with check(force_suggests = FALSE)). This is useful if some of the suggested packages are not available on all platforms.\nR packages can not have a cycle of dependencies: i.e. if package A requires B, then B can not require A (otherwise which one would you load first?). If you see this error, you’ll need to rethink the design of your package. One easy fix is to move the conflicting package from Imports or Depends to Suggests.\nAny packages used in the NAMESPACE must be listed in one of Imports (most commonly) or Depends (only in special cases). See Section 11.5.2 for more details.\nEvery package listed in Depends must also be imported in the NAMESPACE or accessed with pkg::foo. If you don’t do this, your package will work when attached to the search path (with library(mypackage)) but will not work when only loaded (e.g. mypackage::foo())\n\n\n\n\n\nChecking CRAN incoming feasibility. These checks only apply if you’re submitting to CRAN.\n\nIf you’re submitting a new package, you can’t use the same name as an existing package. You’ll need to come up with a new name.\nIf you’re submitting an update, the version number must be higher than the current CRAN version. Update the Version field in DESCRIPTION.\nIf the maintainer of the package has changed (even if it’s just a change in email address), the new maintainer should submit to CRAN, and the old maintainer should send a confirmation email.\nYou must use a standard open source license, as listed in https://svn.r-project.org/R/trunk/share/licenses/license.db. You can not use a custom license as CRAN does not have the legal resources to review custom agreements.\nThe Title and Description must be free from spelling mistakes. The title of the package must be in title case. Neither title nor description should include either the name of your package or the word “package”. Reword your title and description as needed.\nIf you’re submitting a new package, you’ll always get a NOTE. This reminds the CRAN maintainers to do some extra manual checks.\nAvoid submitting multiple versions of the same package in a short period of time. CRAN prefers at most one submission per month. If you need to fix a major bug, be apologetic.\n\n\n\n20.3.4 Namespace\n\n\n\nChecking if there is a namespace. You must have a NAMESPACE file. This is handled for your automatically by the devtools workflow.\n\n\n\nChecking package namespace information. The NAMESPACE should be parseable by parseNamespaceFile() and valid. If this check fails, it’s a bug in roxygen2.\n\n\n\nChecking whether the package can be loaded with stated dependencies. Runs library(pkg) with R_DEFAULT_PACKAGES=NULL, so the search path is empty (i.e. stats, graphics, grDevices, utils, datasets and methods are not attached like usual). Failure here typically indicates that you’re missing a dependency on one of those packages.\n\n\n\nChecking whether the namespace can be loaded with stated dependencies. Runs loadNamespace(pkg) with R_DEFAULT_PACKAGES=NULL. Failure usually indicates a problem with the namespace.\n\n20.3.5 R code\n\n\n\nChecking R files for non-ASCII characters. For maximum portability (i.e. so people can use your package on Windows) you should avoid using non-ASCII characters in R files. It’s ok to use them in comments, but object names shouldn’t use them, and in strings you should use unicode escapes. See Section 7.8 for more details.\n\n\n\nChecking R files for syntax errors. Obviously your R code must be valid. You’re unlikely to see this error if you’ve been regularly using devtools::load_all().\n\n\n\nChecking dependencies in R code. Errors here often indicate that you’ve forgotten to declare a needed package in the DESCRIPTION. Remember that you should never use require() or library() inside a package - see Section 11.5.4 for more details on best practices.\nAlternatively, you may have accidentally used ::: to access an exported function from a package. Switch to :: instead.\n\n\n\n\nChecking S3 generic/method consistency. S3 methods must have a compatible function signature with their generic. This means that the method must have the same arguments as its generic, with one exception: if the generic includes ... the method can have additional arguments.\nA common cause of this error is defining print methods, because the print() generic contains...:\n\n# BAD\nprint.my_class <- function(x) cat(\"Hi\")\n\n# GOOD\nprint.my_class <- function(x, ...) cat(\"Hi\")\n\n# Also ok\nprint.my_class <- function(x, ..., my_arg = TRUE) cat(\"Hi\")\n\n\n\n\n\nChecking replacement functions. Replacement functions (e.g. functions that are called like foo(x) <- y), must have value as the last argument.\n\n\n\nChecking R code for possible problems. This is a compound check for a wide range of problems:\n\nCalls to library.dynam() (and library.dynam.unload()) should look like library.dynam(\"name\"), not library.dynam(\"name.dll\"). Remove the extension to fix this error.\nPut library.dynam() in .onLoad(), not .onAttach(); put packageStartupMessage() in .onAttach(), not .onLoad(). Put library.dynam.unload() in .onUnload(). If you use any of these functions, make sure they’re in the right place.\nDon’t use unlockBinding() or assignInNamespace() to modify objects that don’t belong to you.\ncodetools::checkUsagePackage() is called to check that your functions don’t use variables that don’t exist. This sometimes raises false positives with functions that use non-standard evaluation (NSE), like subset() or with(). Generally, I think you should avoid NSE in package functions, and hence avoid this NOTE, but if you can not, see ?globalVariables for how to suppress this NOTE.\nYou are not allowed to use .Internal() in a package. Either call the R wrapper function, or write your own C function. (If you copy and paste the C function from base R, make sure to maintain the copyright notice, use a GPL-2 compatible license, and list R-core in the Author field.)\nSimilarly you are not allowed to use ::: to access non-exported functions from other packages. Either ask the package maintainer to export the function you need, or write your own version of it using exported functions. Alternatively, if the licenses are compatible you can copy and paste the exported function into your own package. If you do this, remember to update Authors@R.\n\nDon’t use assign() to modify objects in the global environment. If you need to maintain state across function calls, create your own environment with e <- new.env(parent = emptyenv()) and set and get values in it:\n\ne <- new.env(parent = emptyenv())\n\nadd_up <- function(x) {\n  if (is.null(e$last_x)) {\n    old <- 0\n  } else {\n    old <- e$last_x\n  }\n\n  new <- old + x\n  e$last_x <- new\n  new\n}\nadd_up(10)\n#> [1] 10\nadd_up(20)\n#> [1] 30\n\n\nDon’t use attach() in your code. Instead refer to variables explicitly.\nDon’t use data() without specifying the envir argument. Otherwise the data will be loaded in the global environment.\nDon’t use deprecated or defunct functions. Update your code to use the latest versions.\nYou must use TRUE and FALSE in your code (and examples), not T and F.\n\n\n\n\n\nChecking whether the package can be loaded. R loads your package with library(). Failure here typically indicates a problem with .onLoad() or .onAttach().\n\n\n\nChecking whether the package can be unloaded cleanly. Loads with library() and then detach()es. If this fails, check .onUnload() and .onDetach().\n\n\n\nChecking whether the namespace can be unloaded cleanly. Runs loadNamespace(\"pkg\"); unloadNamespace(\"pkg\"). Check .onUnload() for problems.\n\n\n\nChecking loading without being on the library search path. Calls library(x, lib.loc = ...). Failure here indicates that you are making a false assumption in .onLoad() or .onAttach().\n\n20.3.6 Data\n\n\n\nChecking contents of ‘data’ directory.\n\nThe data directory can only contain file types described in Section 8.2.\nData files can contain non-ASCII characters only if the encoding is correctly set. This usually shouldn’t be a problem if you’re saving .Rdata files. If you do see this error, look at the Encoding() of each column in the data frame, and ensure none are “unknown”. (You’ll typically need to fix this somewhere in the import process).\nIf you’ve compressed a data file with bzip2 or xz you need to declare at least Depends: R (>= 2.10) in your DESCRIPTION.\nIf you’ve used a sub-optimal compression algorithm for your data, re-compress with the suggested algorithm.\n\n\n\n20.3.7 Documentation\nYou can run the most common of these outside devtools::check() with devtools::check_man() (which automatically calls devtools::document() for you). If you have documentation problems, it’s best to iterate quickly with check_man(), rather than running the full check each time.\n\n\n\nChecking Rd files. This checks that all man/*.Rd files use the correct Rd syntax. If this fails, it indicates a bug in roxygen2.\n\n\n\nChecking Rd metadata. Names and aliases must be unique across all documentation files in a package. If you encounter this problem you’ve accidentally used the same @name or @aliases in multiple places; make sure they’re unique.\n\n\n\nChecking Rd line widths. Lines in Rd files must be less than 90 characters wide. This is unlikely to occur if you wrap your R code, and hence roxygen comments, to 80 characters. For very long urls, use a link-shortening service like bit.ly.\n\n\n\nChecking Rd cross-references. Errors here usually represent typos. Recall the syntax for linking to functions in other packages: \\link[package_name]{function_name}. Sometimes I accidentally switch the order of \\code{} and \\link{}: \\link{\\code{function}} will not work.\n\n\n\nChecking for missing documentation entries. All exported objects must be documented. See ?tools::undoc for more details.\n\n\n\nChecking for code/documentation mismatches. This check ensures that the documentation matches the code. This should never fail because you’re using roxygen2 which automatically keeps them in sync.\n\n\n\nChecking Rd \\usage sections. All arguments must be documented, and all @params must document an existing argument. You may have forgotten to document an argument, forgotten to remove the documentation for an argument that you’ve removed, or misspelled an argument name.\nS3 and S4 methods need to use special \\S3method{} and \\S4method{} markup in the Rd file. Roxygen2 will generate this for you automatically.\n\n\n\n\nChecking Rd contents. This checks for autogenerated content made by package.skeleton(). Since you’re not using package.skeleton() you should never have a problem here.\n\n\n\nChecking for unstated dependencies in examples. If you use a package only for an example, make sure it’s listed in the Suggests field. Before running example code that depends on it, test to see if it’s available with requireNamespace(\"pkg\", quietly = TRUE):\n\n#' @examples\n#' if (requireNamespace(\"dplyr\", quietly = TRUE)) {\n#'   ...\n#' }\n\n\n\n\n\nChecking examples. Every documentation example must run without errors, and must not take too long. See sec-examples for details.\n\n\n\nChecking PDF version of manual. Occassionally you’ll get an error when building the PDF manual. This is usually because the pdf is built by latex and you’ve forgotten to escape something. Debugging this is painful - your best bet is to look up the latex logs and combined tex file and work back from there to .Rd files then back to a roxygen comment. I consider any such failure to be a bug in roxygen2, so please let me know.\n\n20.3.8 Demos\n\n\n\nChecking index information. If you’ve written demos, each demo must be listed in demo/00Index. The file should look like:\ndemo-name-without-extension  Demo description\nanother-demo-name            Another description\n\n\n20.3.9 Compiled code\n\n\n\nChecking foreign function calls. .Call(), .C(), .Fortran(), .External() must always be called either with a NativeSymbolInfo object (as created with @useDynLib) or use the .package argument. See ?tools::checkFF for more details.\n\n\n\nChecking line endings in C/C++/Fortran sources/headers. Always use LF as a line ending.\n\n\n\nChecking line endings in Makefiles. As above.\n\n\n\nChecking for portable use of $(BLAS_LIBS) and $(LAPACK_LIBS). Errors here indicate an issue with your use of BLAS and LAPACK.\n\n\n\nChecking compiled code. Checks that you’re not using any C functions that you shouldn’t. ### Tests {#tests}\n\n\n\nChecking for unstated dependencies in tests. Every package used by tests must be included in the dependencies.\n\n\n\nChecking tests. Each file in tests/ is run. If you’ve followed the instructions in Chapter 13 you’ll have at least one file: testthat.R. The output from R CMD check is not usually that helpful, so you may need to look at the logfile package.Rcheck/tests/testthat.Rout. Fix any failing tests by iterating with devtools::test().\nOccasionally you may have a problem where the tests pass when run interactively with devtools::test(), but fail when in R CMD check. This usually indicates that you’ve made a faulty assumption about the testing environment, and it’s often hard to figure it out.\n\n\n20.3.10 Vignettes\n\n\n\nChecking ‘build’ directory. build/ is used to track vignette builds. I’m not sure how this check could fail unless you’ve accidentally .Rbuildignored the build/ directory.\n\n\n\nChecking installed files from ‘inst/doc’. Don’t put files in inst/doc - vignettes now live in vignettes/.\n\n\n\nChecking files in ‘vignettes’. Problems here are usually straightforward - you’ve included files that are already included in R (like jss.cls, jss.bst, or Sweave.sty), or you have leftover latex compilation files. Delete these files.\n\n\n\nChecking for sizes of PDF files under ‘inst/doc’. If you’re making PDF vignettes, you can make them as small as possible by running tools::compactPDF().\n\n\n\nChecking for unstated dependencies in vignettes. As with tests, every package that you use in a vignette must be listed in the DESCRIPTION. If a package is used only for a vignette, and not elsewhere, make sure it’s listed in Suggests.\n\n\n\nChecking package vignettes in ‘inst/doc’. This checks that every source vignette (i.e. .Rmd) has a built equivalent (i.e. .html) in inst/doc. This shouldn’t fail if you’ve used the standard process outlined in Chapter 17. If there is a problem, start by checking your .Rbuildignore.\n\n\n\nChecking running R code from vignettes. The R code from each vignette is run. If you want to deliberately execute errors (to show the user what failure looks like), make sure the chunk has error = TRUE, purl = FALSE.\n\n\n\nChecking re-building of vignette outputs. Each vignette is re-knit to make sure that the output corresponds to the input. Again, this shouldn’t fail in normal circumstances.\n\nTo run vignettes, the package first must be installed. That means check():\n\nBuilds the package.\nInstalls the package without vignettes.\nBuilds all the vignettes.\nRe-installs the package with vignettes.\n\nIf you have a lot of compiled code, this can be rather slow. You may want to add --no-build-vignettes to the commands list in “Build Source Packages” field in the project options:"
  },
  {
    "objectID": "continuous-integration.html",
    "href": "continuous-integration.html",
    "title": "21  Continuous integration",
    "section": "",
    "text": "Second edition\n\n\n\nYou are reading the work-in-progress second edition of R Packages. This chapter is currently a dumping ground for ideas, and we don’t recommend reading it."
  },
  {
    "objectID": "continuous-integration.html#introduction",
    "href": "continuous-integration.html#introduction",
    "title": "21  Continuous integration",
    "section": "\n21.1 Introduction",
    "text": "21.1 Introduction"
  },
  {
    "objectID": "continuous-integration.html#sec-gha",
    "href": "continuous-integration.html#sec-gha",
    "title": "21  Continuous integration",
    "section": "\n21.2 Checking after every commit with GitHub actions",
    "text": "21.2 Checking after every commit with GitHub actions\nIf you’re using GitHub, as we recommend, you should also use GitHub Actions. GitHub Actions allow you to run code every time you push to GitHub.\nTo use GitHub Actions:\n\nRun usethis::use_github_action_check_standard() to set up a GitHub Action that runs R CMD check on Linux, Mac, and Windows. Literally, this adds a new file to your package, below .github/workflows/.\nCommit this new file and push to GitHub.\nWait a few minutes to see the results in your email.\n\nWith this setup in place, R CMD check will be run every time you push to GitHub or whenever someone submits a pull request. You’ll find out about failures right away, which makes them easier to fix. Using automated checks also encourages me to check more often locally, because I know if it fails I’ll find out about it a few minutes later, often once I’ve moved on to a new problem.\n\n21.2.1 Other uses\nSince GitHub Actions allows you to run arbitrary code, there are many other things that you can use it for:\n\nRe-publishing a book website every time you make a change to the source. (Like this book!)\nBuilding vignettes and publishing them to a website.\nAutomatically building a documentation website for your package.\n\nLearn more about using GitHub Actions with R at https://github.com/r-lib/actions/tree/master/examples."
  },
  {
    "objectID": "release.html",
    "href": "release.html",
    "title": "22  Releasing to CRAN",
    "section": "",
    "text": "Second edition\n\n\n\nYou are reading the work-in-progress second edition of R Packages. This chapter is undergoing heavy restructuring and may be confusing or incomplete."
  },
  {
    "objectID": "release.html#introduction",
    "href": "release.html#introduction",
    "title": "22  Releasing to CRAN",
    "section": "\n22.1 Introduction",
    "text": "22.1 Introduction\nIf you want your package to have significant traction in the R community, you need to submit it to CRAN. Submitting to CRAN is a lot more work than just providing a version on github, but the vast majority of R users do not install packages from github, because CRAN provides discoverability, ease of installation and a stamp of authenticity. The CRAN submission process can be frustrating, but it’s worthwhile, and this chapter will make it as painless as possible.\nTo get your package ready to release, follow these steps:\n\nPick a version number.\nRun and document R CMD check.\nCheck that you’re aligned with CRAN policies.\nUpdate README.md and NEWS.md.\nSubmit the package to CRAN.\nPrepare for the next version by updating version numbers.\nPublicise the new version."
  },
  {
    "objectID": "release.html#sec-release-process",
    "href": "release.html#sec-release-process",
    "title": "22  Releasing to CRAN",
    "section": "\n22.2 The submission process",
    "text": "22.2 The submission process\nTo manually submit your package to CRAN, you create a package bundle (with devtools::build()) then upload it to https://cran.r-project.org/submit.html, along with some comments which describe the process you followed. This section shows you how to make submission as easy as possible by providing a standard structure for those comments. Later, in Section 22.4, you’ll see how to actually submit the package with devtools::release().\nWhen submitting to CRAN, remember that CRAN is staffed by volunteers, all of whom have other full-time jobs. A typical week has over 100 submissions and only three volunteers to process them all. The less work you make for them the more likely you are to have a pleasant submission experience.\nI recommend that you store your submission comments in a file called cran-comments.md. cran-comments.md should be checked into git (so you can track it over time), and listed in .Rbuildignore (so it’s not included in the package). As the extension suggests, I recommend using Markdown because it gives a standard way of laying out plain text. However, because the contents will never be rendered to another format, you don’t need to worry about sticking to it too closely. Here are the cran-comments.md from a recent version of httr:\n## R CMD check results\nThere were no ERRORs or WARNINGs. \n\nThere was 1 NOTE:\n\n* checking dependencies in R code ... NOTE\n  Namespace in Imports field not imported from: 'R6'\n\n  R6 is a build-time dependency.\n\n## Downstream dependencies\nI have also run R CMD check on downstream dependencies of httr \n(https://github.com/wch/checkresults/blob/master/httr/r-release). \nAll packages that I could install passed except:\n\n* Ecoengine: this appears to be a failure related to config on \n  that machine. I couldn't reproduce it locally, and it doesn't \n  seem to be related to changes in httr (the same problem exists \n  with httr 0.4).\nThis layout is designed to be easy to skim, and easy to match up to the R CMD check results seen by CRAN maintainers. It includes two sections:\n\nCheck results: I always state that there were no errors or warnings. Any NOTEs go in a bulleted list. For each NOTE, I include the message from R CMD check and a brief description of why I think it’s OK. If there were no NOTEs, I’d say “There were no ERRORs, WARNINGs or NOTEs”\nDownstream dependencies: If there are downstream dependencies, I run R CMD check on each package and summarise the results. If there are no downstream dependencies, keep this section, but say: “There are currently no downstream dependencies for this package”.\n\nThese are described in more detail below.\n\n22.2.1 Test environments\nWhen checking your package you need to make sure that it passed with the current development version of R and it works on at least two platforms. R CMD check is continuously evolving, so it’s a good idea to check your package with the latest development version, R-devel. You can install R-devel on your own machine:\n\nMac: install from https://mac.r-project.org/.\nWindows: install from https://cran.r-project.org/bin/windows/base/rdevel.html\nLinux: either build it from source, or better, learn about Docker containers and run the R-devel container from https://github.com/rocker-org/rocker.\n\nIt’s painful to manage multiple R versions, especially since you’ll need to reinstall all your packages. Instead, you can run R CMD check on CRAN’s servers with the devtools::check_win_*() family of functions. They build your package and submit it to the CRAN win-builder. 10-20 minutes after submission, you’ll receive an e-mail telling you the check results.\nCRAN runs on multiple platforms: Windows, Mac OS X, Linux, and Solaris. You don’t need to run R CMD check on every one of these platforms, but it’s a really good idea to do it on at least two. This increases your chances of spotting code that relies on the idiosyncrasies of specific platform. There are two easy ways to check on different platforms:\n\nrhub::check() which lets you manually run R CMD check on the platform of your choosing.\nusethis::use_github_action_check_standard() which helps you set up GitHub actions to automatically run R CMD check every time you push to GitHub.\n\nDebugging code that works on your computer but fails elsewhere is painful. If that happens to you, either install a virtualisation tool so that you can run another operating system locally, or find a friend to help you figure out the problem. Don’t submit the package and hope CRAN will help you figure out the problem.\n\n22.2.2 Check results\nYou’ve already learned how to use R CMD check and why it’s important in Chapter 20. Compared to running R CMD check locally, there are a few important differences when running it for a CRAN submission:\n\nYou must fix all ERRORs and WARNINGs. A package that contains any errors or warnings will not be accepted by CRAN.\n\nEliminate as many NOTEs as possible. Each NOTE requires human oversight, which is a precious commodity. If there are notes that you do not believe are important, it is almost always easier to fix them (even if the fix is a bit of a hack) than to persuade CRAN that they’re OK. See Section 20.3 for details on how to fix individual problems.\nIf you have no NOTEs it is less likely that your package will be flagged for additional human checks. These are time consuming for both you and CRAN, so are best avoided if possible.\n\n\nIf you can’t eliminate a NOTE, document it in cran-comments.md, describing why you think it is spurious. Your comments should be easy to scan, and easy to match up with R CMD check. Provide the CRAN maintainers with everything they need in one place, even if it means repeating yourself.\nNB: There will always be one NOTE when you first submit your package. This reminds CRAN that this is a new submission and that they’ll need to do some extra checks. You can’t eliminate this, so just mention in cran-comments.md that this is your first submission.\n\n\n22.2.3 Reverse dependencies\nFinally, if you’re releasing a new version of an existing package, it’s your responsibility to check that downstream dependencies (i.e. all packages that list your package in the Depends, Imports, Suggests or LinkingTo fields) continue to work. To help you do this, devtools provides devtools::revdep_check(). This section is slated for revision for the 2nd edition. In the meantime, know that this functionality is now provided by the revdepcheck package. This:\n\nSets up a temporary library so it doesn’t clobber any existing packages you have installed.\nInstalls all of the dependencies of the downstream dependencies.\nRuns R CMD check on each package.\nSummarises the results in a single file.\n\nRun usethis::use_revdep() to set up your package with a useful template.\nIf any packages fail R CMD check, you should give package authors at least two weeks to fix the problem before you submit your package to CRAN (you can easily get all maintainer e-mail addresses with revdep_maintainers()). After the two weeks is up, re-run the checks, and list any remaining failures in cran-comments.md. Each package should be accompanied by a brief explanation that either tells CRAN that it’s a false positive in R CMD check (e.g. you couldn’t install a dependency locally) or that it’s a legitimate change in the API (which the maintainer hasn’t fixed yet).\nInform CRAN of your release process: “I advised all downstream package maintainers of these problems two weeks ago”. Here’s an example from a recent release of dplyr:\nImportant reverse dependency check notes (full details at \nhttps://github.com/wch/checkresults/tree/master/dplyr/r-release);\n\n* COPASutils, freqweights, qdap, simPH: fail for various reasons. All package \n  authors were informed of the upcoming release and shown R CMD check issues \n  over two weeks ago.\n\n* ggvis: You'll be receiving a submission that fixes these issues very shortly\n  from Winston.\n\n* repra, rPref: uses a deprecated function."
  },
  {
    "objectID": "release.html#cran-policies",
    "href": "release.html#cran-policies",
    "title": "22  Releasing to CRAN",
    "section": "\n22.3 CRAN policies",
    "text": "22.3 CRAN policies\nAs well as the automated checks provided by R CMD check, there are a number of CRAN policies that must be checked manually. The CRAN maintainers will typically look at this very closely on a package’s first submission.\nI’ve summarised the most common problems below:\n\nIt’s vital that the maintainer’s e-mail address is stable because this is the only way that CRAN has to contact you, and if there are problems and they can’t get in touch with you they will remove your package from CRAN. So make sure it’s something that’s likely to be around for a while, and that it’s not heavily filtered.\nYou must have clearly identified the copyright holders in DESCRIPTION: if you have included external source code, you must ensure that the license is compatible. See Chapter 12 for more details.\nYou must “make all reasonable efforts” to get your package working across multiple platforms. Packages that don’t work on at least two will not normally be considered.\nDo not make external changes without explicit user permission. Don’t write to the file system, change options, install packages, quit R, send information over the internet, open external software, etc.\nDo not submit updates too frequently. The policy suggests a new version once every 1-2 months at most.\n\nI recommend following the CRAN Policy Watch Twitter account which tweets whenever there’s a policy change. You can also look at the GitHub repository that powers it: https://github.com/eddelbuettel/crp/commits/master/texi."
  },
  {
    "objectID": "release.html#sec-release-submission",
    "href": "release.html#sec-release-submission",
    "title": "22  Releasing to CRAN",
    "section": "\n22.4 Release",
    "text": "22.4 Release\nYou’re now ready to submit your package to CRAN. The easiest way to do this is to run devtools::release(). This:\n\nBuilds the package and runs R CMD check one last time.\nAsks you a number of yes/no questions to verify that you followed the most common best practices.\n\nAllows you to add your own questions to the check process by including an unexported release_questions() function in your package. This should return a character vector of questions to ask. For example, httr has:\n\nrelease_questions <- function() {\n  c(\n    \"Have you run all the OAuth demos?\",\n    \"Is inst/cacert.pem up to date?\"\n  )\n}\n\nThis is useful for reminding you to do any manual tasks that can’t otherwise be automated.\n\nUploads the package bundle to the CRAN submission form including the comments in cran-comments.md.\n\nWithin the next few minutes, you’ll receive an email notifying you of the submission and asking you to approve it (this confirms that the maintainer address is correct). Next the CRAN maintainers will run their checks and get back to you with the results. This normally takes around 24 hours, but occasionally can take up to 5 days.\n\n22.4.1 On failure\nIf your package does not pass R CMD check or is in violation of CRAN policies, a CRAN maintainer will e-mail you and describe the problem(s). Failures are frustrating, and the feedback may be curt and may feel downright insulting. Arguing with CRAN maintainers will likely waste both your time and theirs. Instead:\n\nBreathe. A rejected CRAN package is not the end of the world. It happens to everyone. Even members of R-core have to go through the same process and CRAN is no friendlier to them. I have had numerous packages rejected by CRAN. I was banned from submitting to CRAN for two weeks because too many of my existing packages had minor problems.\nIf the response gets you really riled up, take a couple of days to cool down before responding. Ignore any ad hominem attacks, and strive to respond only to technical issues.\nIf a devtools problem causes a CRAN maintainer to be annoyed with you, I am deeply sorry. If you forward me the message along with your address, I’ll send you a hand-written apology card.\n\nUnless you feel extremely strongly that discussion is merited, don’t respond to the e-mail. Instead:\n\nFix the identified problems and make recommended changes. Re-run devtools::check() to make sure you didn’t accidentally introduce any new problems.\n\nAdd a “Resubmission” section at the top of cran-comments.md. This should clearly identify that the package is a resubmission, and list the changes that you made.\n## Resubmission\nThis is a resubmission. In this version I have:\n\n* Converted the DESCRIPTION title to title case.\n\n* More clearly identified the copyright holders in the DESCRIPTION\n  and LICENSE files.\n\nIf necessary, update the check results and downstream dependencies sections.\nRun devtools::submit_cran() to re-submit the package without working through all the release() questions a second time.\n\n22.4.2 Binary builds\nAfter the package has been accepted by CRAN it will be built for each platform. It’s possible this may uncover further errors. Wait 48 hours until all the checks for all packages have been run, then go to the check results page for your package:\n\n\n\n\n\nPrepare a patch release that fixes the problems and submit using the same process as above."
  },
  {
    "objectID": "release.html#post-release",
    "href": "release.html#post-release",
    "title": "22  Releasing to CRAN",
    "section": "\n22.5 Prepare for next version",
    "text": "22.5 Prepare for next version\nOnce your package has been accepted by CRAN, you have a couple of technical tasks to do:\n\nIf you use GitHub, go to the repository release page. Create a new release with tag version v1.2.3 (i.e. “v” followed by the version of your package). Copy and paste the contents of the relevant NEWS.md section into the release notes.\nIf you use git, but not GitHub, tag the release with git tag -a v1.2.3.\nAdd the .9000 suffix to the Version field in the DESCRIPTION to indicate that this is a development version. Create a new heading in NEWS.md and commit the changes."
  },
  {
    "objectID": "release.html#promotion",
    "href": "release.html#promotion",
    "title": "22  Releasing to CRAN",
    "section": "\n22.6 Publicising your package",
    "text": "22.6 Publicising your package\nNow you’re ready for the fun part: publicising your package. This is really important. No one will use your helpful new package if they don’t know that it exists.\nStart by writing a release announcement. This should be an R Markdown document that briefly describes what the package does (so people who haven’t used it before can understand why they should care), and what’s new in this version. Start with the contents of NEWS.md, but you’ll need to modify it. The goal of NEWS.md is to be comprehensive; the goal of the release announcement is to highlight the most important changes. Include a link at the end of the announcement to the full release notes so people can see all the changes. Where possible, I recommend showing examples of new features: it’s much easier to understand the benefit of a new feature if you can see it in action.\nThere are a number of places you can include the announcement:\n\nIf you have a blog, publish it there. I now publish all package release announcements on the RStudio blog.\nIf you use Twitter, tweet about it with the #rstats hashtag.\nSend it to the r-packages mailing list. Messages sent to this list are automatically forwarded to the R-help mailing list."
  },
  {
    "objectID": "release.html#congratulations",
    "href": "release.html#congratulations",
    "title": "22  Releasing to CRAN",
    "section": "\n22.7 Congratulations!",
    "text": "22.7 Congratulations!\nYou have released your first package to CRAN and made it to the end of the book!"
  },
  {
    "objectID": "lifecycle.html",
    "href": "lifecycle.html",
    "title": "23  Lifecycle",
    "section": "",
    "text": "Second edition\n\n\n\nYou are reading the work-in-progress second edition of R Packages. This chapter is currently a dumping ground for ideas, and we don’t recommend reading it."
  },
  {
    "objectID": "lifecycle.html#description-version",
    "href": "lifecycle.html#description-version",
    "title": "23  Lifecycle",
    "section": "\n23.1 Introduction",
    "text": "23.1 Introduction"
  },
  {
    "objectID": "lifecycle.html#version",
    "href": "lifecycle.html#version",
    "title": "23  Lifecycle",
    "section": "\n23.2 Version",
    "text": "23.2 Version\nFormally, an R package version is a sequence of at least two integers separated by either . or -. For example, 1.0 and 0.9.1-10 are valid versions, but 1 and 1.0-devel are not. You can parse a version number with numeric_version().\n\nnumeric_version(\"1.9\") == numeric_version(\"1.9.0\")\n#> [1] TRUE\nnumeric_version(\"1.9.0\") < numeric_version(\"1.10.0\")\n#> [1] TRUE\n\nFor example, a package might have a version 1.9. This version number is considered by R to be the same as 1.9.0, less than version 1.9.2, and all of these are less than version 1.10 (which is version “one point ten”, not “one point one zero”). R uses version numbers to determine whether package dependencies are satisfied. A package might, for example, import package devtools (>= 1.9.2), in which case version 1.9 or 1.9.0 wouldn’t work.\nHere is our recommended framework for managing the package version number:\n\nAlways use . as the separator, never -.\nA released version number consists of three numbers, <major>.<minor>.<patch>. For version number 1.9.2, 1 is the major number, 9 is the minor number, and 2 is the patch number. Never use versions like 1.0, instead always spell out the three components, 1.0.0.\n\nAn in-development package has a fourth component: the development version. This should start at 9000. For example, the first version of the package should be 0.0.0.9000. There are two reasons for this recommendation: First, it makes it easy to see if a package is released or in-development. Also, the use of the fourth place means that you’re not limited to what the next version will be. 0.0.1, 0.1.0, and 1.0.0 are all greater than 0.0.0.9000.\nIncrement the development version, e.g. from 9000 to 9001, if you’ve added an important feature that another development package needs to depend on.\n\n\nThe advice above is inspired in part by Semantic Versioning and by the X.Org versioning schemes. Read them if you’d like to understand more about the standards of versioning used by many open source projects. Finally, know that other maintainers follow different philosophies on how to manage the package version number.\nThe version number of your package increases with subsequent releases of a package, but it’s more than just an incrementing counter – the way the number changes with each release can convey information about what kind of changes are in the package. We discuss this and more in Section 23.3. For now, just remember that the first version of your package should be 0.0.0.9000. usethis::create_package() does this, by default. usethis::use_version() increments the package version; when called interactively, with no argument, it presents a helpful menu:\n\nusethis::use_version()\n#> Current version is 0.1.\n#> What should the new version be? (0 to exit) \n#> \n#> 1: major --> 1.0\n#> 2: minor --> 0.2\n#> 3: patch --> 0.1.1\n#> 4:   dev --> 0.1.0.9000\n#> \n#> Selection:"
  },
  {
    "objectID": "lifecycle.html#sec-release-version",
    "href": "lifecycle.html#sec-release-version",
    "title": "23  Lifecycle",
    "section": "\n23.3 Version number",
    "text": "23.3 Version number\nIf you’ve been following our advice, the version number of your in-development package will have four components, major.minor.patch.dev, where dev is at least 9000. The number 9000 is arbitrary, but provides a strong visual signal there’s something different about this version number. Released packages don’t have a dev component, so now you need to drop that and pick a version number based on the changes you’ve made. For example, if the current version is 0.8.1.9000 will the next CRAN version be 0.8.2, 0.9.0 or 1.0.0? Use this advice to decide:\n\nIncrement patch, e.g. 0.8.2 for a patch: you’ve fixed bugs without adding any significant new features. I’ll often do a patch release if, after release, I discover a show-stopping bug that needs to be fixed ASAP. Most releases will have a patch number of 0.\nIncrement minor, e.g. 0.9.0, for a minor release. A minor release can include bug fixes, new features and changes in backward compatibility. This is the most common type of release. It’s perfectly fine to have so many minor releases that you need to use two (or even three!) digits, e.g. 1.17.0.\n\nIncrement major, e.g. 1.0.0, for a major release. This is best reserved for changes that are not backward compatible and that are likely to affect many users. Going from 0.b.c to 1.0.0 typically indicates that your package is feature complete with a stable API.\nIn practice, backward compatibility is not an all-or-nothing threshold. For example, if you make an API-incompatible change to a rarely-used part of your code, it may not deserve a major number change. But if you fix a bug that many people depend on, it will feel like an API breaking change. Use your best judgement."
  },
  {
    "objectID": "lifecycle.html#compatibility",
    "href": "lifecycle.html#compatibility",
    "title": "23  Lifecycle",
    "section": "\n23.4 Backward compatibility",
    "text": "23.4 Backward compatibility\nThe big difference between major and minor versions is whether or not the code is backward compatible. This difference is a bit academic in the R community because the way most people update packages is by running update.packages(), which always updates to the latest version of the package, even if the major version has changed, potentially breaking code. While more R users are becoming familiar with tools like packrat, which capture package versions on a per-project basis, you do need to be a little cautious when making big backward incompatible changes, regardless of what you do with the version number.\nThe importance of backward compatibility is directly proportional to the number of people using your package: you are trading your time for your users’ time. The harder you strive to maintain backward compatibility, the harder it is to develop new features or fix old mistakes. Backward compatible code also tends to be harder to read because of the need to maintain multiple paths to support functionality from previous versions. Be concerned about backward compatibility, but don’t let it paralyse you.\nThere are good reasons to make backward incompatible changes - if you made a design mistake that makes your package harder to use it’s better to fix it sooner rather than later. If you do need to make a backward incompatible change, it’s best to do it gradually. Provide interim version(s) between where are you now and where you’d like to be, and provide advice about what’s going to change. Depending on what you’re changing, use one of the following techniques to let your users know what’s happening:\n\n\nDon’t immediately remove a function. First deprecate it. For example, imagine your package is version 0.5.0 and you want to remove fun(). In version, 0.6.0, you’d use .Deprecated() to display a warning message whenever someone uses the function:\n\n# 0.6.0\nfun <- function(x, y, z) {\n  .Deprecated(\"sum\")\n  x + y + z\n}\n\nfun(1, 2, 3)\n#> Warning: 'fun' is deprecated.\n#> Use 'sum' instead.\n#> See help(\"Deprecated\")\n#> [1] 6\n\nThen, remove the function once you got to 0.7.0 (or if you are being very strict, once you got to 1.0.0 since it’s a backward incompatible change).\n\n\nSimilarly, if you’re removing a function argument, first warn about it:\n\nbar <- function(x, y, z) {\n  if (!missing(y)) {\n    warning(\"argument y is deprecated; please use z instead.\", \n      call. = FALSE)\n    z <- y\n  }\n}\n\nbar(1, 2, 3)\n#> Warning: argument y is deprecated; please use z instead.\n\n\n\nIf you’re deprecating a lot of code, it can be useful to add a helper function. For example, ggplot2 has gg_dep which automatically displays a message, warning or error, depending on how much the version number has changed.\n\ngg_dep <- function(version, msg) {\n  v <- as.package_version(version)\n  cv <- packageVersion(\"ggplot2\")\n\n  # If current major number is greater than last-good major number, or if\n  # current minor number is more than 1 greater than last-good minor number,\n  # return an error.\n  if (cv[[1,1]] > v[[1,1]]  ||  cv[[1,2]] > v[[1,2]] + 1) {\n    stop(msg, \" (Defunct; last used in version \", version, \")\",\n      call. = FALSE)\n\n  # If minor number differs by one, give a warning\n  } else if (cv[[1,2]] > v[[1,2]]) {\n    warning(msg, \" (Deprecated; last used in version \", version, \")\",\n      call. = FALSE)\n\n  # If only subminor number is greater, provide a message\n  } else if (cv[[1,3]] > v[[1,3]]) {\n    message(msg, \" (Deprecated; last used in version \", version, \")\")\n  }\n\n  invisible()\n}\n\n\nSignificant changes to an existing function requires planning, including making gradual changes over multiple versions. Try and develop a sequence of transformations where each change can be accompanied by an informative error message.\n\nIf you want to use functionality in a new version of another package, don’t make it a hard install-time dependency in the DESCRIPTION (forcing your users to upgrade that package might break other code). Instead check for the version at run-time:\n\nif (packageVersion(\"ggplot2\") < \"1.0.0\") {\n  stop(\"ggplot2 >= 1.0.0 needed for this function.\", call. = FALSE)\n}\n\nThis is also useful if you’re responding to changes in one of your dependencies - you’ll want to have a version that will work both before and after the change. This will allow you to submit it to CRAN at any time, even before the other package. Doing this may generate some R CMD check notes. For example:\n\nif (packageVersion(\"foo\") > \"1.0.0\") {\n  foo::baz()\n} else {\n  foo::bar()\n}\n\nIf baz doesn’t exist in foo version 1.0.0, you’ll get a note that it doesn’t exist in foo’s namespace. Just explain that you’re working around a difference between versions in your submission to CRAN."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "24  References",
    "section": "",
    "text": "Marwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018a.\n“Packaging Data Analytical Work Reproducibly Using r (and\nFriends).” The American Statistician 72 (1): 80–88. https://doi.org/10.1080/00031305.2017.1375986.\n\n\n———. 2018b. “Packaging Data Analytical Work Reproducibly Using r\n(and Friends).” PeerJ Preprints 6 (March): e3192v2. https://doi.org/10.7287/peerj.preprints.3192v2.\n\n\nMüller, Kirill, and Lorenz Walthert. 2018. Styler: Non-Invasive\nPretty Printing of R Code. http://styler.r-lib.org.\n\n\nSilge, Julia, John C. Nash, and Spencer Graves. 2018. “Navigating the R Package Universe.”\nThe R Journal 10 (2): 558–63. https://doi.org/10.32614/RJ-2018-058."
  }
]